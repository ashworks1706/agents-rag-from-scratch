{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36be9c2",
   "metadata": {},
   "source": [
    "# Agents and RAG, A Technical Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92002c",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389560b9",
   "metadata": {},
   "source": [
    "I'll go through the fundamentals of Agents and rag with the help of langchain library \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/awan_getting_langchain_ecosystem_1-1024x574.png\" width=700>\n",
    "\n",
    "<img src=\"https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2023/10/12015949/LlamaIndex.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb5de",
   "metadata": {},
   "source": [
    "### Brief History\n",
    "\n",
    "Before we dive into building agents, let's take a moment to understand the journey that brought us to this exciting point in AI history. Understanding where agents came from will help you appreciate why the systems we're building today represent such a significant breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4eff09",
   "metadata": {},
   "source": [
    "Let me tell you a story about how we got here. The concept of intelligent agents has evolved dramatically over the past seven decades, transforming from simple rule-based systems to today's sophisticated AI companions that can reason, plan, and act autonomously. \n",
    "\n",
    "**The Early Days (1950s-1980s):** Understanding this progression is essential because it helps us appreciate why modern agentic systems represent such a breakthrough. The journey began in the 1950s when researchers like Allen Newell and Herbert Simon created the Logic Theorist, a program that could prove mathematical theorems by exploring different logical paths. These early agents were like skilled craftsmen—they could perform specific tasks very well, but only within narrow, pre-defined domains.\n",
    "\n",
    "The 1970s and 1980s brought expert systems like MYCIN for medical diagnosis and DENDRAL for chemical analysis. While impressive, these systems required months of manual knowledge engineering, where human experts had to explicitly encode their domain knowledge into rigid rule sets. Imagine trying to teach someone to be a doctor by writing down every possible symptom combination and treatment - that's essentially what early AI researchers had to do!\n",
    "\n",
    "**The Networking Era (1990s-2000s):** The 1990s marked a shift toward more flexible software agents that could operate in networked environments and coordinate with other agents. This period introduced the concept of multi-agent systems, where multiple specialized agents could collaborate to solve complex problems. However, these systems still required extensive manual programming and could only handle situations their creators had anticipated.\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*Ygen57Qiyrc8DXAFsjZLNA.gif\" width=700>\n",
    "\n",
    "**The Learning Revolution (2000s-2010s):** The real transformation began in the 2000s with machine learning advances. Agents could now learn from data rather than relying solely on hand-coded rules. Virtual assistants like Siri and Alexa brought agent technology to mainstream consumers, though they remained relatively narrow in scope—essentially sophisticated voice interfaces for search and simple task execution.\n",
    "\n",
    "**The LLM Breakthrough (2020s):** The breakthrough moment arrived with large language models starting around 2020. Systems like GPT-3 and GPT-4 combined vast knowledge with sophisticated reasoning abilities, creating agents that could understand natural language, maintain context across conversations, and tackle a wide variety of tasks without task-specific programming. \n",
    "\n",
    "Unlike their predecessors, these modern agents can break down complex problems into steps, use external tools when needed, and adapt to new situations they've never encountered before. This evolution represents a fundamental shift from automation to augmentation—where early agents automated specific, predefined tasks, today's agents can understand our goals and work as collaborative partners in problem-solving.\n",
    "\n",
    "**Why This History Matters for You:** Understanding this evolution helps us appreciate that we're not just building better chatbots—we're creating systems that can handle ambiguous instructions, incomplete information, and constantly changing contexts. These capabilities make them invaluable for building sophisticated applications like the retrieval-augmented generation systems we'll explore in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0100815",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b4ae",
   "metadata": {},
   "source": [
    "When we talk about agents in 2025, we're entering a landscape where the term has become both ubiquitous and somewhat ambiguous. Different organizations and researchers use \"agent\" to describe everything from simple chatbots to fully autonomous systems that can operate independently for weeks. \n",
    "\n",
    "Another confusion lies with reinforcement learning name conventions, the agent described in reinforcement learnign is different from the LLM agents that we deal with now, even though, they share similar vision.\n",
    "\n",
    "But don't let this confusion discourage you! This diversity in definition isn't just academic—it reflects fundamentally different architectural approaches that will determine how we build the next generation of AI applications. Let me help you navigate this landscape.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!A_Oy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3177e12-432e-4e41-814f-6febf7a35f68_1360x972.png\" width=700>\n",
    "\n",
    "**What Actually Makes an Agent?** At its core, an agent is a system that can perceive its environment, make decisions, and take actions to achieve specific goals. Sounds simple, right? But the way these capabilities are implemented varies dramatically.\n",
    "\n",
    "Some define agents as fully autonomous systems that operate independently over extended periods, using various tools and adapting their strategies based on feedback. Think of these like a personal assistant who can manage your entire schedule, book flights, handle emails, and make decisions on your behalf without constant supervision.\n",
    "\n",
    "Others use the term more broadly to describe any system that follows predefined workflows to accomplish tasks. These implementations are more like following a detailed recipe—each step is predetermined, and while the system can handle some variations, it operates within clearly defined boundaries.\n",
    "\n",
    "**Why This Distinction Matters to You:** The difference between these approaches is crucial because it affects everything from system reliability to development complexity. Understanding this spectrum will help you choose the right approach for your specific needs and avoid over-engineering solutions.\n",
    "\n",
    "**The Spectrum of Control:** The most useful way to think about this spectrum is through the lens of control and decision-making:\n",
    "\n",
    "- **Workflows** are systems where large language models and tools are orchestrated through predefined code paths. Every decision point is anticipated by the developer, and the system follows predetermined logic to handle different scenarios.\n",
    "\n",
    "- **Agents** are systems where the LLM dynamically directs its own processes and tool usage, maintaining control over how it accomplishes tasks. The model itself decides what to do next, which tools to use, and how to adapt when things don't go as planned.\n",
    "\n",
    "Think of workflows as following a GPS route—you know exactly where you're going and how to get there. Agents are more like having an experienced local guide who can adapt the route based on traffic, weather, or interesting stops along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069523fb",
   "metadata": {},
   "source": [
    "#### Simplicity Defines Perfectionism, Not Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db57831",
   "metadata": {},
   "source": [
    "Now, here's some advice that might surprise you: when building applications with LLMs, the fundamental principle should be finding the simplest solution that meets your requirements. This might mean not building agentic systems at all!\n",
    "\n",
    "Let me explain why this matters. Agentic systems inherently trade latency and cost for better task performance. Every additional decision point, tool call, and reasoning step adds time and expense to your application. You need to carefully consider when this tradeoff makes sense for your specific use case.\n",
    "\n",
    "**When to Choose Workflows:** Workflows offer predictability and consistency for well-defined tasks where you can anticipate most scenarios and edge cases. They're excellent for:\n",
    "- Standardized processes like data processing pipelines\n",
    "- Content moderation workflows\n",
    "- Structured analysis tasks\n",
    "- Any situation where you need reliable, repeatable results\n",
    "\n",
    "**When to Choose Agents:** Agents become the better choice when you need flexibility and model-driven decision-making at scale. This includes situations where:\n",
    "- The variety of inputs and required responses is too broad to predefine\n",
    "- The system needs to adapt to entirely new scenarios\n",
    "- You're dealing with open-ended problems that require creative problem-solving\n",
    "- The complexity of decision trees would make workflow programming impractical\n",
    "\n",
    "**The Simple Truth:** Here's what I've learned from building production AI systems: for many applications, the most effective approach involves optimizing single LLM calls with retrieval and in-context examples rather than building complex agentic systems. \n",
    "\n",
    "Before you architect a sophisticated multi-agent system with elaborate tool chains, ask yourself: \"Could I solve this with a well-crafted prompt and some good examples?\" You'd be surprised how often the answer is yes.\n",
    "\n",
    "**But When Complexity is Worth It:** However, as we'll explore throughout this tutorial, there are compelling scenarios where the additional complexity of agents becomes not just beneficial, but necessary for achieving your goals. Understanding when and how to make this transition is what separates effective AI system builders from those who over-engineer solutions to problems that could be solved more simply.\n",
    "\n",
    "The key is developing good judgment about when to add complexity. Start simple, measure performance, and only add complexity when you can clearly demonstrate that it improves outcomes for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bd7ab",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Let's start with the most fundamental skill you'll need as an agent builder: crafting effective prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb1b8e",
   "metadata": {},
   "source": [
    "Let's talk about the foundation of everything we'll build: prompts. Think of prompts as the bridge between human intent and AI capabilities—they're how we translate our natural language requests into structured instructions that language models can understand and act upon.\n",
    "\n",
    "But here's what makes prompts fascinating in agentic systems: they're not just about getting good answers to single questions. In the context of agents, prompts become the architectural blueprints that define not only *what* we want the agent to accomplish, but *how* the agent should approach problem-solving, what tools it can use, and how it should reason through complex tasks.\n",
    "\n",
    "**Why Prompts Are Your Most Important Tool:** I like to think of prompts as the instruction manual for your AI agent. Just as a well-written manual can make the difference between a novice successfully assembling furniture or ending up with a pile of confused parts, a well-crafted prompt determines whether your agent performs brilliantly or struggles to understand your intent.\n",
    "\n",
    "The quality and structure of your prompts directly influence:\n",
    "- The agent's reasoning capabilities\n",
    "- How it chooses and uses tools  \n",
    "- Its overall effectiveness in completing tasks\n",
    "- The consistency of results across different inputs\n",
    "\n",
    "<img src=\"https://www.datablist.com/_next/image?url=%2Fhowto_images%2Fhow-to-write-prompt-ai-agents%2Fstructured-ai-agent-prompt.png&w=3840&q=75\" width=700>\n",
    "\n",
    "**The Different Types of Prompts You'll Use:** As we build more sophisticated systems, you'll work with several types of prompts, each serving different purposes:\n",
    "\n",
    "- **System prompts** establish the agent's role, personality, and fundamental operating principles—these are like giving someone their job description and company handbook before they start work\n",
    "- **User prompts** contain the specific tasks or questions you want the agent to handle\n",
    "- **Few-shot prompts** provide examples of desired input-output patterns to guide the agent's responses\n",
    "- **Chain-of-thought prompts** encourage step-by-step reasoning, helping agents break down complex problems into manageable pieces\n",
    "\n",
    "**The Multi-Step Challenge:** In multi-step agentic workflows, prompt engineering becomes particularly sophisticated because you need to design prompts that not only solve individual tasks but also coordinate between different stages of processing. The agent needs to understand when to use specific tools, how to interpret tool outputs, and how to maintain context across multiple interaction cycles.\n",
    "\n",
    "This requires careful consideration of prompt structure, token efficiency, and the logical flow of information through your system. Don't worry—we'll practice all of this together as we build real systems.\n",
    "\n",
    "**Let's See It in Action:** Now that you understand why prompts matter so much, let's explore how to implement effective prompt templates using LangChain with Google's Gemini model. We'll start with basics and gradually work up to sophisticated multi-step prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# COMPREHENSIVE SETUP AND IMPORTS\n",
    "# ================================\n",
    "# This cell contains all imports and basic setup for the entire tutorial\n",
    "# Think of this as setting up your workspace before starting a project\n",
    "\n",
    "# Core LangChain and LLM imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "import os\n",
    "import tempfile\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Memory system imports\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory, \n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationEntityMemory,\n",
    "    CombinedMemory,\n",
    "    ReadOnlySharedMemory,\n",
    "    SimpleMemory\n",
    ")\n",
    "from langchain.memory.entity import InMemoryEntityStore\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Mathematical libraries for calculations\n",
    "import numpy as np\n",
    "\n",
    "# ================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Initialize our PRIMARY LLM instance\n",
    "# This is like having one expert that we'll keep consulting throughout the tutorial\n",
    "# Instead of hiring different experts for each task, we'll extend this one with different skills\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\", \n",
    "    temperature=0.3,  # Balanced creativity and consistency\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Tutorial state acts as our persistent memory across all cells\n",
    "# This is where we'll store reusable components so we don't recreate them\n",
    "tutorial_state = {\n",
    "    \"current_section\": \"setup\",\n",
    "    \"demo_data\": {},\n",
    "    \"conversation_history\": [],\n",
    "    \"skills_registry\": {},\n",
    "    \"memory_systems\": {}\n",
    "}\n",
    "\n",
    "print(\"🚀 Agents and RAG Tutorial - Setup Complete\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📦 All imports loaded successfully\")\n",
    "print(\"🔧 Global LLM instance created:\")\n",
    "print(f\"   • Model: {llm.model}\")\n",
    "print(f\"   • Temperature: {llm.temperature} (balanced)\")\n",
    "print(\"📋 Tutorial state initialized\")\n",
    "print(\"\\n💡 KEY CONCEPT: Throughout this tutorial, we'll REUSE this LLM instance\")\n",
    "print(\"   Instead of creating new instances, we'll extend it with:\")\n",
    "print(\"   • Different prompt templates\")\n",
    "print(\"   • Various tools and capabilities\")\n",
    "print(\"   • Different memory systems\")\n",
    "print(\"   • Specialized configurations\")\n",
    "print(\"\\n   This mirrors real-world development where you typically have\")\n",
    "print(\"   one LLM instance that you configure for different purposes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dba25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the tutorial\n",
    "%pip install langchain langchain-google-genai langchain-core numpy\n",
    "\n",
    "print(\"📦 Installing packages for Agents and RAG tutorial...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416983e",
   "metadata": {},
   "source": [
    "we'll create some prompt examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create prompt templates that we'll reuse throughout the tutorial\n",
    "# These will work with our global llm instance\n",
    "\n",
    "def setup_prompt_templates():\n",
    "    \"\"\"Initialize reusable prompt templates for the tutorial\"\"\"\n",
    "    \n",
    "    # Basic instructional prompt - for general explanations\n",
    "    basic_template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"audience\"],\n",
    "        template=\"\"\"You are an expert educator who excels at explaining complex topics clearly.\n",
    "        \n",
    "        Topic: {topic}\n",
    "        Audience: {audience}\n",
    "        \n",
    "        Please provide a clear, engaging explanation that includes:\n",
    "        1. Core concept definition\n",
    "        2. Relevant examples or analogies  \n",
    "        3. Key takeaways for the audience level\n",
    "        \n",
    "        Keep your explanation appropriate for the specified audience.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Conversational prompt - for interactive discussions\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant with expertise in technology and science. \n",
    "        You provide accurate, clear explanations and engage in detailed discussions.\n",
    "        Always think step-by-step when solving problems and explain your reasoning.\"\"\"),\n",
    "        (\"human\", \"I need help understanding {concept}. Can you break it down for me?\"),\n",
    "        (\"ai\", \"I'd be happy to help explain {concept}! Let me break this down step by step.\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    \n",
    "    # Store templates in tutorial_state for reuse throughout the notebook\n",
    "    tutorial_state[\"prompt_templates\"] = {\n",
    "        \"basic\": basic_template,\n",
    "        \"chat\": chat_template\n",
    "    }\n",
    "    \n",
    "    # Create reusable chains with our global llm\n",
    "    tutorial_state[\"chains\"] = {\n",
    "        \"basic\": basic_template | llm | StrOutputParser(),\n",
    "        \"chat\": chat_template | llm | StrOutputParser()\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Prompt templates created and stored in tutorial_state\")\n",
    "    print(\"🔗 Chains connected to our global llm instance\")\n",
    "    print(\"📝 Templates available: basic, chat\")\n",
    "    return tutorial_state[\"prompt_templates\"]\n",
    "\n",
    "# Initialize our reusable prompt system\n",
    "prompt_templates = setup_prompt_templates()\n",
    "\n",
    "print(\"\\n\udca1 These templates will be reused throughout the tutorial\")\n",
    "print(\"\udd04 No need to recreate them - they're stored in tutorial_state\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacbea9",
   "metadata": {},
   "source": [
    "Great! now our LLM can respond to our questions, but how can we tweak it more to determine how much it weighs the prompt guideline while responding with it's own knowledge and reasoning? let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c858ca",
   "metadata": {},
   "source": [
    "###  Hyperparameters\n",
    "\n",
    "Once you've mastered basic prompting, the next level of control comes from understanding how to tune your model's behavior through hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d7ff9",
   "metadata": {},
   "source": [
    "Now let's dive into one of the most fascinating aspects of working with language models: hyperparameters. These are the control knobs that determine how a language model generates responses, acting like the settings on a sophisticated instrument that can dramatically change the output quality and behavior.\n",
    "\n",
    "**Why Understanding Hyperparameters Matters:** Understanding these parameters is crucial for building effective agents because they directly influence:\n",
    "- How the model balances following prompt instructions versus drawing on its pre-trained knowledge\n",
    "- How creative or conservative its responses are\n",
    "- How consistently it behaves across multiple interactions\n",
    "- Whether it takes safe, predictable paths or explores more novel solutions\n",
    "\n",
    "Let me walk you through the key parameters and show you the mathematical foundations that drive their behavior.\n",
    "\n",
    "**Temperature (τ) - The Creativity Knob:** Temperature controls the randomness in the model's token selection process through the softmax function. Here's how it works mathematically:\n",
    "\n",
    "Given logits $z_i$ for each possible token $i$, the probability distribution is calculated as:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{z_i/τ}}{\\sum_{j=1}^{V} e^{z_j/τ}}$$\n",
    "\n",
    "Where:\n",
    "- $τ$ (tau) is the temperature parameter\n",
    "- $V$ is the vocabulary size  \n",
    "- Lower $τ$ → sharper distribution (more deterministic)\n",
    "- Higher $τ$ → flatter distribution (more random)\n",
    "\n",
    "At $τ = 1$, we get the standard softmax. As $τ → 0$, the distribution approaches a one-hot encoding of the highest logit (very predictable). As $τ → ∞$, the distribution becomes uniform (completely random).\n",
    "\n",
    "**Top-p (Nucleus Sampling) - The Focus Control:** Top-p works by selecting the smallest set of tokens whose cumulative probability exceeds threshold $p$:\n",
    "\n",
    "$$\\text{Nucleus} = \\{i : \\sum_{j \\in \\text{top-k tokens}} P(token_j) \\leq p\\}$$\n",
    "\n",
    "This creates a dynamic vocabulary size—sometimes the model considers many options, sometimes just a few, depending on how confident it is.\n",
    "\n",
    "**Top-k - The Hard Limit:** Top-k simply restricts consideration to the $k$ highest-probability tokens, where $k$ is a fixed integer. It's simpler than top-p but less adaptive.\n",
    "\n",
    "**Practical Control Parameters:**\n",
    "- **Max tokens** provides an upper bound $N_{max}$ on sequence length\n",
    "- **Stop sequences** define termination conditions based on specific token patterns\n",
    "\n",
    "**The Art of Parameter Selection:** The key insight is that these parameters create fundamental tradeoffs. You're not just adjusting \"creativity\"—you're choosing between instruction-following precision and knowledge-bringing flexibility. \n",
    "\n",
    "For agents, this choice becomes critical: Do you want an agent that follows instructions exactly, or one that can creatively adapt its approach? The answer depends entirely on your use case.\n",
    "\n",
    "Let's explore how these parameters affect model behavior in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af59b",
   "metadata": {},
   "source": [
    "we'll have three types of model instances defined to differentiate between their creativity and max tokens as far as we can get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's explore how hyperparameters affect our existing LLM's behavior\n",
    "# We'll create variants using our global llm configuration as a template\n",
    "\n",
    "def demonstrate_temperature_effects(topic=\"quantum computing\"):\n",
    "    \"\"\"\n",
    "    Demonstrate how temperature affects the same LLM's responses\n",
    "    We'll use our existing llm instance and adjust only temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use our existing prompt template from tutorial_state\n",
    "    prompt = tutorial_state[\"prompt_templates\"][\"basic\"]\n",
    "    \n",
    "    # Create temperature variants using the same model as our global llm\n",
    "    temperatures = {\n",
    "        \"conservative\": 0.1,   # τ = 0.1 for high determinism\n",
    "        \"balanced\": 0.7,       # τ = 0.7 (same as our global llm)\n",
    "        \"creative\": 1.2        # τ = 1.2 for high creativity\n",
    "    }\n",
    "    \n",
    "    print(\"🌡️ TESTING TEMPERATURE EFFECTS ON RESPONSES\")\n",
    "    print(f\"Using the same model: {llm.model}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, temp_value in temperatures.items():\n",
    "        # Create a temporary llm instance with different temperature\n",
    "        temp_llm = ChatGoogleGenerativeAI(\n",
    "            model=llm.model,  # Same model as global llm\n",
    "            temperature=temp_value,\n",
    "            max_tokens=150,\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Use our existing chain pattern\n",
    "        chain = prompt | temp_llm | StrOutputParser()\n",
    "        response = chain.invoke({\n",
    "            \"topic\": topic,\n",
    "            \"audience\": \"technical professionals\"\n",
    "        })\n",
    "        \n",
    "        results[config_name] = response\n",
    "        print(f\"\\n{config_name.upper()} (τ={temp_value}):\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Store results in our tutorial state\n",
    "    tutorial_state[\"demo_data\"][\"hyperparameter_comparison\"] = results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with our reusable function\n",
    "print(\"\\n🧪 Demonstrating how temperature affects our LLM's behavior\")\n",
    "hyperparameter_results = demonstrate_temperature_effects()\n",
    "\n",
    "print(\"\\n✅ Temperature demonstration complete\")\n",
    "print(\"📊 Notice how the same model produces different outputs at different temperatures\")\n",
    "print(\"💡 Our global llm uses τ=0.3 for balanced results throughout the tutorial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfe18f",
   "metadata": {},
   "source": [
    "now let's see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24947aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our hyperparameter demonstrations and see the results\n",
    "print(\"🧪 Running hyperparameter demonstrations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test temperature effects using the function we defined\n",
    "print(\"\\n1️⃣ Testing Temperature Effects on the Same Query\")\n",
    "temp_results = demonstrate_temperature_effects(topic=\"neural networks\")\n",
    "\n",
    "print(\"\\n2️⃣ Analyzing the Results\")\n",
    "print(\"Notice how the same model at different temperatures produces:\")\n",
    "print(\"   • Conservative (τ=0.1): Focused, predictable responses\")\n",
    "print(\"   • Balanced (τ=0.7): Mix of consistency and variety\") \n",
    "print(\"   • Creative (τ=1.2): More diverse, exploratory responses\")\n",
    "\n",
    "print(\"\\n💡 Our global llm uses τ=0.3 throughout this tutorial\")\n",
    "print(\"   This gives us reliable, consistent behavior while allowing some flexibility\")\n",
    "\n",
    "print(\"\\n✅ Hyperparameter demonstrations complete\")\n",
    "print(\"📊 Results stored in tutorial_state['demo_data']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347026a1",
   "metadata": {},
   "source": [
    "**What We Just Discovered:** The examples above demonstrate something fundamental about how hyperparameters work in practice. They create a crucial tradeoff between instruction following and creative knowledge application. \n",
    "\n",
    "**Low Temperature Models:** Excel at following precise formatting requirements and maintaining consistency across multiple calls. This makes them ideal for:\n",
    "- Structured data extraction\n",
    "- API responses that need consistent formatting\n",
    "- Workflows where predictability is paramount\n",
    "- Any situation where you need the model to be a reliable, consistent executor\n",
    "\n",
    "**Higher Temperature Models:** Bring more of the model's training knowledge into play, generating more diverse responses and creative solutions. They're better for:\n",
    "- Creative writing and content generation\n",
    "- Problem-solving that benefits from novel approaches\n",
    "- Situations where you want the model to \"think outside the box\"\n",
    "- Applications where some variation in responses is actually beneficial\n",
    "\n",
    "**The Agent Design Choice:** This balance becomes critical in agentic systems where you need to decide whether your agent should be a precise executor of specific instructions or a creative problem-solver that can adapt its approach based on context. \n",
    "\n",
    "The choice often depends on your use case: customer service bots might need low-temperature consistency to ensure professional, predictable responses, while creative writing assistants might benefit from higher-temperature diversity to generate fresh ideas and varied approaches.\n",
    "\n",
    "**Moving Forward:** Now that we understand how to control our model's behavior through prompts and hyperparameters, we need to give our agents the ability to extend beyond their base knowledge and interact with the world. This is where tools come into play—they're what transform a language model from a sophisticated text generator into an active agent that can perform real actions and access current information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df627673",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "With prompts and hyperparameters mastered, it's time to give your agents the ability to interact with the world beyond their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf486d",
   "metadata": {},
   "source": [
    "Now we're getting to one of the most exciting parts of building agentic systems: tools! Tools are what transform language models from sophisticated text generators into active agents capable of performing real-world actions and accessing live information.\n",
    "\n",
    "**Think of Tools as Your Agent's Hands and Senses:** Without tools, even the most advanced language model is limited to working with only the knowledge it was trained on, which becomes stale the moment training ends. Tools bridge this gap by allowing agents to interact with databases, APIs, web services, file systems, and any other external systems your application needs to work with.\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGyFCaSY8w4Ag/article-cover_image-shrink_720_1280/B4DZYg8dDRHAAI-/0/1744309441965?e=1762992000&v=beta&t=NS3gCnYSTWkxVwnRpHX6tCG7wcXcGgEknNpowIVAo2k\" width=700>\n",
    "\n",
    "**How Tool Calling Actually Works:** The fundamental concept behind tools in agentic systems is function calling (also known as tool calling). Here's what makes this so powerful: modern language models like GPT-4, Claude, and Gemini have been specifically trained to understand when they need external information or capabilities, and can generate structured function calls with appropriate parameters.\n",
    "\n",
    "When an agent encounters a question about current weather, stock prices, or needs to perform calculations, it doesn't hallucinate an answer—instead, it recognizes the limitation and calls the appropriate tool. This is a game-changer for building reliable systems!\n",
    "\n",
    "**The Tool Execution Dance:** Let me walk you through how this works in practice:\n",
    "\n",
    "1. **Request Analysis:** The agent receives a user request and analyzes what information or actions are needed\n",
    "2. **Tool Selection:** It determines which tools to use based on the requirements  \n",
    "3. **Parameter Formatting:** It formats the tool calls with proper parameters\n",
    "4. **Execution:** The tools are executed and return results\n",
    "5. **Synthesis:** The agent receives the results and synthesizes a response using both its knowledge and the tool outputs\n",
    "\n",
    "**The Power of Tool Chaining:** This creates a powerful feedback loop where agents can chain multiple tool calls together, use the output of one tool as input to another, and dynamically adapt their approach based on intermediate results. Imagine an agent that searches the web for recent news, summarizes the findings, then generates a report—all in one coherent workflow!\n",
    "\n",
    "**Three Categories of Tools We'll Explore:**\n",
    "\n",
    "1. **Built-in tools** that come pre-integrated with language model providers\n",
    "2. **Explicit tools** that you define and implement yourself  \n",
    "3. **Model Context Protocol (MCP) tools** that provide standardized interfaces for complex integrations\n",
    "\n",
    "Each category serves different purposes and offers varying levels of customization and complexity. Let's start exploring them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233785",
   "metadata": {},
   "source": [
    "#### Starting Simple: Built-in Tools\n",
    "\n",
    "The easiest way to get started with agent tools is to use the capabilities that come built into your language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b29da",
   "metadata": {},
   "source": [
    "Let's start with the easiest way to give your agents powerful capabilities: built-in tools. These are native capabilities provided directly by language model providers, eliminating the need for external integrations or custom implementations.\n",
    "\n",
    "**Why Built-in Tools Are Awesome:** Google's Gemini models, for example, come with several powerful built-in tools including Google Search integration, code execution capabilities, and mathematical computation tools. These tools are particularly valuable because:\n",
    "\n",
    "- **Optimized Integration:** They're optimized for the specific model with minimal latency overhead\n",
    "- **No Extra Setup:** You don't need additional API keys or setup beyond your primary model access  \n",
    "- **Seamless Experience:** The model provider handles all the complexity of tool execution, result formatting, and error handling\n",
    "- **Reliability:** They're battle-tested and maintained by the model provider\n",
    "\n",
    "**Real-World Example:** When you enable Google Search for Gemini, the model can perform web searches and incorporate real-time information directly into its responses without any additional code on your part. It's like giving your agent instant access to the entire internet!\n",
    "\n",
    "Similarly, the code execution tool allows Gemini to write and run Python code in a sandboxed environment, making it excellent for data analysis, mathematical calculations, and generating visualizations. Imagine asking your agent to \"analyze this sales data and create a chart\" and having it actually execute the code to do so!\n",
    "\n",
    "**The Trade-off to Consider:** The main limitation of built-in tools is that you're constrained to what the provider offers. You can't customize their behavior or add your own specialized functionality. But for many use cases, the convenience and reliability make this a great starting point.\n",
    "\n",
    "Let's see how to use these powerful capabilities with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_builtin_tool_agents():\n",
    "    \"\"\"\n",
    "    Create agents with different built-in tool configurations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Base configuration -  our global llm settings\n",
    "    base_config = {\n",
    "        \"model\": \"gemini-1.5-pro\",\n",
    "        \"google_api_key\": os.getenv(\"GOOGLE_API_KEY\")\n",
    "    }\n",
    "    \n",
    "    # Agent with Google Search integration - extends our base config\n",
    "    search_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.3,  # Same as our global llm\n",
    "        tools=[\"google_search_retrieval\"]\n",
    "    )\n",
    "    \n",
    "    # Agent with code execution - different temperature for reliability\n",
    "    code_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.1,  # Lower temperature for code reliability\n",
    "        tools=[\"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    # Multi-tool agent - combines capabilities\n",
    "    multi_tool_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.4,\n",
    "        tools=[\"google_search_retrieval\", \"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    tutorial_state[\"builtin_agents\"] = {\n",
    "        \"search_agent\": search_agent,\n",
    "        \"code_agent\": code_agent, \n",
    "        \"multi_tool_agent\": multi_tool_agent\n",
    "    }\n",
    "    \n",
    "    return tutorial_state[\"builtin_agents\"]\n",
    "\n",
    "def test_builtin_tools():\n",
    "    \"\"\"\n",
    "    Test various built-in tool capabilities\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get our agents (created above)\n",
    "    agents = create_builtin_tool_agents()\n",
    "    \n",
    "    base_chat_template = tutorial_state[\"prompt_templates\"][\"chat\"]\n",
    "    \n",
    "    # Create specialized variants by modifying the system message\n",
    "    search_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can search for current information when needed. Use this capability when the user asks about recent events or needs up-to-date information.\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    code_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can execute Python code for calculations and analysis. Use this when mathematical computations or data analysis is needed.\"),\n",
    "        (\"human\", \"{analysis_request}\")\n",
    "    ])\n",
    "    \n",
    "    tutorial_state[\"prompt_templates\"].update({\n",
    "        \"search_enhanced\": search_prompt,\n",
    "        \"code_enhanced\": code_prompt\n",
    "    })\n",
    "    \n",
    "    search_chain = search_prompt | agents[\"search_agent\"] | StrOutputParser()\n",
    "    code_chain = code_prompt | agents[\"code_agent\"] | StrOutputParser()\n",
    "    \n",
    "    tutorial_state[\"chains\"].update({\n",
    "        \"search_chain\": search_chain,\n",
    "        \"code_chain\": code_chain\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"search_chain\": search_chain,\n",
    "        \"code_chain\": code_chain\n",
    "    }\n",
    "\n",
    "# Execute the functions\n",
    "agents = create_builtin_tool_agents()\n",
    "chains = test_builtin_tools()\n",
    "\n",
    "print(\"\\n🎯 BUILT-IN TOOLS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Agents created using shared configuration\")\n",
    "print(\"✅ Prompt templates extended from existing base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's demonstrate our built-in tool agents\n",
    "# These extend our global llm with additional capabilities\n",
    "\n",
    "print(\"🔧 Testing Built-in Tool Capabilities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our agents from tutorial_state (they use the same base llm)\n",
    "agents = tutorial_state.get(\"builtin_agents\", {})\n",
    "\n",
    "if agents:\n",
    "    print(\"\\n✅ Using pre-configured agents with built-in tools\")\n",
    "    print(f\"   Available agents: {list(agents.keys())}\")\n",
    "    \n",
    "    # Get our chains that use these agents\n",
    "    chains = tutorial_state.get(\"chains\", {})\n",
    "    \n",
    "    if \"search_chain\" in chains:\n",
    "        print(\"\\n🔍 Example: Search-enabled agent\")\n",
    "        print(\"   This agent can search for current information when needed\")\n",
    "        print(\"   💡 It uses our same base llm but with Google Search capability\")\n",
    "    \n",
    "    if \"code_chain\" in chains:\n",
    "        print(\"\\n💻 Example: Code execution agent\")\n",
    "        print(\"   This agent can write and execute Python code\")\n",
    "        print(\"   💡 It uses our same base llm but with code execution capability\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Built-in agents not yet initialized\")\n",
    "    print(\"   They will be created when needed using our global llm\")\n",
    "\n",
    "print(\"\\n💡 Key insight: All these agents share the same base LLM\")\n",
    "print(\"   We're just adding different tool capabilities on top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f08dc",
   "metadata": {},
   "source": [
    "#### Explicit Tools : Building Agent Memory\n",
    "\n",
    "As we build more sophisticated agents, we quickly run into a fundamental challenge: how do we help our agents remember important information across conversations and interactions? This is where memory systems become crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a351b4",
   "metadata": {},
   "source": [
    "**Why Memory Matters:** Think about how frustrating it would be to work with a colleague who forgot everything you discussed after each meeting. That's essentially what happens with stateless language models—each interaction starts fresh, with no memory of previous conversations or learned preferences.\n",
    "\n",
    "Memory systems solve this by allowing agents to:\n",
    "- **Maintain Context**: Remember what you've discussed previously\n",
    "- **Learn Preferences**: Adapt to your communication style and needs over time  \n",
    "- **Build Relationships**: Create more natural, ongoing conversations\n",
    "- **Accumulate Knowledge**: Learn from interactions to become more effective\n",
    "\n",
    "**The Challenge:** The tricky part is deciding what to remember, how long to keep it, and how to retrieve relevant memories when needed. Different memory strategies work better for different types of applications.\n",
    "\n",
    "Let's explore the various memory systems available and learn when to use each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_custom_tools():\n",
    "    \"\"\"Define custom tools using @tool decorator for explicit functionality\"\"\"\n",
    "    \n",
    "    @tool\n",
    "    def get_weather(city: str, country: str = \"US\") -> str:\n",
    "        \"\"\"\n",
    "        Get current weather information for a specified city.\n",
    "        \n",
    "        Args:\n",
    "            city: The name of the city to get weather for\n",
    "            country: The country code (default: US)\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with weather information\n",
    "        \"\"\"\n",
    "        # Simulate weather API call - replace with real API in production\n",
    "        weather_conditions = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\", \"partly cloudy\"]\n",
    "        temperature = random.randint(-10, 35)\n",
    "        condition = random.choice(weather_conditions)\n",
    "        \n",
    "        weather_data = {\n",
    "            \"city\": city,\n",
    "            \"country\": country,\n",
    "            \"temperature\": temperature,\n",
    "            \"condition\": condition,\n",
    "            \"humidity\": random.randint(30, 90),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return json.dumps(weather_data, indent=2)\n",
    "\n",
    "    @tool\n",
    "    def calculate_compound_interest(principal: float, rate: float, time: int, compounds_per_year: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Calculate compound interest using the formula: A = P(1 + r/n)^(nt)\n",
    "        \n",
    "        Mathematical Foundation:\n",
    "        A = P(1 + r/n)^(nt)\n",
    "        Where:\n",
    "        - A = final amount\n",
    "        - P = principal (initial investment) \n",
    "        - r = annual interest rate (as decimal)\n",
    "        - n = number of times interest compounds per year\n",
    "        - t = time in years\n",
    "        \n",
    "        Args:\n",
    "            principal: Initial investment amount\n",
    "            rate: Annual interest rate (as decimal, e.g., 0.05 for 5%)\n",
    "            time: Number of years\n",
    "            compounds_per_year: Compounding frequency (default: 1)\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string with calculation details\n",
    "        \"\"\"\n",
    "        # Apply compound interest formula\n",
    "        amount = principal * (1 + rate/compounds_per_year) ** (compounds_per_year * time)\n",
    "        interest_earned = amount - principal\n",
    "        \n",
    "        result = {\n",
    "            \"principal\": principal,\n",
    "            \"annual_rate\": f\"{rate*100}%\", \n",
    "            \"time_years\": time,\n",
    "            \"compounds_per_year\": compounds_per_year,\n",
    "            \"final_amount\": round(amount, 2),\n",
    "            \"interest_earned\": round(interest_earned, 2),\n",
    "            \"total_return_percentage\": round((interest_earned/principal)*100, 2)\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "\n",
    "    @tool  \n",
    "    def search_user_database(query: str, user_type: str = \"all\") -> str:\n",
    "        \"\"\"\n",
    "        Search a simulated user database for customer information.\n",
    "        \n",
    "        Args:\n",
    "            query: Search term (name, email, or ID)\n",
    "            user_type: Filter by user type - \"premium\", \"basic\", or \"all\"\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with user information\n",
    "        \"\"\"\n",
    "        # Mock database - replace with actual database queries in production\n",
    "        mock_users = [\n",
    "            {\"id\": \"001\", \"name\": \"Alice Johnson\", \"email\": \"alice@email.com\", \"type\": \"premium\", \"status\": \"active\"},\n",
    "            {\"id\": \"002\", \"name\": \"Bob Smith\", \"email\": \"bob@email.com\", \"type\": \"basic\", \"status\": \"active\"}, \n",
    "            {\"id\": \"003\", \"name\": \"Carol Davis\", \"email\": \"carol@email.com\", \"type\": \"premium\", \"status\": \"inactive\"},\n",
    "            {\"id\": \"004\", \"name\": \"David Wilson\", \"email\": \"david@email.com\", \"type\": \"basic\", \"status\": \"active\"}\n",
    "        ]\n",
    "        \n",
    "        # Apply user type filter\n",
    "        if user_type != \"all\":\n",
    "            mock_users = [user for user in mock_users if user[\"type\"] == user_type]\n",
    "        \n",
    "        # Search logic with fuzzy matching\n",
    "        results = []\n",
    "        query_lower = query.lower()\n",
    "        for user in mock_users:\n",
    "            if (query_lower in user[\"name\"].lower() or \n",
    "                query_lower in user[\"email\"].lower() or \n",
    "                query_lower == user[\"id\"]):\n",
    "                results.append(user)\n",
    "        \n",
    "        return json.dumps({\"query\": query, \"results\": results}, indent=2)\n",
    "    \n",
    "    return [get_weather, calculate_compound_interest, search_user_database]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127adc",
   "metadata": {},
   "source": [
    "great now we'll create the armed agent and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf42e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an agent that uses our custom tools\n",
    "# This agent will use our existing global llm instance\n",
    "\n",
    "def create_custom_tool_agent():\n",
    "    \"\"\"\n",
    "    Create an agent with custom tools using our existing llm instance\n",
    "    This shows how to extend our base agent with specific capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the tools we created earlier\n",
    "    custom_tools = tutorial_state.get(\"tools\", {}).get(\"custom_tools\", create_custom_tools())\n",
    "    \n",
    "    # Create a prompt that works with our existing llm\n",
    "    tool_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant with access to several specialized tools:\n",
    "        \n",
    "        🌤️  get_weather: Get current weather for any city\n",
    "        💰 calculate_compound_interest: Calculate investment returns with compound interest\n",
    "        👥 search_user_database: Look up customer information in database\n",
    "        \n",
    "        Use these tools when needed to provide accurate, helpful responses.\n",
    "        Always explain which tool you're using and why.\n",
    "        Format JSON data nicely for users.\"\"\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    \n",
    "    # Create agent using our global llm\n",
    "    agent = create_tool_calling_agent(llm, custom_tools, tool_prompt)\n",
    "    \n",
    "    # Create executor\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=custom_tools, \n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    # Store in tutorial state for reuse\n",
    "    tutorial_state[\"agents\"] = tutorial_state.get(\"agents\", {})\n",
    "    tutorial_state[\"agents\"][\"custom_tool_agent\"] = agent_executor\n",
    "    \n",
    "    print(\"\udd16 Custom tool agent created using our global llm\")\n",
    "    print(f\"🔧 Tools available: {len(custom_tools)}\")\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "# Create our reusable agent\n",
    "tool_agent = create_custom_tool_agent()\n",
    "\n",
    "print(\"✅ Agent ready and stored in tutorial_state['agents']\")\n",
    "print(\"💡 We can reuse this agent for multiple queries without recreating it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3328",
   "metadata": {},
   "source": [
    "#### Model Context Protocol (MCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56fff",
   "metadata": {},
   "source": [
    "Model Context Protocol (MCP) represents the next evolution in AI tool integration, providing a standardized way for AI applications to securely connect to data sources and tools. Think of MCP as a universal translator that allows any AI system to communicate with any external service through a common protocol, eliminating the need for custom integrations for each tool or data source.\n",
    "\n",
    "<img src=\"https://mintcdn.com/mcp/bEUxYpZqie0DsluH/images/mcp-simple-diagram.png?w=1100&fit=max&auto=format&n=bEUxYpZqie0DsluH&q=85&s=341b88d6308188ab06bf05748c80a494\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/tweet_video_thumb/Gl7C44tXYAAdDSJ.jpg\" width=700>\n",
    "\n",
    "<img src=\"https://miro.medium.com/0*qtnzILuhG39c2DML.jpeg\" width=700>\n",
    "\n",
    "\n",
    "\n",
    "MCP was developed by Anthropic to solve the fragmentation problem in AI tool ecosystems. Before MCP, every AI application had to implement its own custom integrations for databases, APIs, file systems, and other external resources. This led to duplicated effort, security inconsistencies, and tools that only worked with specific AI platforms. MCP standardizes these interactions through a client-server architecture where MCP servers expose resources (like databases or file systems) and tools (like calculators or API clients) through a uniform interface.\n",
    "\n",
    "The protocol operates on JSON-RPC 2.0, enabling real-time, bidirectional communication between AI applications (MCP clients) and external resources (MCP servers). This means your agent can not only call tools but also receive real-time updates, notifications, and streaming data from external systems. The security model is built around explicit capability declarations and sandboxed execution, ensuring that agents can only access resources they've been explicitly granted permission to use.\n",
    "\n",
    "What makes MCP particularly powerful for RAG and agentic systems is its ability to provide **contextual data access**. Instead of just calling functions, MCP servers can expose rich contextual information about resources - like database schemas, file structures, or API capabilities - allowing agents to make more informed decisions about how to interact with external systems.\n",
    "\n",
    "Let's explore how to integrate MCP servers with LangChain and Gemini. For this example, we'll use the MCP SDK to create a simple server and then connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio loops for Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Real MCP Server Implementation\n",
    "class BusinessMCPServer:\n",
    "    \"\"\"Real MCP Server that exposes business data and tools\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.resources = {\n",
    "            \"customer_db\": {\n",
    "                \"customers\": [\n",
    "                    {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\", \"tier\": \"gold\", \"balance\": 15000},\n",
    "                    {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"tier\": \"silver\", \"balance\": 5000},\n",
    "                    {\"id\": 3, \"name\": \"Bob Wilson\", \"email\": \"bob@example.com\", \"tier\": \"bronze\", \"balance\": 1200}\n",
    "                ],\n",
    "                \"schema\": {\n",
    "                    \"id\": \"integer\",\n",
    "                    \"name\": \"string\", \n",
    "                    \"email\": \"string\",\n",
    "                    \"tier\": \"string\",\n",
    "                    \"balance\": \"number\"\n",
    "                }\n",
    "            },\n",
    "            \"inventory\": {\n",
    "                \"items\": [\n",
    "                    {\"sku\": \"A001\", \"name\": \"Premium Laptop\", \"quantity\": 50, \"price\": 1299.99, \"category\": \"electronics\"},\n",
    "                    {\"sku\": \"A002\", \"name\": \"Wireless Mouse\", \"quantity\": 200, \"price\": 29.99, \"category\": \"accessories\"},\n",
    "                    {\"sku\": \"A003\", \"name\": \"USB-C Hub\", \"quantity\": 75, \"price\": 59.99, \"category\": \"accessories\"}\n",
    "                ],\n",
    "                \"schema\": {\n",
    "                    \"sku\": \"string\",\n",
    "                    \"name\": \"string\",\n",
    "                    \"quantity\": \"integer\", \n",
    "                    \"price\": \"number\",\n",
    "                    \"category\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"analytics\": {\n",
    "                \"sales\": {\"month\": 245000, \"trend\": \"up\", \"growth\": 12.5},\n",
    "                \"users\": {\"month\": 1850, \"trend\": \"up\", \"growth\": 8.2},\n",
    "                \"revenue\": {\"month\": 189000, \"trend\": \"stable\", \"growth\": 2.1}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def start_server(self):\n",
    "        \"\"\"Start the MCP server process\"\"\"\n",
    "        # Create a simple server script\n",
    "        server_script = '''\n",
    "import asyncio\n",
    "import json\n",
    "from mcp.server import Server\n",
    "from mcp.server.stdio import stdio_server\n",
    "from mcp.types import Resource, Tool, TextContent\n",
    "\n",
    "app = Server(\"business-mcp-server\")\n",
    "\n",
    "# Server resources and data\n",
    "resources_data = ''' + json.dumps(self.resources) + '''\n",
    "\n",
    "@app.list_resources()\n",
    "async def list_resources() -> list[Resource]:\n",
    "    \"\"\"List available resources\"\"\"\n",
    "    return [\n",
    "        Resource(\n",
    "            uri=\"mcp://business/customer_db\",\n",
    "            name=\"Customer Database\",\n",
    "            description=\"Customer information and account details\",\n",
    "            mimeType=\"application/json\"\n",
    "        ),\n",
    "        Resource(\n",
    "            uri=\"mcp://business/inventory\", \n",
    "            name=\"Inventory System\",\n",
    "            description=\"Product inventory and stock levels\",\n",
    "            mimeType=\"application/json\"\n",
    "        ),\n",
    "        Resource(\n",
    "            uri=\"mcp://business/analytics\",\n",
    "            name=\"Analytics System\", \n",
    "            description=\"Business analytics and metrics\",\n",
    "            mimeType=\"application/json\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "@app.read_resource()\n",
    "async def read_resource(uri: str) -> str:\n",
    "    \"\"\"Read resource content\"\"\"\n",
    "    if uri == \"mcp://business/customer_db\":\n",
    "        return json.dumps(resources_data[\"customer_db\"])\n",
    "    elif uri == \"mcp://business/inventory\":\n",
    "        return json.dumps(resources_data[\"inventory\"])\n",
    "    elif uri == \"mcp://business/analytics\":\n",
    "        return json.dumps(resources_data[\"analytics\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resource: {uri}\")\n",
    "\n",
    "@app.list_tools()\n",
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"List available tools\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"query_analytics\",\n",
    "            description=\"Query business analytics and metrics\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"metric\": {\"type\": \"string\", \"enum\": [\"sales\", \"users\", \"revenue\"]},\n",
    "                    \"period\": {\"type\": \"string\", \"enum\": [\"day\", \"week\", \"month\", \"year\"]}\n",
    "                },\n",
    "                \"required\": [\"metric\"]\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"send_notification\",\n",
    "            description=\"Send notifications to users or systems\", \n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"recipient\": {\"type\": \"string\"},\n",
    "                    \"message\": {\"type\": \"string\"},\n",
    "                    \"priority\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n",
    "                },\n",
    "                \"required\": [\"recipient\", \"message\"]\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"update_inventory\",\n",
    "            description=\"Update product inventory levels\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {\n",
    "                    \"sku\": {\"type\": \"string\"},\n",
    "                    \"quantity\": {\"type\": \"integer\"},\n",
    "                    \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"set\"]}\n",
    "                },\n",
    "                \"required\": [\"sku\", \"quantity\", \"operation\"]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "@app.call_tool()\n",
    "async def call_tool(name: str, arguments: dict) -> list[TextContent]:\n",
    "    \"\"\"Execute tools\"\"\"\n",
    "    if name == \"query_analytics\":\n",
    "        metric = arguments.get(\"metric\", \"sales\")\n",
    "        period = arguments.get(\"period\", \"month\")\n",
    "        data = resources_data[\"analytics\"].get(metric, {})\n",
    "        result = {\n",
    "            \"metric\": metric,\n",
    "            \"period\": period,\n",
    "            \"value\": data.get(period, 0),\n",
    "            \"trend\": data.get(\"trend\", \"unknown\"),\n",
    "            \"growth\": data.get(\"growth\", 0),\n",
    "            \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "        }\n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "        \n",
    "    elif name == \"send_notification\":\n",
    "        result = {\n",
    "            \"status\": \"sent\",\n",
    "            \"recipient\": arguments.get(\"recipient\"),\n",
    "            \"message\": arguments.get(\"message\"), \n",
    "            \"priority\": arguments.get(\"priority\", \"medium\"),\n",
    "            \"delivery_id\": f\"notify_{hash(str(arguments)) % 10000}\",\n",
    "            \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "        }\n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "        \n",
    "    elif name == \"update_inventory\":\n",
    "        sku = arguments.get(\"sku\")\n",
    "        quantity = arguments.get(\"quantity\", 0)\n",
    "        operation = arguments.get(\"operation\", \"set\")\n",
    "        \n",
    "        # Find item in inventory\n",
    "        items = resources_data[\"inventory\"][\"items\"]\n",
    "        item = next((item for item in items if item[\"sku\"] == sku), None)\n",
    "        \n",
    "        if not item:\n",
    "            result = {\"error\": f\"SKU {sku} not found\"}\n",
    "        else:\n",
    "            old_qty = item[\"quantity\"]\n",
    "            if operation == \"add\":\n",
    "                item[\"quantity\"] += quantity\n",
    "            elif operation == \"subtract\":\n",
    "                item[\"quantity\"] = max(0, item[\"quantity\"] - quantity)\n",
    "            else:  # set\n",
    "                item[\"quantity\"] = quantity\n",
    "                \n",
    "            result = {\n",
    "                \"sku\": sku,\n",
    "                \"operation\": operation,\n",
    "                \"old_quantity\": old_qty,\n",
    "                \"new_quantity\": item[\"quantity\"],\n",
    "                \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "            }\n",
    "        \n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "    \n",
    "    else:\n",
    "        return [TextContent(type=\"text\", text=json.dumps({\"error\": f\"Unknown tool: {name}\"}))]\n",
    "\n",
    "async def main():\n",
    "    async with stdio_server() as (read_stream, write_stream):\n",
    "        await app.run(read_stream, write_stream, app.create_initialization_options())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "        \n",
    "        # Save server script to temporary file\n",
    "        self.server_file = tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False)\n",
    "        self.server_file.write(server_script)\n",
    "        self.server_file.close()\n",
    "        \n",
    "        print(f\"✅ MCP Server script created at: {self.server_file.name}\")\n",
    "        return self.server_file.name\n",
    "    \n",
    "    async def connect(self, server_script_path: str):\n",
    "        \"\"\"Connect to the MCP server\"\"\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"python\",\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.stdio_client = stdio_client(server_params)\n",
    "            self.read_stream, self.write_stream, self.session = await self.stdio_client.__aenter__()\n",
    "            print(\"✅ Connected to MCP server successfully\")\n",
    "            \n",
    "            # List available resources and tools\n",
    "            resources = await self.session.list_resources()\n",
    "            tools = await self.session.list_tools()\n",
    "            \n",
    "            print(f\"📂 Available Resources: {len(resources.resources)}\")\n",
    "            for resource in resources.resources:\n",
    "                print(f\"   - {resource.name}: {resource.description}\")\n",
    "                \n",
    "            print(f\"🔧 Available Tools: {len(tools.tools)}\")\n",
    "            for tool in tools.tools:\n",
    "                print(f\"   - {tool.name}: {tool.description}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to connect to MCP server: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def read_resource(self, uri: str) -> str:\n",
    "        \"\"\"Read resource from MCP server\"\"\"\n",
    "        try:\n",
    "            result = await self.session.read_resource(uri)\n",
    "            return result.contents[0].text if result.contents else \"{}\"\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to read resource {uri}: {str(e)}\"})\n",
    "    \n",
    "    async def call_tool(self, name: str, arguments: dict) -> str:\n",
    "        \"\"\"Call tool on MCP server\"\"\"\n",
    "        try:\n",
    "            result = await self.session.call_tool(name, arguments)\n",
    "            return result.content[0].text if result.content else \"{}\"\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to call tool {name}: {str(e)}\"})\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup MCP server connection\"\"\"\n",
    "        if hasattr(self, 'stdio_client'):\n",
    "            try:\n",
    "                await self.stdio_client.__aexit__(None, None, None)\n",
    "            except:\n",
    "                pass\n",
    "        if hasattr(self, 'server_file'):\n",
    "            try:\n",
    "                os.unlink(self.server_file.name)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Initialize the real MCP server\n",
    "async def setup_mcp_server():\n",
    "    \"\"\"Setup and start the MCP server\"\"\"\n",
    "    server = BusinessMCPServer()\n",
    "    server_script = await server.start_server()\n",
    "    \n",
    "    # Give the server a moment to initialize\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "    success = await server.connect(server_script)\n",
    "    if success:\n",
    "        return server\n",
    "    else:\n",
    "        raise Exception(\"Failed to setup MCP server\")\n",
    "\n",
    "# Run the MCP server setup\n",
    "print(\"🚀 Setting up Real MCP Server...\")\n",
    "business_mcp = await setup_mcp_server()\n",
    "print(\"✅ Real MCP Server ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain tools that interface with our REAL MCP server\n",
    "# These tools provide a bridge between LangChain and MCP\n",
    "\n",
    "@tool\n",
    "def mcp_read_resource(resource_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Read data from MCP server resources like databases or file systems.\n",
    "    \n",
    "    Args:\n",
    "        resource_name: Name of the resource to read (customer_db, inventory, analytics)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with resource data\n",
    "    \"\"\"\n",
    "    uri_map = {\n",
    "        \"customer_db\": \"mcp://business/customer_db\",\n",
    "        \"customers\": \"mcp://business/customer_db\", \n",
    "        \"inventory\": \"mcp://business/inventory\",\n",
    "        \"products\": \"mcp://business/inventory\",\n",
    "        \"analytics\": \"mcp://business/analytics\",\n",
    "        \"metrics\": \"mcp://business/analytics\"\n",
    "    }\n",
    "    \n",
    "    uri = uri_map.get(resource_name.lower())\n",
    "    if not uri:\n",
    "        return json.dumps({\"error\": f\"Resource '{resource_name}' not found. Available: {list(uri_map.keys())}\"})\n",
    "    \n",
    "    # Use asyncio to call the async MCP method\n",
    "    async def _read():\n",
    "        return await business_mcp.read_resource(uri)\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_read())\n",
    "\n",
    "@tool\n",
    "def mcp_query_analytics(metric: str, period: str = \"month\") -> str:\n",
    "    \"\"\"\n",
    "    Query business analytics through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        metric: The metric to query (sales, users, revenue)\n",
    "        period: Time period for the metric (day, week, month, year)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with analytics data\n",
    "    \"\"\"\n",
    "    async def _query():\n",
    "        return await business_mcp.call_tool(\"query_analytics\", {\n",
    "            \"metric\": metric,\n",
    "            \"period\": period\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_query())\n",
    "\n",
    "@tool  \n",
    "def mcp_send_notification(recipient: str, message: str, priority: str = \"medium\") -> str:\n",
    "    \"\"\"\n",
    "    Send notifications through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        recipient: Who to send the notification to\n",
    "        message: The notification message\n",
    "        priority: Priority level (low, medium, high)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with delivery confirmation\n",
    "    \"\"\"\n",
    "    async def _notify():\n",
    "        return await business_mcp.call_tool(\"send_notification\", {\n",
    "            \"recipient\": recipient,\n",
    "            \"message\": message,\n",
    "            \"priority\": priority\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_notify())\n",
    "\n",
    "@tool\n",
    "def mcp_update_inventory(sku: str, quantity: int, operation: str = \"set\") -> str:\n",
    "    \"\"\"\n",
    "    Update product inventory levels through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        sku: Product SKU to update\n",
    "        quantity: Quantity to add, subtract, or set\n",
    "        operation: Operation type (add, subtract, set)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with update confirmation\n",
    "    \"\"\"\n",
    "    async def _update():\n",
    "        return await business_mcp.call_tool(\"update_inventory\", {\n",
    "            \"sku\": sku,\n",
    "            \"quantity\": quantity,\n",
    "            \"operation\": operation\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_update())\n",
    "\n",
    "# Create MCP-enabled tools list\n",
    "mcp_tools = [mcp_read_resource, mcp_query_analytics, mcp_send_notification, mcp_update_inventory]\n",
    "\n",
    "# Create an agent that can use MCP tools\n",
    "mcp_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "mcp_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a business intelligence assistant with access to company systems through the Model Context Protocol (MCP).\n",
    "    \n",
    "    🔗 **Available MCP Resources:**\n",
    "    - customer_db: Customer information and account details with tiers and balances\n",
    "    - inventory: Product inventory with SKUs, quantities, prices, and categories  \n",
    "    - analytics: Real-time business metrics including sales, users, and revenue data\n",
    "    \n",
    "    🛠️ **Available MCP Tools:**\n",
    "    - mcp_query_analytics: Get business metrics and analytics with trends\n",
    "    - mcp_send_notification: Send notifications to users or systems\n",
    "    - mcp_read_resource: Read data from company databases and systems\n",
    "    - mcp_update_inventory: Modify product inventory levels (add/subtract/set)\n",
    "    \n",
    "    **Your Capabilities:**\n",
    "    - Access real-time business data through MCP resources\n",
    "    - Execute business operations through MCP tools  \n",
    "    - Provide comprehensive insights with actual company data\n",
    "    - Take actions like updating inventory or sending notifications\n",
    "    \n",
    "    Always explain what MCP resources or tools you're using and format results clearly for business users.\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "mcp_agent = create_tool_calling_agent(mcp_llm, mcp_tools, mcp_prompt)\n",
    "mcp_executor = AgentExecutor(\n",
    "    agent=mcp_agent,\n",
    "    tools=mcp_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "print(\"=== Real MCP-Enabled Agent Created ===\")\n",
    "print(\"🤖 Agent ready with REAL MCP server integration\")\n",
    "print(\"📡 Connected to business systems via Model Context Protocol\")\n",
    "print(\"🔧 Available tools:\", len(mcp_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the REAL MCP-enabled agent with comprehensive business scenarios\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🧪 TESTING REAL MCP SERVER INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n=== Test 1: Customer Data Analysis via MCP ===\")\n",
    "print(\"🔍 Using MCP resource: customer_db\")\n",
    "customer_analysis = mcp_executor.invoke({\n",
    "    \"input\": \"Analyze our customer data. Show me the customer information, tier distribution, and total customer value.\"\n",
    "})\n",
    "print(\"📋 Response:\", customer_analysis['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 2: Real-time Business Analytics via MCP Tools ===\") \n",
    "print(\"📊 Using MCP tool: query_analytics\")\n",
    "analytics_query = mcp_executor.invoke({\n",
    "    \"input\": \"Get our current sales and revenue metrics for this month. Also check user growth trends.\"\n",
    "})\n",
    "print(\"📈 Response:\", analytics_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 3: Inventory Management via MCP ===\")\n",
    "print(\"📦 Using MCP resource and tools: inventory + update_inventory\")\n",
    "inventory_management = mcp_executor.invoke({\n",
    "    \"input\": \"Check our current inventory levels, then update the laptop inventory by adding 25 units. Also check if we're low on any items.\"\n",
    "})\n",
    "print(\"🏪 Response:\", inventory_management['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 4: Business Operations - Notification System ===\")\n",
    "print(\"📢 Using MCP tool: send_notification\")\n",
    "notification_test = mcp_executor.invoke({\n",
    "    \"input\": \"Send a high-priority notification to the warehouse manager about low stock levels for any items under 100 units.\"\n",
    "})\n",
    "print(\"🔔 Response:\", notification_test['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 5: Comprehensive Business Dashboard ===\")\n",
    "print(\"🎯 Using multiple MCP resources and tools\")\n",
    "dashboard_query = mcp_executor.invoke({\n",
    "    \"input\": \"\"\"Create a comprehensive business dashboard showing:\n",
    "    1. Customer tier distribution and total value\n",
    "    2. Current sales performance and trends  \n",
    "    3. Inventory status with any low-stock alerts\n",
    "    4. Send a summary notification to the CEO\n",
    "    \n",
    "    Use all available MCP resources and tools to gather this information.\"\"\"\n",
    "})\n",
    "print(\"📊 Dashboard Response:\", dashboard_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ REAL MCP INTEGRATION TESTS COMPLETED\")\n",
    "print(\"🎉 Model Context Protocol successfully integrated!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8b567",
   "metadata": {},
   "source": [
    "\n",
    "This real MCP implementation demonstrates how modern AI systems can safely and efficiently integrate with enterprise systems using standardized protocols rather than ad-hoc custom integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cleanup MCP Server Resources\n",
    "# Run this when you're done with the MCP server to clean up resources\n",
    "\n",
    "async def cleanup_mcp_server():\n",
    "    \"\"\"Cleanup MCP server resources\"\"\"\n",
    "    try:\n",
    "        await business_mcp.cleanup()\n",
    "        print(\"✅ MCP server resources cleaned up successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning: {e}\")\n",
    "\n",
    "# Uncomment the line below if you want to cleanup the MCP server\n",
    "# await cleanup_mcp_server()\n",
    "\n",
    "print(\"💡 MCP server is ready for use!\")\n",
    "print(\"🧹 Run cleanup_mcp_server() when finished to release resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56c6e2",
   "metadata": {},
   "source": [
    "The examples above demonstrate the power of tools in transforming language models into capable agents. We've seen how **built-in tools** provide immediate capabilities with minimal setup, **explicit tools** offer complete customization for your specific needs, and **MCP tools** enable standardized integration with complex systems while maintaining security and scalability.\n",
    "\n",
    "The key insight is that tools are what bridge the gap between language model intelligence and real-world utility. Without tools, even the most sophisticated language model is limited to generating text based on its training data. With tools, agents become active participants in your business processes, capable of querying databases, performing calculations, calling APIs, and taking actions in response to user needs.\n",
    "\n",
    "As we design agentic systems, the choice between different tool types depends on your specific requirements:\n",
    "- Use **built-in tools** when the model provider offers functionality that meets your needs\n",
    "- Create **explicit tools** when you need custom integration with your specific systems  \n",
    "- Implement **MCP tools** when you need standardized, scalable integrations across multiple AI applications\n",
    "\n",
    "Now that our agents can take actions in the world through tools, we need to ensure they can maintain context and remember information across interactions. This is where memory and context management become crucial for building agents that can handle complex, multi-step workflows and maintain coherent conversations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0edd85",
   "metadata": {},
   "source": [
    "### Context Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7cd88",
   "metadata": {},
   "source": [
    "Context management is the cognitive backbone of sophisticated agents, determining how they maintain awareness of ongoing conversations, remember past interactions, and build upon previous knowledge to provide coherent, contextually relevant responses. Without proper context management, even the most capable agents become like individuals with severe short-term memory loss—they might excel at individual tasks but fail to maintain meaningful, coherent interactions over time.\n",
    "\n",
    "Think of context management as the difference between having a conversation with a knowledgeable expert who remembers your entire discussion versus repeatedly starting fresh with someone who has no recollection of what you've already covered. The former builds understanding progressively, references earlier points, and adapts their communication based on your evolving needs. The latter, while potentially knowledgeable, forces you to repeat yourself and cannot build on the conversational foundation you've established.\n",
    "\n",
    "In agentic systems, context management becomes even more critical because agents need to coordinate information across multiple tool calls, maintain state during complex workflows, and remember important details that influence future decisions. An agent helping with financial planning needs to remember your risk tolerance, investment timeline, and previous decisions to provide consistent advice. A customer service agent should recall your account history, previous issues, and preferences to deliver personalized support.\n",
    "\n",
    "The challenge lies in balancing several competing factors: **memory capacity** (how much information can be retained), **relevance** (what information is most important to keep), **efficiency** (managing token limits and processing costs), and **persistence** (maintaining memory across sessions). Different memory strategies excel in different scenarios, and the best approach often involves combining multiple memory types to create a comprehensive context management system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b3a57",
   "metadata": {},
   "source": [
    "<img src=\"https://substackcdn.com/image/fetch/$s_!AyLS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0e3c002-0841-4d5f-9171-3eb63c321824_1600x1224.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592d71",
   "metadata": {},
   "source": [
    "Memory systems in agentic applications serve different purposes and have distinct strengths and limitations. Understanding these differences is crucial for selecting the right memory strategy for your specific use case. Let's explore the major categories of memory available in LangChain and how they can be effectively utilized.\n",
    "\n",
    "**Buffer-based memories** store raw conversation history up to certain limits, providing complete fidelity but consuming significant token space. **Summary-based memories** compress conversation history into concise summaries, trading some detail for efficiency. **Window-based memories** maintain only recent interactions, ensuring relevance while discarding older context. **Token-aware memories** dynamically manage content based on token consumption, balancing completeness with cost constraints.\n",
    "\n",
    "Each memory type excels in specific scenarios: use buffer memory for short conversations where every detail matters, summary memory for long-running sessions where themes and key decisions need tracking, window memory for task-oriented interactions where only recent context is relevant, and token buffer memory for cost-sensitive applications with unpredictable conversation lengths.\n",
    "\n",
    "- **Buffer Memory**: Stores everything - perfect recall but grows indefinitely\n",
    "- **Summary Memory**: Compresses older content - manageable size with key information preserved  \n",
    "- **Window Memory**: Only recent context - predictable size but limited history\n",
    "- **Token Memory**: Smart pruning based on token limits - cost-controlled with intelligent truncation\n",
    "- **Entity Memory**: Relationship tracking - maintains entity awareness across conversations\n",
    "\n",
    "\n",
    "Let's implement and compare these different memory systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create different memory systems that work with our global llm\n",
    "# These will help our agent remember conversations in different ways\n",
    "\n",
    "def setup_memory_systems():\n",
    "    \"\"\"\n",
    "    Initialize various memory systems for our agent\n",
    "    All will use the same global llm but with different memory strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dedicated LLM for memory operations (slightly lower temperature for consistency)\n",
    "    memory_llm = ChatGoogleGenerativeAI(\n",
    "        model=llm.model,  # Same model as global llm\n",
    "        temperature=0.2,  # Lower temperature for more consistent memory operations\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Store the memory llm in tutorial_state\n",
    "    tutorial_state[\"memory_llm\"] = memory_llm\n",
    "    \n",
    "    # Initialize our memory systems\n",
    "    memory_systems = {\n",
    "        \"Buffer (Complete)\": ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Summary (Compressed)\": ConversationSummaryMemory(\n",
    "            llm=memory_llm,\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Window (Last 3)\": ConversationBufferWindowMemory(\n",
    "            k=3,  # Keep last 3 conversation pairs\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Token Limited\": ConversationTokenBufferMemory(\n",
    "            llm=memory_llm,\n",
    "            max_token_limit=500,\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Entity Tracking\": ConversationEntityMemory(\n",
    "            llm=memory_llm,\n",
    "            entity_store=InMemoryEntityStore(),\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Store in tutorial_state for reuse\n",
    "    tutorial_state[\"memory_systems\"] = memory_systems\n",
    "    \n",
    "    # Create conversation chains for each memory type\n",
    "    memory_chains = {}\n",
    "    for name, memory_system in memory_systems.items():\n",
    "        memory_chains[name] = ConversationChain(\n",
    "            llm=memory_llm,  # Use our consistent memory llm\n",
    "            memory=memory_system,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    tutorial_state[\"memory_chains\"] = memory_chains\n",
    "    \n",
    "    print(\"🧠 Memory Systems Initialized\")\n",
    "    print(f\"   📊 {len(memory_systems)} different memory strategies\")\n",
    "    print(f\"   🔗 All using consistent memory LLM (τ=0.2)\")\n",
    "    print(f\"   💾 Available types: {list(memory_systems.keys())}\")\n",
    "    \n",
    "    return memory_systems, memory_chains\n",
    "\n",
    "# Initialize our memory systems\n",
    "memory_systems, memory_chains = setup_memory_systems()\n",
    "\n",
    "print(\"\\n✅ Memory systems ready for use throughout the tutorial\")\n",
    "print(\"💡 These will help our agent remember conversations in different ways\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095169",
   "metadata": {},
   "source": [
    "##### Comparing Memory Systems Side-by-Side:\n",
    "\n",
    "Now that we understand each memory type individually, let's create a direct comparison to see how they behave differently with the same input. This will help you understand when to choose each approach:\n",
    "\n",
    "\n",
    "Let's test them all with the same business conversation scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our memory systems with a business scenario\n",
    "# We'll use the chains we already created in tutorial_state\n",
    "\n",
    "print(\"🧪 Testing Memory Systems with Business Scenario\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our pre-configured memory chains\n",
    "memory_chains = tutorial_state.get(\"memory_chains\", {})\n",
    "\n",
    "if not memory_chains:\n",
    "    print(\"⚠️ Memory chains not initialized, setting them up now...\")\n",
    "    memory_systems, memory_chains = setup_memory_systems()\n",
    "\n",
    "# Define a realistic conversation scenario\n",
    "test_scenario = [\n",
    "    \"Hi, I'm working on the TechCorp project with a $2M budget.\",\n",
    "    \"The project manager is Sarah Chen, and we're targeting Q4 launch.\", \n",
    "    \"We need to coordinate with the development team led by Mike Rodriguez.\",\n",
    "    \"The main deliverable is a cloud migration to Azure platform.\",\n",
    "    \"Sarah mentioned the timeline is aggressive - only 3 months to complete.\",\n",
    "    \"What are the key risks we should be monitoring for this project?\"\n",
    "]\n",
    "\n",
    "print(f\"📝 Testing with {len(test_scenario)} conversation turns\")\n",
    "print(\"\\n\udca1 Each memory system will process the same conversation\")\n",
    "print(\"   Watch how they handle context differently\")\n",
    "\n",
    "# Test each memory system\n",
    "scenario_results = {}\n",
    "\n",
    "for memory_name, chain in memory_chains.items():\n",
    "    print(f\"\\n--- Testing {memory_name} ---\")\n",
    "    \n",
    "    # Process all conversation turns with this memory system\n",
    "    for i, user_input in enumerate(test_scenario, 1):\n",
    "        response = chain.predict(input=user_input)\n",
    "        print(f\"Turn {i}: ✅\")\n",
    "    \n",
    "    # Store the final response for comparison\n",
    "    final_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    scenario_results[memory_name] = {\n",
    "        \"final_response\": final_response,\n",
    "        \"memory_type\": memory_name\n",
    "    }\n",
    "    \n",
    "    # Clear memory for next test\n",
    "    chain.memory.clear()\n",
    "    print(f\"✓ Completed and cleared\")\n",
    "\n",
    "# Store results\n",
    "tutorial_state[\"memory_test_results\"] = scenario_results\n",
    "\n",
    "print(f\"\\n🏁 Completed testing all {len(memory_chains)} memory systems!\")\n",
    "print(\"📊 Results stored in tutorial_state for analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee12e43",
   "metadata": {},
   "source": [
    "Real-world applications often benefit from combining multiple memory strategies to create sophisticated context management systems that leverage the strengths of different approaches while mitigating their individual limitations. CombinedMemory allows you to orchestrate multiple memory systems simultaneously, creating layered context awareness that can handle both immediate needs and long-term relationship building.\n",
    "\n",
    "For example, you might combine ConversationBufferWindowMemory for immediate context with ConversationEntityMemory for long-term entity tracking, plus a custom memory component for domain-specific information. This creates a multi-layered memory architecture where recent interactions provide immediate context, entity memory maintains relationship continuity, and specialized memory components handle domain-specific requirements like user preferences or system configurations.\n",
    "\n",
    "Let's implement a combined memory system that demonstrates this architectural approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build a sophisticated combined memory system\n",
    "# This will use our existing memory_llm from tutorial_state\n",
    "\n",
    "print(\"🏗️ Building Combined Memory Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm (created earlier with consistent settings)\n",
    "memory_llm = tutorial_state.get(\"memory_llm\")\n",
    "\n",
    "if not memory_llm:\n",
    "    print(\"⚠️ Memory LLM not found, creating it...\")\n",
    "    memory_llm = ChatGoogleGenerativeAI(\n",
    "        model=llm.model,\n",
    "        temperature=0.2,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    tutorial_state[\"memory_llm\"] = memory_llm\n",
    "\n",
    "# Create individual memory components\n",
    "print(\"\\n1️⃣ Setting up memory components...\")\n",
    "\n",
    "# Recent Memory - immediate context\n",
    "recent_memory = ConversationBufferWindowMemory(\n",
    "    k=2,\n",
    "    memory_key=\"recent_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "print(\"   ✅ Recent Memory (last 2 turns)\")\n",
    "\n",
    "# Entity Tracker - long-term relationships\n",
    "entity_tracker = ConversationEntityMemory(\n",
    "    llm=memory_llm,  # Reusing our memory_llm\n",
    "    entity_store=InMemoryEntityStore(),\n",
    "    memory_key=\"entities\",\n",
    "    return_messages=False\n",
    ")\n",
    "print(\"   ✅ Entity Tracker (people, projects, companies)\")\n",
    "\n",
    "# Preferences - user settings\n",
    "preferences_memory = SimpleMemory(\n",
    "    memories={\"user_preferences\": \"No specific preferences set yet\"}\n",
    ")\n",
    "print(\"   ✅ Preferences Memory (user settings)\")\n",
    "\n",
    "# Combine them all\n",
    "print(\"\\n2️⃣ Combining into unified system...\")\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Create custom prompt for combined memory\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the conversation chain using our memory_llm\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,  # Reusing our existing memory_llm\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Store in tutorial_state for reuse\n",
    "tutorial_state[\"combined_memory\"] = combined_memory\n",
    "tutorial_state[\"combined_chain\"] = combined_chain\n",
    "\n",
    "print(\"\\n✅ Combined Memory System Created!\")\n",
    "print(\"   🔄 Orchestrates: Recent context + Entity tracking + Preferences\")\n",
    "print(\"   🧠 Uses our existing memory_llm (consistent with other memory ops)\")\n",
    "print(\"   💾 Stored in tutorial_state for reuse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980c322",
   "metadata": {},
   "source": [
    "**Understanding the Architecture:** \n",
    "\n",
    "What we just created is a three-layer memory system:\n",
    "\n",
    "1. **Recent Memory** provides immediate conversational context - what was just said in the last few exchanges\n",
    "2. **Entity Tracker** maintains long-term awareness of important entities (people, companies, projects) mentioned throughout the conversation\n",
    "3. **Preferences Memory** stores user-specific settings and preferences that should persist across conversations\n",
    "\n",
    "This architecture mirrors how human memory works - we have immediate working memory for current context, long-term memory for important relationships and facts, and persistent preferences that guide our behavior.\n",
    "\n",
    "Next, let's combine these systems into a unified memory architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Memory Systems\n",
    "# Now let's orchestrate all three memory types into a unified system\n",
    "\n",
    "# Create the combined memory that coordinates all components\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Create a prompt template that utilizes all memory types\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the conversation chain with our combined memory\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"🧠 Combined Memory System Created!\")\n",
    "print(\"   🔄 Orchestrates: Recent context + Entity tracking + User preferences\")\n",
    "print(\"   📋 Custom prompt template utilizes all memory types\")\n",
    "print(\"   ⚙️  Ready for sophisticated context-aware conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c5c33",
   "metadata": {},
   "source": [
    "**How Combined Memory Works:**\n",
    "\n",
    "The `CombinedMemory` system is like having a team of specialists working together:\n",
    "\n",
    "- **Recent Memory** acts as the \"immediate context specialist\" - always aware of what just happened\n",
    "- **Entity Tracker** serves as the \"relationship specialist\" - remembering who's who and what's what across conversations  \n",
    "- **Preferences Memory** functions as the \"personalization specialist\" - maintaining user-specific settings and preferences\n",
    "\n",
    "When you ask a question, all three systems contribute their expertise:\n",
    "1. Recent memory provides immediate conversational context\n",
    "2. Entity tracker identifies relevant relationships and entities \n",
    "3. Preferences memory ensures responses align with user preferences\n",
    "\n",
    "The custom prompt template weaves all this information together, creating responses that are both contextually aware and personally relevant.\n",
    "\n",
    "Let's test this system with a realistic conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80218763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our combined memory system\n",
    "# We'll use the chain we just created and stored in tutorial_state\n",
    "\n",
    "print(\"🧪 Testing Combined Memory System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our combined chain\n",
    "combined_chain = tutorial_state.get(\"combined_chain\")\n",
    "\n",
    "if not combined_chain:\n",
    "    print(\"⚠️ Combined chain not found, please run the previous cell first\")\n",
    "else:\n",
    "    # Define test conversation\n",
    "    test_conversation = [\n",
    "        \"Hi, I'm Sarah and I prefer concise responses. I'm working on a Python project.\",\n",
    "        \"I need help with data analysis using pandas. Can you recommend some techniques?\", \n",
    "        \"Actually, I'm working with customer data for my company TechFlow Solutions.\",\n",
    "        \"Our CEO Mike Johnson wants insights on customer retention patterns.\",\n",
    "        \"Can you suggest a visualization approach for this data?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"📝 Running {len(test_conversation)} conversation turns\")\n",
    "    print(\"💡 Watch how the combined memory system:\")\n",
    "    print(\"   • Remembers Sarah prefers concise responses\")\n",
    "    print(\"   • Tracks entities (Sarah, TechFlow, Mike Johnson)\")\n",
    "    print(\"   • Maintains recent context\")\n",
    "    print()\n",
    "\n",
    "    # Process each conversation turn\n",
    "    for i, user_input in enumerate(test_conversation, 1):\n",
    "        print(f\"\\n--- Turn {i} ---\")\n",
    "        print(f\"User: {user_input}\")\n",
    "        \n",
    "        # Use our combined memory chain\n",
    "        response = combined_chain.predict(input=user_input)\n",
    "        \n",
    "        # Show brief preview\n",
    "        preview = response[:100] + \"...\" if len(response) > 100 else response\n",
    "        print(f\"Preview: {preview}\")\n",
    "        print(f\"✅ Turn {i} processed\")\n",
    "\n",
    "    print(f\"\\n🎯 Completed {len(test_conversation)} turns with combined memory!\")\n",
    "    print(\"💾 All context preserved across the conversation\")\n",
    "    \n",
    "    # Store conversation in tutorial_state\n",
    "    tutorial_state[\"combined_memory_conversation\"] = test_conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a057b",
   "metadata": {},
   "source": [
    "**Analyzing What Just Happened:**\n",
    "\n",
    "In this conversation, watch how the combined memory system demonstrated all three memory types working together:\n",
    "\n",
    "1. **Turn 1**: Sarah introduces herself and sets preferences (concise responses) - captured by preferences memory\n",
    "2. **Turn 2**: Discusses pandas and data analysis - entity memory starts tracking \"pandas\" and \"data analysis\"  \n",
    "3. **Turn 3**: Introduces \"TechFlow Solutions\" - entity memory now tracks this company\n",
    "4. **Turn 4**: Mentions \"Mike Johnson\" as CEO - entity memory connects him to TechFlow Solutions\n",
    "5. **Turn 5**: Asks about visualization - recent memory provides immediate context while entity memory maintains awareness of all the players and context\n",
    "\n",
    "This creates a conversation experience where the agent:\n",
    "- Remembers Sarah prefers concise responses (preferences)\n",
    "- Knows she works at TechFlow Solutions with CEO Mike Johnson (entities)  \n",
    "- Understands the current conversation is about customer retention visualization (recent context)\n",
    "\n",
    "Let's examine what our memory systems captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what our memory systems captured\n",
    "# And see the full picture of our reusable agent components\n",
    "\n",
    "print(\"\\n🧠 MEMORY SYSTEM ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check recent memory\n",
    "recent_memory = tutorial_state.get(\"combined_memory\")\n",
    "if recent_memory:\n",
    "    print(\"✅ Combined Memory System Active\")\n",
    "    print(\"   Components working together:\")\n",
    "    print(\"   • Recent Memory (last 2 turns)\")\n",
    "    print(\"   • Entity Tracker (relationships)\")\n",
    "    print(\"   • Preferences (user settings)\")\n",
    "\n",
    "print(\"\\n📊 REUSABLE COMPONENTS INVENTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show all our reusable components\n",
    "components_summary = {\n",
    "    \"LLM Instances\": 0,\n",
    "    \"Prompt Templates\": 0,\n",
    "    \"Chains\": 0,\n",
    "    \"Memory Systems\": 0,\n",
    "    \"Agents\": 0,\n",
    "    \"Tools\": 0\n",
    "}\n",
    "\n",
    "if \"memory_llm\" in tutorial_state:\n",
    "    components_summary[\"LLM Instances\"] += 1\n",
    "    \n",
    "if \"prompt_templates\" in tutorial_state:\n",
    "    components_summary[\"Prompt Templates\"] = len(tutorial_state[\"prompt_templates\"])\n",
    "    \n",
    "if \"chains\" in tutorial_state:\n",
    "    components_summary[\"Chains\"] = len(tutorial_state[\"chains\"])\n",
    "    \n",
    "if \"memory_systems\" in tutorial_state:\n",
    "    components_summary[\"Memory Systems\"] = len(tutorial_state[\"memory_systems\"])\n",
    "    \n",
    "if \"agents\" in tutorial_state:\n",
    "    components_summary[\"Agents\"] = len(tutorial_state[\"agents\"])\n",
    "    \n",
    "if \"tools\" in tutorial_state:\n",
    "    if \"custom_tools\" in tutorial_state[\"tools\"]:\n",
    "        components_summary[\"Tools\"] = len(tutorial_state[\"tools\"][\"custom_tools\"])\n",
    "\n",
    "print(\"\\n📈 Component Summary:\")\n",
    "for component_type, count in components_summary.items():\n",
    "    print(f\"   {component_type}: {count}\")\n",
    "\n",
    "print(\"\\n✅ Memory tutorial section completed!\")\n",
    "print(f\"💾 All components stored in tutorial_state\")\n",
    "print(f\"\udd04 Ready to be reused in subsequent sections\")\n",
    "\n",
    "print(\"\\n💡 TUTORIAL PHILOSOPHY:\")\n",
    "print(\"   Instead of creating new instances everywhere,\")\n",
    "print(\"   we build components once and reuse them throughout.\")\n",
    "print(\"   This mirrors real-world development practices!\")\n",
    "\n",
    "# Update tutorial state\n",
    "tutorial_state['memory_systems_tested'] = [\n",
    "    'ConversationBufferMemory', \n",
    "    'ConversationSummaryMemory',\n",
    "    'ConversationBufferWindowMemory', \n",
    "    'ConversationTokenBufferMemory',\n",
    "    'ConversationEntityMemory',\n",
    "    'CombinedMemory'\n",
    "]\n",
    "tutorial_state['current_section'] = 'memory_complete'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6f83a",
   "metadata": {},
   "source": [
    "The examples above demonstrate the spectrum of memory management strategies available for agentic systems. Each approach serves different purposes and excels in specific scenarios:\n",
    "\n",
    "**ConversationBufferMemory** provides perfect recall for short conversations where every detail matters, but becomes expensive in extended interactions. **ConversationSummaryMemory** enables indefinitely long conversations by maintaining key themes while sacrificing some detail. **ConversationBufferWindowMemory** offers predictable performance by keeping only recent context, ideal for task-oriented interactions. **ConversationTokenBufferMemory** provides optimal context utilization with cost control, perfect for production applications.\n",
    "\n",
    "**ConversationEntityMemory** excels at tracking relationships and building long-term understanding, while **CombinedMemory** allows sophisticated orchestration of multiple memory strategies. The choice depends on your specific requirements: conversation length, cost constraints, detail requirements, and the importance of long-term relationship building.\n",
    "\n",
    "In practice, most production agentic systems benefit from combining multiple memory approaches, using recent memory for immediate context, entity memory for relationship continuity, and token-aware management for cost control. This creates robust context management that adapts to different conversation patterns while maintaining performance and reliability.\n",
    "\n",
    "\n",
    "\n",
    "Now that our agents have sophisticated memory capabilities, let's explore how they can develop and refine specialized skills that make them even more effective at specific tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729477c1",
   "metadata": {},
   "source": [
    "#### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e400",
   "metadata": {},
   "source": [
    "### Skills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22156a",
   "metadata": {},
   "source": [
    "As we build more sophisticated agents, we quickly discover that while general-purpose language models are incredibly versatile, they often lack the specialized expertise needed for complex, domain-specific tasks. This is where the concept of \"skills\" becomes crucial—they're like giving your agent professional training in specific areas.\n",
    "\n",
    "**What Are Agent Skills?** Think of skills as specialized capabilities that combine prompts, tools, memory patterns, and domain knowledge to excel at specific types of problems. Just like a human expert develops specialized skills over years of practice, we can build focused capabilities that allow our agents to perform at expert levels in particular domains.\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fddd7e6e572ad0b6a943cacefe957248455f6d522-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F191bf5dd4b6f8cfe6f1ebafe6243dd1641ed231c-1650x1069.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F441b9f6cc0d2337913c1f41b05357f16f51f702e-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "**Real-World Examples:**\n",
    "- A **financial analysis skill** might combine market data tools, statistical calculation capabilities, and specialized prompts for interpreting economic indicators\n",
    "- A **creative writing skill** could integrate research tools, style guidelines, and iterative refinement processes  \n",
    "- A **technical debugging skill** might include code analysis tools, documentation search, and systematic troubleshooting approaches\n",
    "\n",
    "**Why Skills Matter for Your Agents:**\n",
    "\n",
    "- **Specialization**: Agents can develop deep expertise in specific areas rather than being mediocre generalists\n",
    "- **Consistency**: Similar problems are approached with proven, refined techniques that improve over time\n",
    "- **Reusability**: Successful skill patterns can be applied across different contexts and even shared between agents\n",
    "- **Composability**: Complex workflows where multiple skills collaborate to solve multifaceted problems\n",
    "\n",
    "**The Challenges to Consider:** Skills also introduce challenges you need to be aware of:\n",
    "- **Over-specialization** where agents become inflexible outside their trained domains\n",
    "- **Complexity** that makes systems harder to debug and maintain\n",
    "- **Coordination overhead** when multiple skills need to work together effectively\n",
    "\n",
    "The key is finding the right balance between specialization and flexibility for your specific use case. Let's build a practical skills system to see these concepts in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class SkillResult:\n",
    "    \"\"\"Result of executing a skill -  our existing dataclass patterns\"\"\"\n",
    "    success: bool\n",
    "    output: str\n",
    "    confidence: float\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class BaseSkill(ABC):\n",
    "    \"\"\"\n",
    "    Base class for agent skills\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str, llm_instance=None):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.execution_count = 0\n",
    "        \n",
    "        self.llm = llm_instance or llm  # Falls back to our global LLM\n",
    "        \n",
    "        if \"skills\" not in tutorial_state:\n",
    "            tutorial_state[\"skills\"] = {}\n",
    "        tutorial_state[\"skills\"][name] = self\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        \"\"\"Execute the skill with given input\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get skill metadata and performance stats\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description, \n",
    "            \"executions\": self.execution_count,\n",
    "            \"llm_model\": self.llm.model if hasattr(self.llm, 'model') else 'unknown'\n",
    "        }\n",
    "\n",
    "class FinancialAnalysisSkill(BaseSkill):\n",
    "    def __init__(self, llm_instance=None):\n",
    "        super().__init__(\n",
    "            name=\"Financial Analysis\",\n",
    "            description=\"Analyze financial data and provide investment insights\",\n",
    "            llm_instance=llm_instance\n",
    "        )\n",
    "        \n",
    "        # Notice how this follows the same structure as our basic_template\n",
    "        self.analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"analysis_type\"],\n",
    "            template=\"\"\"You are a senior financial analyst with expertise in investment analysis.\n",
    "            \n",
    "            Data to analyze: {data}\n",
    "            Analysis type: {analysis_type}\n",
    "            \n",
    "            Provide a comprehensive analysis including:\n",
    "            1. Key metrics interpretation\n",
    "            2. Risk assessment (mathematical risk calculation where Risk = σ²/μ for volatility)\n",
    "            3. Investment recommendation\n",
    "            4. Confidence level (1-10)\n",
    "            \n",
    "            Focus on actionable insights and clearly explain your reasoning.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create a reusable chain using our established pattern\n",
    "        self.analysis_chain = self.analysis_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        print(f\"💰 Financial Analysis Skill initialized using existing LLM\")\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        \n",
    "        # Default analysis type if not provided in context\n",
    "        analysis_type = context.get(\"analysis_type\", \"general financial analysis\") if context else \"general financial analysis\"\n",
    "        \n",
    "        try:\n",
    "            result = self.analysis_chain.invoke({\n",
    "                \"data\": input_data,\n",
    "                \"analysis_type\": analysis_type\n",
    "            })\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=0.85,\n",
    "                metadata={\n",
    "                    \"skill_name\": self.name,\n",
    "                    \"analysis_type\": analysis_type,\n",
    "                    \"execution_number\": self.execution_count\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Analysis failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "\n",
    "# Research Skill - Builds on existing search capabilities\n",
    "class ResearchSkill(BaseSkill):\n",
    "    def __init__(self, llm_instance=None):\n",
    "        super().__init__(\n",
    "            name=\"Research Assistant\",\n",
    "            description=\"Conduct thorough research on any topic\",\n",
    "            llm_instance=llm_instance\n",
    "        )\n",
    "        \n",
    "        self.research_prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\", \"depth\"],\n",
    "            template=\"\"\"You are a thorough research assistant with access to comprehensive knowledge.\n",
    "            \n",
    "            Research Topic: {topic}\n",
    "            Research Depth: {depth}\n",
    "            \n",
    "            Provide a well-structured research report including:\n",
    "            1. Executive summary\n",
    "            2. Key findings and facts\n",
    "            3. Different perspectives or viewpoints\n",
    "            4. Relevant data and statistics\n",
    "            5. Conclusions and implications\n",
    "            \n",
    "            Make your research {depth} and cite reasoning for your conclusions.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.research_chain = self.research_prompt | self.llm | StrOutputParser()\n",
    "        print(f\"🔍 Research Skill initialized using existing LLM\")\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        \n",
    "        depth = context.get(\"depth\", \"comprehensive\") if context else \"comprehensive\"\n",
    "        \n",
    "        try:\n",
    "            result = self.research_chain.invoke({\n",
    "                \"topic\": input_data,\n",
    "                \"depth\": depth\n",
    "            })\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=0.8,\n",
    "                metadata={\n",
    "                    \"skill_name\": self.name,\n",
    "                    \"research_depth\": depth,\n",
    "                    \"execution_number\": self.execution_count\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Research failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "\n",
    "# Create skills using our global LLM instead of new instances\n",
    "financial_skill = FinancialAnalysisSkill(llm_instance=llm)\n",
    "research_skill = ResearchSkill(llm_instance=llm)\n",
    "\n",
    "# Store skills registry in tutorial state for easy access later\n",
    "tutorial_state[\"active_skills\"] = {\n",
    "    \"financial\": financial_skill,\n",
    "    \"research\": research_skill\n",
    "}\n",
    "\n",
    "print(\"\\n✅ SKILLS SYSTEM READY\")\n",
    "print(\"🔄 All skills use the same LLM instance (memory efficient)\")\n",
    "print(\"🔄 Prompt templates follow established patterns\")\n",
    "print(f\"🎯 {len(tutorial_state['active_skills'])} skills available\")\n",
    "\n",
    "# Quick test to show they work\n",
    "print(\"\\n🧪 Quick Skills Test:\")\n",
    "test_result = financial_skill.execute(\n",
    "    \"AAPL stock price $150, P/E ratio 25, revenue growth 8%\",\n",
    "    {\"analysis_type\": \"quick assessment\"}\n",
    ")\n",
    "print(f\"Financial skill test: {'✅ Success' if test_result.success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beac902",
   "metadata": {},
   "source": [
    "### Workflows and Chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372add07",
   "metadata": {},
   "source": [
    "Now that we've mastered the building blocks of agentic systems—prompts, tools, memory, and skills—it's time to explore how we orchestrate these components into sophisticated workflows.\n",
    "\n",
    "**Think of Workflows as Choreography:** I like to think of workflows as the \"choreography\" of your agentic system. Just like a ballet performance, they define how different components interact, when they execute, and how information flows between them. Without good choreography, even the most talented individual performers can't create something beautiful together.\n",
    "\n",
    "**The Transformation:** Workflows transform simple LLM interactions into powerful, multi-step reasoning systems. Instead of asking an LLM to solve a complex problem in one shot (which often leads to mediocre results), workflows break down tasks into manageable pieces, allowing for specialization, validation, and iterative improvement.\n",
    "\n",
    "Here's why this matters so much:\n",
    "\n",
    "**Why Workflows Are Game-Changers:**\n",
    "\n",
    "1. **Task Decomposition**: Complex problems become manageable when broken into smaller, focused steps. Instead of \"write a marketing campaign,\" you might have \"research audience → generate concepts → create copy → review and refine.\"\n",
    "\n",
    "2. **Specialization**: Different parts of your system can excel at different aspects of the problem. Your research specialist can be different from your creative writer, each optimized for their specific role.\n",
    "\n",
    "3. **Quality Control**: You can add validation and error checking at each step. If the research step fails, you catch it before moving to content generation.\n",
    "\n",
    "4. **Scalability**: Parallel execution and efficient resource utilization mean you can handle more complex tasks without proportional increases in time.\n",
    "\n",
    "5. **Maintainability**: It's easier to debug, test, and improve individual components rather than trying to fix one monolithic prompt.\n",
    "\n",
    "**Understanding the Spectrum:** Workflows exist on a spectrum from simple sequential chains to fully autonomous agents:\n",
    "\n",
    "```\n",
    "Simple → Sequential → Parallel → Dynamic → Autonomous\n",
    "Chain     Routing     Execution   Orchestration   Agents\n",
    "```\n",
    "\n",
    "Each level adds complexity but also capability. The key is choosing the right level for your specific use case—sometimes a simple chain is perfect, other times you need full autonomy.\n",
    "\n",
    "**What We'll Build Together:** We'll start with basic prompt chaining, then work our way up to intelligent routing systems, parallel execution patterns, and eventually full autonomous agents. Each step builds on the previous one, so you'll understand not just how to build these systems, but when and why to use each approach.\n",
    "\n",
    "Building on Anthropic's foundational patterns, we can implement more sophisticated agentic systems that combine multiple workflows and demonstrate emergent behaviors. These advanced patterns represent the cutting edge of production agentic systems.\n",
    "\n",
    "##### Mathematical Foundations of Workflow Optimization\n",
    "\n",
    "**Error Propagation in Chains**: In prompt chaining, if each step has error rate ε, the cumulative error follows: \n",
    "$$E_{total} = 1 - \\prod_{i=1}^{n}(1-\\varepsilon_i)$$\n",
    "\n",
    "For identical error rates: $E_{total} = 1 - (1-\\varepsilon)^n$\n",
    "\n",
    "**Parallel Processing Speedup**: Theoretical speedup from parallelization follows Amdahl's Law:\n",
    "$$S = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where P is the parallelizable fraction and N is the number of processors.\n",
    "\n",
    "**Consensus Accuracy**: For voting systems with individual accuracy p, ensemble accuracy follows:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Iterative Improvement**: Quality improvement in evaluator-optimizer workflows can be modeled as:\n",
    "$$Q_n = Q_0 \\cdot (1 + \\alpha \\cdot \\beta^n)$$\n",
    "\n",
    "Where α is the improvement factor and β is the diminishing returns coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45774676",
   "metadata": {},
   "source": [
    "#### 1. Prompt Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7d88c",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ff0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Our First Workflow: Prompt Chaining System\n",
    "# We'll use our existing LLM to create a sequential workflow\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import SequentialChain\n",
    "import time\n",
    "\n",
    "print(\"🔗 PROMPT CHAINING WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what LLM instances we have available\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "class PromptChain:\n",
    "    \"\"\"\n",
    "    Sequential workflow system using our existing LLM\n",
    "    \n",
    "    This is like a factory assembly line where each step:\n",
    "    - Takes output from the previous step\n",
    "    - Performs focused transformation\n",
    "    - Passes result to next step\n",
    "    \n",
    "    Key insight: We reuse the same LLM throughout the chain,\n",
    "    just with different prompts for each step!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        self.llm = llm_instance\n",
    "        self.steps_executed = 0\n",
    "        print(f\"🏗️ Prompt Chain initialized\")\n",
    "        print(f\"   Using LLM: {self.llm.model}\")\n",
    "        print(f\"   Temperature: {self.llm.temperature}\")\n",
    "        \n",
    "    def create_step(self, name: str, instruction: str, gate_check=None):\n",
    "        \"\"\"\n",
    "        Define a step in our chain\n",
    "        \n",
    "        Args:\n",
    "            name: Step identifier for tracking\n",
    "            instruction: What this step should do\n",
    "            gate_check: Optional validation function\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"instruction\": instruction,\n",
    "            \"gate_check\": gate_check\n",
    "        }\n",
    "    \n",
    "    def execute_step(self, step, input_text):\n",
    "        \"\"\"\n",
    "        Execute a single step using our LLM\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Executing: {step['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Quality gate check\n",
    "        if step.get('gate_check') and not step['gate_check'](input_text):\n",
    "            print(f\"❌ Gate check failed for {step['name']}\")\n",
    "            return None\n",
    "            \n",
    "        # Create prompt for this step\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"instruction\"],\n",
    "            template=\"\"\"Task: {instruction}\n",
    "\n",
    "Input: {input}\n",
    "\n",
    "Provide a clear, focused response that can be used as input for the next step in the workflow.\n",
    "Be thorough but concise - the next step depends on your output quality.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Execute using our LLM\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"input\": input_text,\n",
    "            \"instruction\": step[\"instruction\"]\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        self.steps_executed += 1\n",
    "        \n",
    "        print(f\"✅ Completed in {execution_time:.2f}s\")\n",
    "        print(f\"   Output: {len(result)} characters\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create or reuse prompt chain\n",
    "if 'prompt_chain' not in tutorial_state:\n",
    "    prompt_chain = PromptChain(memory_llm)\n",
    "    tutorial_state['prompt_chain'] = prompt_chain\n",
    "    print(\"\\n✅ New Prompt Chain created\")\n",
    "else:\n",
    "    prompt_chain = tutorial_state['prompt_chain']\n",
    "    print(f\"\\n✅ Reusing existing Prompt Chain\")\n",
    "    print(f\"   Steps executed so far: {prompt_chain.steps_executed}\")\n",
    "\n",
    "print(\"\\n💡 This chain will reuse our memory_llm for all steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our prompt chain is already initialized in the previous cell\n",
    "# Let's verify it's ready and show what we have\n",
    "\n",
    "print(\"🔍 Verifying Workflow System Status\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if prompt_chain:\n",
    "    print(\"✅ Prompt Chain System Ready\")\n",
    "    print(f\"   LLM Model: {prompt_chain.llm.model}\")\n",
    "    print(f\"   Temperature: {prompt_chain.llm.temperature}\")\n",
    "    print(f\"   Steps executed: {prompt_chain.steps_executed}\")\n",
    "    print(f\"   Status: Ready for sequential workflows\")\n",
    "else:\n",
    "    print(\"⚠️ Prompt chain not found, please run previous cell\")\n",
    "\n",
    "print(\"\\n💡 Ready to build and execute sequential workflows!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ba278",
   "metadata": {},
   "source": [
    "Now Let's Build and Test Our First Chain\n",
    "We'll create a practical workflow for marketing copy that demonstrates all the key concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build and execute a marketing workflow using our existing prompt chain\n",
    "# Notice how we reuse the chain we created earlier\n",
    "\n",
    "print(\"📝 BUILDING MARKETING WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our prompt chain from tutorial_state\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if not prompt_chain:\n",
    "    print(\"⚠️ Prompt chain not initialized. Please run previous cells.\")\n",
    "else:\n",
    "    print(f\"✅ Using existing Prompt Chain (executed {prompt_chain.steps_executed} steps so far)\")\n",
    "    \n",
    "    # Define our workflow steps\n",
    "    print(\"\\n🔧 Defining workflow steps...\")\n",
    "    \n",
    "    # Step 1: Content Creation\n",
    "    content_step = prompt_chain.create_step(\n",
    "        \"content_creation\",\n",
    "        \"Create compelling marketing copy for a new AI productivity tool. Focus on benefits for busy professionals and include a strong call-to-action.\"\n",
    "    )\n",
    "    print(\"   1. Content Creation ○\")\n",
    "    \n",
    "    # Step 2: Quality Review with Gate Check\n",
    "    quality_step = prompt_chain.create_step(\n",
    "        \"quality_review\", \n",
    "        \"Review this marketing copy for clarity, persuasiveness, and professional tone. Improve grammar and strengthen the value proposition.\",\n",
    "        gate_check=lambda x: len(x) > 50 and len(x.split()) > 10\n",
    "    )\n",
    "    print(\"   2. Quality Review ✓ (with gate check)\")\n",
    "    \n",
    "    # Step 3: Translation\n",
    "    translation_step = prompt_chain.create_step(\n",
    "        \"translation\",\n",
    "        \"Translate this marketing copy to Spanish while maintaining tone and persuasiveness.\"\n",
    "    )\n",
    "    print(\"   3. Translation ○\")\n",
    "    \n",
    "    # Execute the workflow\n",
    "    steps = [content_step, quality_step, translation_step]\n",
    "    print(f\"\\n\ude80 EXECUTING WORKFLOW ({len(steps)} steps)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    current_input = \"AI productivity tool for busy professionals\"\n",
    "    results = []\n",
    "    \n",
    "    for i, step in enumerate(steps, 1):\n",
    "        print(f\"\\n--- Step {i}: {step['name']} ---\")\n",
    "        print(f\"Input: {current_input[:60]}...\")\n",
    "        \n",
    "        # Execute using our reusable prompt chain\n",
    "        result = prompt_chain.execute_step(step, current_input)\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"❌ Chain terminated due to step failure\")\n",
    "            break\n",
    "        \n",
    "        results.append({\n",
    "            \"step_number\": i,\n",
    "            \"step_name\": step['name'],\n",
    "            \"input_length\": len(current_input),\n",
    "            \"output_length\": len(result),\n",
    "            \"output_preview\": result[:100] + \"...\"\n",
    "        })\n",
    "        \n",
    "        # Output becomes next input\n",
    "        current_input = result\n",
    "    \n",
    "    # Store results\n",
    "    tutorial_state['chain_results'] = results\n",
    "    tutorial_state['latest_workflow_output'] = current_input\n",
    "    \n",
    "    print(f\"\\n✅ Workflow completed: {len(results)} steps executed\")\n",
    "    print(f\"📊 Total steps by this chain: {prompt_chain.steps_executed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze our workflow execution results\n",
    "print(\"📊 WORKFLOW EXECUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get results from tutorial_state\n",
    "results = tutorial_state.get('chain_results', [])\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n✅ Successfully completed {len(results)} steps\")\n",
    "    \n",
    "    print(\"\\n📈 Content Evolution:\")\n",
    "    for result in results:\n",
    "        print(f\"  Step {result['step_number']} - {result['step_name']}:\")\n",
    "        print(f\"    Input → Output: {result['input_length']} → {result['output_length']} chars\")\n",
    "        print(f\"    Preview: {result['output_preview']}\")\n",
    "    \n",
    "    # Show final output\n",
    "    final_output = tutorial_state.get('latest_workflow_output')\n",
    "    if final_output:\n",
    "        print(f\"\\n📝 Final Output Preview:\")\n",
    "        print(f\"   {final_output[:150]}...\")\n",
    "    \n",
    "    # Show chain stats\n",
    "    if prompt_chain:\n",
    "        print(f\"\\n📊 Chain Statistics:\")\n",
    "        print(f\"   Total steps executed by this chain: {prompt_chain.steps_executed}\")\n",
    "        print(f\"   LLM reused throughout: {prompt_chain.llm.model}\")\n",
    "    \n",
    "    print(\"\\n💡 Key Insight: One LLM, multiple transformations!\")\n",
    "    print(\"   We didn't create new LLM instances for each step\")\n",
    "    print(\"   We reused the same one with different prompts\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No workflow results found. Please run previous cell.\")\n",
    "\n",
    "print(\"\\n✅ Results stored in tutorial_state for further analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e0d2b",
   "metadata": {},
   "source": [
    "#### 2. Routing Workflows - Intelligent Task Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fcf46",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d6c86",
   "metadata": {},
   "source": [
    "Now let's explore routing workflows, which intelligently classify inputs and direct them to specialized handlers. Think of it as a smart switchboard that sends different types of requests to the most appropriate specialist.\n",
    "\n",
    "**The Problem Routing Solves:**\n",
    "\n",
    "Imagine building a customer service system. You could create one massive prompt that tries to handle all types of inquiries, but this leads to:\n",
    "- Generic responses that aren't specialized enough\n",
    "- Conflicting optimization (improving billing support might hurt technical support)\n",
    "- Difficulty in maintaining and improving specific areas\n",
    "\n",
    "**How Routing Works:**\n",
    "\n",
    "1. **Classification**: Analyze the input to determine its type/category\n",
    "2. **Route Selection**: Choose the appropriate specialized handler\n",
    "3. **Execution**: Process using the selected specialist\n",
    "4. **Response**: Return the specialized result\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "Routing leverages the principle of **specialization gains**. If we have accuracy A_general for a general system and A_specialized for specialists, routing achieves:\n",
    "\n",
    "$$Accuracy_{routed} = \\sum_{i} P(category_i) \\times A_{specialist_i}$$\n",
    "\n",
    "Where P(category_i) is the probability of correct classification.\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Specialization**: Each route can be optimized for specific input types\n",
    "- **Maintainability**: Update one route without affecting others\n",
    "- **Performance**: Use different models/strategies per route (fast vs. accurate)\n",
    "- **Cost Optimization**: Route simple queries to cheaper models\n",
    "\n",
    "Let's build a routing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an Intelligent Routing System\n",
    "\n",
    "class IntelligentRouter:\n",
    "    \"\"\"\n",
    "    An intelligent routing system that acts like a smart receptionist.\n",
    "    \n",
    "    and extend our existing prompt patterns instead of creating everything from scratch.\n",
    "    \n",
    "    This approach shows:\n",
    "    - How to build upon existing components\n",
    "    - Maintaining consistency across the codebase\n",
    "    - Reducing memory usage and initialization time\n",
    "    - Making the tutorial flow more logical and connected\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance=None):\n",
    "        self.llm = llm_instance or llm  # Falls back to global llm\n",
    "        self.routes = {}\n",
    "        print(\"🎯 Initializing intelligent routing system using existing LLM...\")\n",
    "        \n",
    "        # Notice how we're extending the structure we already established\n",
    "        self.router_prompt = PromptTemplate(\n",
    "            input_variables=[\"input_text\", \"available_routes\"],\n",
    "            template=\"\"\"You are an intelligent classification system. Your job is to analyze the input and determine which specialist should handle it.\n",
    "\n",
    "Input to classify: {input_text}\n",
    "\n",
    "Available specialists:\n",
    "{available_routes}\n",
    "\n",
    "CRITICAL: Respond with ONLY the route name that best matches the input type. \n",
    "No explanation, no extra text - just the exact route name.\n",
    "If unsure, choose the most general route available.\"\"\"\n",
    "        )\n",
    "        \n",
    "        tutorial_state[\"routers\"] = tutorial_state.get(\"routers\", {})\n",
    "        tutorial_state[\"routers\"][\"main_router\"] = self\n",
    "        \n",
    "        print(\"🔄 Router initialized and stored in tutorial_state\")\n",
    "    \n",
    "    def register_route(self, name, description, template=None, confidence=0.8):\n",
    "        \"\"\"\n",
    "        Register a new specialist route.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if template is None:\n",
    "            # Check if we have a suitable existing template\n",
    "            existing_templates = tutorial_state.get(\"prompt_templates\", {})\n",
    "            if \"basic\" in existing_templates:\n",
    "                print(f\"🔄 Reusing existing basic template for route '{name}'\")\n",
    "                template = existing_templates[\"basic\"]\n",
    "            else:\n",
    "                # Fallback: create a simple template\n",
    "                template = PromptTemplate(\n",
    "                    input_variables=[\"input\"],\n",
    "                    template=\"Handle this request: {input}\"\n",
    "                )\n",
    "        \n",
    "        self.routes[name] = {\n",
    "            \"description\": description,\n",
    "            \"template\": template,\n",
    "            \"confidence\": confidence,\n",
    "            \"usage_count\": 0  # Track how often this route is used\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def route(self, input_text: str):\n",
    "        \"\"\"\n",
    "        Route input to the appropriate specialist\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.routes:\n",
    "            return \"No routes registered. Please register routes first.\"\n",
    "        \n",
    "        # Build available routes description for the classifier\n",
    "        routes_desc = \"\\n\".join([\n",
    "            f\"- {name}: {route['description']}\" \n",
    "            for name, route in self.routes.items()\n",
    "        ])\n",
    "        \n",
    "        router_chain = self.router_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # Get the route decision\n",
    "            chosen_route = router_chain.invoke({\n",
    "                \"input_text\": input_text,\n",
    "                \"available_routes\": routes_desc\n",
    "            }).strip()\n",
    "            \n",
    "            # Validate the route exists\n",
    "            if chosen_route in self.routes:\n",
    "                # Update usage stats\n",
    "                self.routes[chosen_route][\"usage_count\"] += 1\n",
    "                return chosen_route\n",
    "            else:\n",
    "                # Fallback to first available route\n",
    "                fallback_route = list(self.routes.keys())[0]\n",
    "                print(f\"⚠️ Route '{chosen_route}' not found, using fallback: {fallback_route}\")\n",
    "                return fallback_route\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Routing error: {e}\")\n",
    "            return list(self.routes.keys())[0] if self.routes else None\n",
    "\n",
    "print(\"🚀 Creating Intelligent Router using existing components...\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our global LLM instead of creating a new one\n",
    "intelligent_router = IntelligentRouter(llm_instance=llm)\n",
    "\n",
    "# Register some routes reusing our existing templates\n",
    "print(\"\\n📝 Registering routes with existing templates...\")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"general_chat\",\n",
    "    description=\"General conversation and questions\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"chat\"],\n",
    "    confidence=0.7\n",
    ")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"explanation\", \n",
    "    description=\"Detailed explanations of concepts and topics\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"basic\"],\n",
    "    confidence=0.9\n",
    ")\n",
    "\n",
    "# Register a specialized route (will create new template only if needed)\n",
    "intelligent_router.register_route(\n",
    "    name=\"technical_analysis\",\n",
    "    description=\"Technical analysis and code-related questions\",\n",
    "    confidence=0.8\n",
    ")\n",
    "\n",
    "print(\"\\n✅ ROUTING SYSTEM READY\")\n",
    "print(\"📦 Router stored in tutorial_state for future use\")\n",
    "print(f\"🎯 {len(intelligent_router.routes)} routes registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c21fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our routing system using the global llm\n",
    "# This router will intelligently direct queries to specialists\n",
    "\n",
    "print(\"🎯 Initializing Intelligent Router\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if router already exists\n",
    "if 'router' not in tutorial_state:\n",
    "    # Create router using our global llm\n",
    "    router = IntelligentRouter(llm_instance=llm)\n",
    "    tutorial_state['router'] = router\n",
    "    print(\"✅ New router created using global llm\")\n",
    "else:\n",
    "    router = tutorial_state['router']\n",
    "    print(\"✅ Using existing router from tutorial_state\")\n",
    "\n",
    "print(f\"🔧 Router uses: {router.llm.model}\")\n",
    "print(f\"🌡️  Temperature: {router.llm.temperature}\")\n",
    "print(\"💡 Ready to register specialist routes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Our Specialist Team - Customer Service Routes\n",
    "# Let's build a realistic customer service system with three different specialists\n",
    "\n",
    "print(\"🏗️ BUILDING OUR SPECIALIST TEAM\")\n",
    "print(\"We're creating a customer service system with different experts...\")\n",
    "\n",
    "# Specialist #1: Technical Support Expert\n",
    "# This route handles complex technical issues that need systematic troubleshooting\n",
    "print(\"\\n👨‍💻 Registering Technical Support Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"technical_support\",\n",
    "    description=\"Technical issues, software bugs, troubleshooting, error messages, crashes, performance problems\",\n",
    "    template=\"\"\"You are a senior technical support specialist with deep expertise in software troubleshooting.\n",
    "\n",
    "TECHNICAL ISSUE: {input}\n",
    "\n",
    "Provide systematic troubleshooting guidance following this structure:\n",
    "\n",
    "🔍 DIAGNOSIS:\n",
    "- Ask key diagnostic questions to understand the issue\n",
    "- Identify likely root causes\n",
    "\n",
    "🛠️ SOLUTION STEPS:\n",
    "1. [First step - usually the simplest fix]\n",
    "2. [Progressive steps if needed]\n",
    "3. [Advanced troubleshooting if required]\n",
    "\n",
    "🛡️ PREVENTION:\n",
    "- How to prevent this issue in the future\n",
    "- Best practices to follow\n",
    "\n",
    "⚠️ ESCALATION CRITERIA:\n",
    "- When to contact advanced support\n",
    "- What information to include\n",
    "\n",
    "Be technical but explain concepts clearly. Focus on actionable solutions.\"\"\",\n",
    "    confidence=0.9\n",
    ")\n",
    "\n",
    "# Specialist #2: Billing Support Expert  \n",
    "# This route handles money matters with empathy and clear policy explanations\n",
    "print(\"\\n💳 Registering Billing Support Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"billing_support\", \n",
    "    description=\"Payment issues, subscription questions, refunds, billing errors, account charges, invoices\",\n",
    "    template=\"\"\"You are a billing specialist focused on resolving payment and subscription issues with empathy and clarity.\n",
    "\n",
    "BILLING INQUIRY: {input}\n",
    "\n",
    "Handle this systematically:\n",
    "\n",
    "🔍 ACCOUNT VERIFICATION:\n",
    "- What account information to verify\n",
    "- Security questions to ask\n",
    "\n",
    "💡 ISSUE ANALYSIS:\n",
    "- Clear explanation of what happened\n",
    "- Why the charge/issue occurred\n",
    "\n",
    "✅ RESOLUTION STEPS:\n",
    "- Specific actions to resolve the issue\n",
    "- Timeline for resolution\n",
    "- Follow-up required\n",
    "\n",
    "📋 POLICY INFORMATION:\n",
    "- Relevant billing policies\n",
    "- Customer rights and options\n",
    "\n",
    "Be empathetic, solution-focused, and always explain billing policies in simple terms.\"\"\",\n",
    "    confidence=0.85\n",
    ")\n",
    "\n",
    "# Specialist #3: General Inquiry Handler\n",
    "# This is our friendly generalist who handles everything else\n",
    "print(\"\\n🤝 Registering General Inquiry Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"general_inquiry\",\n",
    "    description=\"Product information, feature questions, general support, how-to questions, account management\", \n",
    "    template=\"\"\"You are a friendly and knowledgeable customer service representative handling general inquiries.\n",
    "\n",
    "CUSTOMER QUESTION: {input}\n",
    "\n",
    "Provide comprehensive help:\n",
    "\n",
    "💡 DIRECT ANSWER:\n",
    "- Clear, specific answer to their question\n",
    "- Include relevant details they might need\n",
    "\n",
    "📚 ADDITIONAL INFORMATION:\n",
    "- Related features or information that might help\n",
    "- Tips for getting the most value\n",
    "\n",
    "🔗 HELPFUL RESOURCES:\n",
    "- Where to find more information\n",
    "- Related documentation or tutorials\n",
    "\n",
    "➡️ NEXT STEPS:\n",
    "- What they can do next\n",
    "- How to get additional help if needed\n",
    "\n",
    "Be friendly, comprehensive, and proactive in providing value beyond just answering the question.\"\"\",\n",
    "    confidence=0.75\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display our registered team\n",
    "print(f\"\\n✅ SPECIALIST TEAM ASSEMBLED\")\n",
    "print(f\"Total specialists registered: {len(router.routes)}\")\n",
    "\n",
    "# Let's see what we've built\n",
    "print(f\"\\n📊 TEAM ROSTER:\")\n",
    "for route_name, route_info in router.routes.items():\n",
    "    print(f\"   🎯 {route_name}\")\n",
    "    print(f\"      Confidence: {route_info['confidence']}\")\n",
    "    print(f\"      Usage: {route_info['usage_count']} times\")\n",
    "    print(f\"      Specialty: {route_info['description'][:60]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"🚀 Ready to start routing customer inquiries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b28df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete routing workflow using our existing router\n",
    "# This demonstrates the full cycle with our pre-configured system\n",
    "\n",
    "def route_and_process(input_text):\n",
    "    \"\"\"\n",
    "    Process a query using our existing router from tutorial_state\n",
    "    This function reuses our configured router and its registered specialists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get our router from tutorial_state\n",
    "    router = tutorial_state.get('router')\n",
    "    \n",
    "    if not router or not router.routes:\n",
    "        print(\"❌ Router not configured. Please register routes first.\")\n",
    "        return {\n",
    "            \"route\": \"unhandled\",\n",
    "            \"result\": \"Router not initialized\",\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "    \n",
    "    print(f\"📨 PROCESSING WITH EXISTING ROUTER\")\n",
    "    print(f\"Input: '{input_text[:60]}{'...' if len(input_text) > 60 else ''}'\")\n",
    "    \n",
    "    # Use our router's classification method\n",
    "    selected_route = router.route(input_text)\n",
    "    \n",
    "    if not selected_route:\n",
    "        print(\"❌ Classification failed - using fallback\")\n",
    "        return {\n",
    "            \"route\": \"unhandled\",\n",
    "            \"result\": \"Could not classify query\",\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "    \n",
    "    print(f\"🎯 Routed to: {selected_route}\")\n",
    "    \n",
    "    # Get specialist configuration\n",
    "    route_config = router.routes[selected_route]\n",
    "    \n",
    "    # Process with specialist's template\n",
    "    route_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=route_config[\"template\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"⚙️ Processing with {selected_route} specialist...\")\n",
    "    \n",
    "    # Use the router's llm (same as our global llm)\n",
    "    chain = route_prompt | router.llm | StrOutputParser()\n",
    "    result = chain.invoke({\"input\": input_text})\n",
    "    \n",
    "    # Update usage statistics\n",
    "    router.routes[selected_route][\"usage_count\"] += 1\n",
    "    \n",
    "    return {\n",
    "        \"route\": selected_route,\n",
    "        \"result\": result,\n",
    "        \"confidence\": route_config[\"confidence\"],\n",
    "        \"usage_count\": router.routes[selected_route][\"usage_count\"]\n",
    "    }\n",
    "\n",
    "print(\"✅ Routing function ready\")\n",
    "print(\"💡 Uses the router we configured earlier in tutorial_state\")\n",
    "print(\"🔄 No need to create new instances - we reuse what we built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f28f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Suite: Real Customer Inquiries\n",
    "# Let's test our routing system with realistic customer service scenarios\n",
    "print(f\"\\n🧪 COMPREHENSIVE ROUTING TEST SUITE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# These are real-world examples that show different types of customer inquiries\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Technical Issue\",\n",
    "        \"query\": \"My app keeps crashing every time I try to export a file. I get error code 500 and then it just closes. This happens on both Windows and Mac versions.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Billing Problem\", \n",
    "        \"query\": \"I was charged twice for my subscription this month and I need a refund for the duplicate charge. My card ending in 1234 shows two charges on October 15th.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Product Question\",\n",
    "        \"query\": \"What's the difference between your premium and enterprise plans? I'm trying to decide which one would be best for a team of 15 people.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Mixed Technical/Billing\",\n",
    "        \"query\": \"I upgraded to premium but I'm still seeing ads and getting limited features. Did my payment go through? How can I check my account status?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_results = []\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n--- TEST {i}: {scenario['scenario']} ---\")\n",
    "    \n",
    "    # Process the inquiry through our complete routing system\n",
    "    result = route_and_process(scenario['query'])\n",
    "    \n",
    "    # Store results for analysis\n",
    "    result['test_scenario'] = scenario['scenario']\n",
    "    result['original_query'] = scenario['query']\n",
    "    routing_results.append(result)\n",
    "    \n",
    "    # Show key metrics for this test\n",
    "    print(f\"🎯 Route: {result['route']}\")\n",
    "    print(f\"📊 Confidence: {result['confidence']}\")\n",
    "    print(f\"📝 Response preview: {result['result'][:120]}...\")\n",
    "    print(f\"📈 Specialist usage count: {result['specialist_usage']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b426786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Performance Analysis\n",
    "print(f\"\\n📊 ROUTING SYSTEM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate routing distribution\n",
    "route_distribution = {}\n",
    "for result in routing_results:\n",
    "    route = result['route']\n",
    "    route_distribution[route] = route_distribution.get(route, 0) + 1\n",
    "\n",
    "print(f\"📈 ROUTING DISTRIBUTION:\")\n",
    "for route_name, count in route_distribution.items():\n",
    "    percentage = (count / len(test_scenarios)) * 100\n",
    "    print(f\"   {route_name}: {count} queries ({percentage:.1f}%)\")\n",
    "\n",
    "# Overall system metrics\n",
    "total_confidence = sum(r['confidence'] for r in routing_results)\n",
    "avg_confidence = total_confidence / len(routing_results)\n",
    "successful_routes = len([r for r in routing_results if r['route'] != 'unhandled'])\n",
    "\n",
    "print(f\"\\n🎯 SYSTEM METRICS:\")\n",
    "print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "print(f\"   Successful routing rate: {successful_routes}/{len(test_scenarios)} ({(successful_routes/len(test_scenarios)*100):.1f}%)\")\n",
    "print(f\"   Total specialists: {len(router.routes)}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "tutorial_state['routing_results'] = routing_results\n",
    "tutorial_state['routing_metrics'] = {\n",
    "    'distribution': route_distribution,\n",
    "    'avg_confidence': avg_confidence,\n",
    "    'success_rate': successful_routes / len(test_scenarios)\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ ROUTING SYSTEM TESTING COMPLETE\")\n",
    "print(\"All results saved to tutorial_state for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea4419",
   "metadata": {},
   "source": [
    "#### 3. Parallelization Workflows - Speed and Consensus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd24e58",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec847a",
   "metadata": {},
   "source": [
    "Parallelization is where things get interesting. Instead of processing sequentially, we can execute multiple tasks simultaneously, either to **divide the work** (sectioning) or to get **multiple perspectives** (voting). This is crucial for production systems where speed and accuracy both matter.\n",
    "\n",
    "**Two Flavors of Parallelization:**\n",
    "\n",
    "1. **Sectioning**: Break a large task into independent parts that can run simultaneously\n",
    "   - Example: Analyzing a document from financial, legal, and technical perspectives\n",
    "   - Benefit: Speed (total time = max individual time, not sum)\n",
    "\n",
    "2. **Voting**: Run the same task multiple times to reach consensus  \n",
    "   - Example: Multiple models evaluating content safety\n",
    "   - Benefit: Accuracy through ensemble effects\n",
    "\n",
    "**Mathematical Foundation - Amdahl's Law:**\n",
    "\n",
    "The theoretical speedup from parallelization follows:\n",
    "$$Speedup = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where:\n",
    "- P = fraction of work that can be parallelized  \n",
    "- N = number of parallel processors\n",
    "\n",
    "**Voting Accuracy (Condorcet's Jury Theorem):**\n",
    "\n",
    "If individual classifiers have accuracy p > 0.5, ensemble accuracy with n classifiers is:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "This means ensemble accuracy increases with more voters (if individual accuracy > 50%).\n",
    "\n",
    "**When to Use Parallelization:**\n",
    "- **Sectioning**: When you can identify independent subtasks\n",
    "- **Voting**: When you need high-confidence decisions\n",
    "- **Speed Requirements**: When latency is critical\n",
    "- **Quality Requirements**: When accuracy is paramount\n",
    "\n",
    "Let's implement both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9896afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing with our existing LLM\n",
    "# We'll reuse our memory_llm for parallel task execution\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "print(\"⚡ PARALLEL PROCESSING SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ParallelProcessor:\n",
    "    \"\"\"\n",
    "    Execute multiple tasks in parallel using our existing LLM\n",
    "    \n",
    "    Key insight: We use the SAME LLM for all parallel tasks,\n",
    "    but execute them simultaneously in different threads!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        self.llm = llm_instance\n",
    "        self.tasks_executed = 0\n",
    "        print(f\"⚡ Parallel Processor initialized\")\n",
    "        print(f\"   Using LLM: {self.llm.model}\")\n",
    "        print(f\"   Temperature: {self.llm.temperature}\")\n",
    "        \n",
    "    def create_section_task(self, name, focus_area, analysis_prompt):\n",
    "        \"\"\"Define a parallel task section\"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"focus\": focus_area,\n",
    "            \"prompt_template\": analysis_prompt\n",
    "        }\n",
    "    \n",
    "    def execute_section(self, task, input_data):\n",
    "        \"\"\"Execute one section using our LLM\"\"\"\n",
    "        print(f\"🔄 Processing: {task['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create prompt for this section\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"focus\"],\n",
    "            template=task[\"prompt_template\"]\n",
    "        )\n",
    "        \n",
    "        # Execute using our shared LLM\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"data\": input_data,\n",
    "            \"focus\": task[\"focus\"]\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        self.tasks_executed += 1\n",
    "        \n",
    "        print(f\"✅ '{task['name']}' done in {execution_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            \"section\": task[\"name\"],\n",
    "            \"focus\": task[\"focus\"],\n",
    "            \"result\": result,\n",
    "            \"execution_time\": execution_time\n",
    "        }\n",
    "\n",
    "# Get our memory_llm for consistent parallel processing\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "# Create or reuse parallel processor\n",
    "if 'parallel_processor' not in tutorial_state:\n",
    "    parallel_processor = ParallelProcessor(memory_llm)\n",
    "    tutorial_state['parallel_processor'] = parallel_processor\n",
    "    print(\"\\n✅ New Parallel Processor created\")\n",
    "else:\n",
    "    parallel_processor = tutorial_state['parallel_processor']\n",
    "    print(f\"\\n✅ Reusing existing Parallel Processor\")\n",
    "    print(f\"   Tasks executed so far: {parallel_processor.tasks_executed}\")\n",
    "\n",
    "print(\"\\n💡 All parallel tasks will use the SAME LLM instance\")\n",
    "print(\"   Parallelization happens at the execution level, not LLM level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our parallel processor using the existing memory_llm\n",
    "print(\"⚡ Initializing Parallel Processor\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm (which we use for consistent processing)\n",
    "memory_llm = tutorial_state.get(\"memory_llm\")\n",
    "\n",
    "if not memory_llm:\n",
    "    print(\"⚠️ Memory LLM not found, using global llm\")\n",
    "    memory_llm = llm\n",
    "\n",
    "# Check if parallel processor exists\n",
    "if 'parallel_processor' not in tutorial_state:\n",
    "    parallel_processor = ParallelProcessor(memory_llm)\n",
    "    tutorial_state['parallel_processor'] = parallel_processor\n",
    "    print(\"✅ New parallel processor created\")\n",
    "else:\n",
    "    parallel_processor = tutorial_state['parallel_processor']\n",
    "    print(\"✅ Using existing parallel processor\")\n",
    "\n",
    "print(f\"🔧 Processor uses same LLM as memory operations\")\n",
    "print(\"💡 Ready for parallel task execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parallel analysis tasks using our existing processor\n",
    "print(\"🏗️ BUILDING PARALLEL BUSINESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our parallel processor\n",
    "parallel_processor = tutorial_state.get('parallel_processor')\n",
    "\n",
    "if not parallel_processor:\n",
    "    print(\"⚠️ Parallel processor not initialized\")\n",
    "else:\n",
    "    print(f\"✅ Using existing Parallel Processor\")\n",
    "    print(f\"   Tasks executed: {parallel_processor.tasks_executed}\")\n",
    "    \n",
    "    # Define parallel sections for business analysis\n",
    "    print(\"\\n📊 Defining analysis sections...\")\n",
    "    \n",
    "    section_tasks = [\n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Financial Analysis\",\n",
    "            focus_area=\"financial metrics and projections\",\n",
    "            analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus specifically on financial health, revenue trends, profitability, and financial risks. \n",
    "Provide key metrics, insights, and recommendations.\"\"\"\n",
    "        ),\n",
    "        \n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Market Analysis\", \n",
    "            focus_area=\"market position and competitive landscape\",\n",
    "            analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on market opportunity, competitive advantages, market risks, and positioning.\n",
    "Provide market insights and strategic recommendations.\"\"\"\n",
    "        ),\n",
    "        \n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Operational Analysis\",\n",
    "            focus_area=\"operational efficiency and scalability\", \n",
    "            analysis_prompt=\"\"\"Analyze this business data from an {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on operational strengths, efficiency metrics, scalability factors, and operational risks.\n",
    "Provide operational insights and improvement recommendations.\"\"\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Created {len(section_tasks)} parallel analysis tasks\")\n",
    "    for task in section_tasks:\n",
    "        print(f\"   • {task['name']}\")\n",
    "    \n",
    "    # Store tasks for execution in next cell\n",
    "    tutorial_state['parallel_tasks'] = section_tasks\n",
    "    \n",
    "    print(\"\\n💡 Each task uses the SAME LLM but runs in parallel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel business analysis using our existing processor\n",
    "print(\"🚀 EXECUTING PARALLEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our processor and tasks\n",
    "parallel_processor = tutorial_state.get('parallel_processor')\n",
    "section_tasks = tutorial_state.get('parallel_tasks', [])\n",
    "\n",
    "if not parallel_processor or not section_tasks:\n",
    "    print(\"⚠️ Prerequisites not ready. Please run previous cells.\")\n",
    "else:\n",
    "    # Business data to analyze\n",
    "    business_data = \"\"\"\n",
    "TechStartup Inc. Q3 2024 Summary:\n",
    "- Revenue: $2.5M (up 150% YoY)\n",
    "- Monthly Active Users: 50,000 (up 200% YoY) \n",
    "- Customer Acquisition Cost: $45\n",
    "- Monthly Churn Rate: 3.2%\n",
    "- Burn Rate: $300K/month\n",
    "- Cash Runway: 18 months\n",
    "- Team Size: 25 employees\n",
    "- Market Size: $10B TAM\n",
    "- Top 3 competitors: BigCorp, StartupX, TechGiant\n",
    "- Key Features: AI automation, real-time collaboration, mobile-first\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"📊 Analyzing business data with {len(section_tasks)} parallel sections\")\n",
    "    \n",
    "    # Execute all sections in parallel\n",
    "    def run_parallel_analysis(tasks, data):\n",
    "        \"\"\"Run sections in parallel using ThreadPoolExecutor\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_task = {\n",
    "                executor.submit(parallel_processor.execute_section, task, data): task\n",
    "                for task in tasks\n",
    "            }\n",
    "            \n",
    "            # Collect results\n",
    "            results = []\n",
    "            for future in as_completed(future_to_task):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return results, total_time\n",
    "    \n",
    "    # Run the parallel analysis\n",
    "    results, total_time = run_parallel_analysis(section_tasks, business_data)\n",
    "    \n",
    "    # Calculate speedup\n",
    "    sequential_time = sum(r[\"execution_time\"] for r in results)\n",
    "    speedup = sequential_time / total_time\n",
    "    \n",
    "    print(f\"\\n⚡ PARALLEL EXECUTION COMPLETE\")\n",
    "    print(f\"   Wall-clock time: {total_time:.2f}s\")\n",
    "    print(f\"   Sequential time would be: {sequential_time:.2f}s\")\n",
    "    print(f\"   Speedup achieved: {speedup:.1f}x\")\n",
    "    \n",
    "    print(f\"\\n📋 Sections completed:\")\n",
    "    for result in results:\n",
    "        print(f\"  • {result['section']}: {result['execution_time']:.2f}s\")\n",
    "    \n",
    "    # Store results\n",
    "    tutorial_state['parallel_results'] = results\n",
    "    tutorial_state['parallel_speedup'] = speedup\n",
    "    \n",
    "    print(f\"\\n💡 Same LLM, parallel execution = {speedup:.1f}x faster!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parallel analysis results\n",
    "print(\"📊 PARALLEL ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = tutorial_state.get('parallel_results', [])\n",
    "speedup = tutorial_state.get('parallel_speedup', 0)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n✅ Completed {len(results)} parallel analyses\")\n",
    "    print(f\"⚡ Speedup: {speedup:.1f}x faster than sequential\")\n",
    "    \n",
    "    print(f\"\\n\udcc8 Results by section:\")\n",
    "    for result in results:\n",
    "        print(f\"\\n  {result['section']}:\")\n",
    "        print(f\"    Time: {result['execution_time']:.2f}s\")\n",
    "        print(f\"    Focus: {result['focus']}\")\n",
    "        print(f\"    Output: {len(result['result'])} characters\")\n",
    "    \n",
    "    print(f\"\\n💡 Key Insight:\")\n",
    "    print(f\"   We used ONE LLM instance for all {len(results)} tasks\")\n",
    "    print(f\"   Parallel execution happened at the thread level\")\n",
    "    print(f\"   This is more efficient than creating multiple LLM instances!\")\n",
    "else:\n",
    "    print(\"⚠️ No results found. Please run previous cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parallel Voting - Consensus Through Multiple Perspectives\n",
    "\n",
    "class VotingSystem:\n",
    "    \"\"\"Implement parallel voting for consensus decisions\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        \"\"\"\n",
    "        Initialize voting system with existing LLM instance\n",
    "        \n",
    "        TUTORIAL NOTE: We receive the LLM instance instead of creating a new one.\n",
    "        This follows our principle of reusing components for efficiency.\n",
    "        \"\"\"\n",
    "        self.llm = llm_instance\n",
    "        self.votes_cast = 0  # Track voting activity\n",
    "        \n",
    "    def create_vote_prompt(self, base_instruction, perspective_twist=\"\"):\n",
    "        \"\"\"Create a voting prompt with slight variation for diversity\"\"\"\n",
    "        return f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "{perspective_twist}\n",
    "\n",
    "Analyze carefully and provide your assessment. End your response with a clear decision:\n",
    "DECISION: [YES/NO/UNCERTAIN]\n",
    "CONFIDENCE: [1-10]\n",
    "\"\"\"\n",
    "    \n",
    "    def cast_vote(self, vote_id, content, instruction, perspective=\"\"):\n",
    "        \"\"\"Cast a single vote in the voting process\"\"\"\n",
    "        prompt_text = self.create_vote_prompt(instruction, perspective)\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"content\"],\n",
    "            template=prompt_text + \"\\n\\nContent to evaluate: {content}\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\"content\": content})\n",
    "        \n",
    "        self.votes_cast += 1  # Track each vote\n",
    "        \n",
    "        # Extract decision (simplified parsing)\n",
    "        decision = \"UNCERTAIN\"\n",
    "        confidence = 5\n",
    "        \n",
    "        if \"DECISION: YES\" in response:\n",
    "            decision = \"YES\"\n",
    "        elif \"DECISION: NO\" in response:\n",
    "            decision = \"NO\"\n",
    "            \n",
    "        # Try to extract confidence\n",
    "        if \"CONFIDENCE:\" in response:\n",
    "            try:\n",
    "                conf_line = [line for line in response.split('\\n') if 'CONFIDENCE:' in line][0]\n",
    "                confidence = int(conf_line.split(':')[1].strip().split()[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            \"vote_id\": vote_id,\n",
    "            \"decision\": decision,\n",
    "            \"confidence\": confidence,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    \n",
    "    def parallel_voting(self, content, base_instruction, num_votes=3):\n",
    "        \"\"\"Execute parallel voting with multiple perspectives\"\"\"\n",
    "        \n",
    "        # Create diverse perspectives for voting\n",
    "        perspectives = [\n",
    "            \"Consider this from a conservative, risk-averse viewpoint.\",\n",
    "            \"Evaluate this from an optimistic, opportunity-focused angle.\", \n",
    "            \"Analyze this from a balanced, neutral perspective.\"\n",
    "        ]\n",
    "        \n",
    "        # Ensure we have enough perspectives\n",
    "        while len(perspectives) < num_votes:\n",
    "            perspectives.append(f\"Provide perspective #{len(perspectives) + 1} evaluation.\")\n",
    "        \n",
    "        print(f\"🗳️ Conducting parallel voting with {num_votes} voters\")\n",
    "        print(f\"   Using existing LLM instance (votes cast so far: {self.votes_cast})\")\n",
    "        \n",
    "        # Execute votes in parallel\n",
    "        with ThreadPoolExecutor(max_workers=num_votes) as executor:\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    self.cast_vote, \n",
    "                    f\"voter_{i+1}\", \n",
    "                    content, \n",
    "                    base_instruction,\n",
    "                    perspectives[i]\n",
    "                )\n",
    "                for i in range(num_votes)\n",
    "            ]\n",
    "            \n",
    "            votes = [future.result() for future in futures]\n",
    "        \n",
    "        # Calculate consensus\n",
    "        decisions = [vote[\"decision\"] for vote in votes]\n",
    "        confidences = [vote[\"confidence\"] for vote in votes]\n",
    "        \n",
    "        yes_votes = decisions.count(\"YES\")\n",
    "        no_votes = decisions.count(\"NO\") \n",
    "        uncertain_votes = decisions.count(\"UNCERTAIN\")\n",
    "        \n",
    "        # Determine consensus\n",
    "        if yes_votes > no_votes and yes_votes > uncertain_votes:\n",
    "            consensus = \"YES\"\n",
    "        elif no_votes > yes_votes and no_votes > uncertain_votes:\n",
    "            consensus = \"NO\"\n",
    "        else:\n",
    "            consensus = \"NO CONSENSUS\"\n",
    "            \n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        \n",
    "        return {\n",
    "            \"votes\": votes,\n",
    "            \"consensus\": consensus,\n",
    "            \"vote_breakdown\": {\n",
    "                \"YES\": yes_votes,\n",
    "                \"NO\": no_votes, \n",
    "                \"UNCERTAIN\": uncertain_votes\n",
    "            },\n",
    "            \"average_confidence\": avg_confidence\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b81d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize voting system using our existing memory_llm\n",
    "print(\"🗳️ Initializing Voting System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm for consistent voting\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "# Check if voting system exists - reuse if available\n",
    "if 'voting_system' not in tutorial_state:\n",
    "    voting_system = VotingSystem(memory_llm)\n",
    "    tutorial_state['voting_system'] = voting_system\n",
    "    print(\"✅ New voting system created with existing LLM\")\n",
    "    print(f\"   LLM instance: memory_llm (temp: 0.2)\")\n",
    "else:\n",
    "    voting_system = tutorial_state['voting_system']\n",
    "    print(\"✅ Reusing existing voting system\")\n",
    "    print(f\"   Total votes cast: {voting_system.votes_cast}\")\n",
    "\n",
    "print(f\"\\n🔧 Configuration:\")\n",
    "print(f\"   • Uses same LLM as memory operations\")\n",
    "print(f\"   • Enables parallel consensus voting\")\n",
    "print(f\"   • Tracks voting activity across sessions\")\n",
    "print(\"\\n💡 Ready for consensus-based decision making\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Voting Demo - Content Safety Assessment\n",
    "\n",
    "# Get our voting system from tutorial state\n",
    "voting_system = tutorial_state['voting_system']\n",
    "\n",
    "# Content to evaluate for safety\n",
    "test_content = \"\"\"\n",
    "Check out this amazing new productivity method that will totally change your life! \n",
    "It's based on scientific research and has helped thousands of people achieve their goals faster than ever before.\n",
    "Join our community today and start your transformation journey!\n",
    "\"\"\"\n",
    "\n",
    "safety_instruction = \"\"\"\n",
    "Evaluate whether this content is appropriate for a professional platform.\n",
    "\n",
    "Consider these factors:\n",
    "- Is it truthful and not misleading?\n",
    "- Does it avoid excessive hype or unrealistic claims?\n",
    "- Is it suitable for a professional audience?\n",
    "- Does it comply with content guidelines?\n",
    "\n",
    "Provide detailed reasoning for your assessment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🛡️ CONTENT SAFETY VOTING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluating content: '{test_content[:60]}...'\")\n",
    "print(f\"Using voting system with {voting_system.votes_cast} votes cast previously\\n\")\n",
    "\n",
    "# Conduct the vote using our existing voting system\n",
    "voting_result = voting_system.parallel_voting(\n",
    "    content=test_content,\n",
    "    base_instruction=safety_instruction,\n",
    "    num_votes=5\n",
    ")\n",
    "\n",
    "# Store results for later reference\n",
    "tutorial_state['voting_results'] = voting_result\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📊 VOTING RESULTS:\")\n",
    "print(f\"Consensus: {voting_result['consensus']}\")\n",
    "print(f\"Average Confidence: {voting_result['average_confidence']:.1f}/10\")\n",
    "print(f\"Vote Breakdown:\")\n",
    "for decision, count in voting_result['vote_breakdown'].items():\n",
    "    print(f\"  {decision}: {count} votes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show individual votes and summary\n",
    "voting_result = tutorial_state['voting_results']\n",
    "voting_system = tutorial_state['voting_system']\n",
    "\n",
    "print(f\"🗳️ INDIVIDUAL VOTES:\")\n",
    "print(\"=\" * 60)\n",
    "for vote in voting_result['votes']:\n",
    "    print(f\"  {vote['vote_id']}: {vote['decision']} (confidence: {vote['confidence']}/10)\")\n",
    "\n",
    "print(f\"\\n📊 VOTING SYSTEM STATISTICS:\")\n",
    "print(f\"   Total votes cast: {voting_system.votes_cast}\")\n",
    "print(f\"   Votes in this session: {len(voting_result['votes'])}\")\n",
    "\n",
    "print(f\"\\n✅ PARALLELIZATION WORKFLOWS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   ✓ Sectioning: Parallel task decomposition for speed\")\n",
    "print(\"   ✓ Voting: Consensus-based decision making for accuracy\")\n",
    "print(\"   ✓ Mathematical foundations: Amdahl's Law & Condorcet's Theorem\")\n",
    "print(f\"\\n💡 All parallel workflows used the SAME LLM instance\")\n",
    "print(f\"   • ParallelProcessor tasks: {tutorial_state['parallel_processor'].tasks_executed}\")\n",
    "print(f\"   • VotingSystem votes: {voting_system.votes_cast}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Summary - Review All Components Built\n",
    "\n",
    "print(\"📊 WORKFLOW PATTERNS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll workflow components built and stored in tutorial_state:\\n\")\n",
    "\n",
    "# 1. Prompt Chaining\n",
    "if 'prompt_chain' in tutorial_state:\n",
    "    chain = tutorial_state['prompt_chain']\n",
    "    print(\"✅ 1. PROMPT CHAINING\")\n",
    "    print(f\"   • Sequential step-by-step processing\")\n",
    "    print(f\"   • Steps executed: {chain.steps_executed}\")\n",
    "    print(f\"   • Uses: memory_llm (consistent LLM instance)\")\n",
    "\n",
    "# 2. Routing System\n",
    "if 'router' in tutorial_state:\n",
    "    router = tutorial_state['router']\n",
    "    print(\"\\n✅ 2. INTELLIGENT ROUTING\")\n",
    "    print(f\"   • Dynamic query classification and routing\")\n",
    "    print(f\"   • Routes registered: {len(router.routes)}\")\n",
    "    print(f\"   • Uses: global llm\")\n",
    "\n",
    "# 3. Parallel Processing\n",
    "if 'parallel_processor' in tutorial_state:\n",
    "    processor = tutorial_state['parallel_processor']\n",
    "    print(\"\\n✅ 3. PARALLEL PROCESSING\")\n",
    "    print(f\"   • Concurrent task execution\")\n",
    "    print(f\"   • Tasks executed: {processor.tasks_executed}\")\n",
    "    print(f\"   • Uses: memory_llm (shared across threads)\")\n",
    "\n",
    "# 4. Voting System\n",
    "if 'voting_system' in tutorial_state:\n",
    "    voting = tutorial_state['voting_system']\n",
    "    print(\"\\n✅ 4. CONSENSUS VOTING\")\n",
    "    print(f\"   • Parallel voting for consensus decisions\")\n",
    "    print(f\"   • Votes cast: {voting.votes_cast}\")\n",
    "    print(f\"   • Uses: memory_llm (same as parallel processor)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"💡 KEY INSIGHT: LLM Instance Reuse\")\n",
    "print(\"=\" * 80)\n",
    "print(\"All workflows share TWO LLM instances:\")\n",
    "print(\"  1. `llm` (temp: 0.3) - Main LLM for general tasks & routing\")\n",
    "print(\"  2. `memory_llm` (temp: 0.2) - Used for memory, chains, parallel work, voting\")\n",
    "print(\"\\n🎯 Benefits of this architecture:\")\n",
    "print(\"   • Reduced memory footprint\")\n",
    "print(\"   • Faster initialization\")\n",
    "print(\"   • Consistent behavior across workflows\")\n",
    "print(\"   • More efficient resource utilization\")\n",
    "print(\"   • Easier to manage and debug\")\n",
    "print(\"\\n✨ This is how production systems should be built!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflows Section Complete - Check Tutorial State\n",
    "\n",
    "print(\"🎉 WORKFLOWS AND CHAINS SECTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show all workflow components in tutorial_state\n",
    "print(\"\\n📦 Components available in tutorial_state:\")\n",
    "workflow_components = ['prompt_chain', 'router', 'parallel_processor', 'voting_system']\n",
    "for component in workflow_components:\n",
    "    if component in tutorial_state:\n",
    "        print(f\"   ✓ {component}\")\n",
    "    else:\n",
    "        print(f\"   ✗ {component} (not found)\")\n",
    "\n",
    "print(\"\\n🔧 LLM Instances:\")\n",
    "print(f\"   • llm (global): {type(llm).__name__} (temp: 0.3)\")\n",
    "if 'memory_llm' in tutorial_state:\n",
    "    print(f\"   • memory_llm: {type(tutorial_state['memory_llm']).__name__} (temp: 0.2)\")\n",
    "\n",
    "print(\"\\n📊 Usage Statistics:\")\n",
    "if 'prompt_chain' in tutorial_state:\n",
    "    print(f\"   • Prompt chain steps: {tutorial_state['prompt_chain'].steps_executed}\")\n",
    "if 'parallel_processor' in tutorial_state:\n",
    "    print(f\"   • Parallel tasks: {tutorial_state['parallel_processor'].tasks_executed}\")\n",
    "if 'voting_system' in tutorial_state:\n",
    "    print(f\"   • Votes cast: {tutorial_state['voting_system'].votes_cast}\")\n",
    "\n",
    "print(\"\\n💡 Ready to proceed to Advanced Agent Systems and RAG!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc688ff4",
   "metadata": {},
   "source": [
    "#### 4. Advanced Workflow Patterns & Agent Systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa093e3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Agentic Systems - Autonomous Agents and Meta-Workflows\n",
    "\n",
    "import time\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "import uuid\n",
    "\n",
    "class AgentState(Enum):\n",
    "    \"\"\"Agent execution states\"\"\"\n",
    "    IDLE = \"idle\"\n",
    "    PLANNING = \"planning\" \n",
    "    EXECUTING = \"executing\"\n",
    "    EVALUATING = \"evaluating\"\n",
    "    BLOCKED = \"blocked\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AgentMemory:\n",
    "    \"\"\"Agent working memory and context\"\"\"\n",
    "    task_history: List[Dict] = field(default_factory=list)\n",
    "    current_context: Dict = field(default_factory=dict)\n",
    "    learned_patterns: Dict = field(default_factory=dict)\n",
    "    error_log: List[str] = field(default_factory=list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedAgentSystem:\n",
    "    \"\"\"\n",
    "    Autonomous agent system implementing Anthropic's agent patterns\n",
    "    Features: Dynamic planning, error recovery, learning, human-in-the-loop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_iterations: int = 10):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.state = AgentState.IDLE\n",
    "        self.memory = AgentMemory()\n",
    "        self.tools = {}\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def register_tool(self, name: str, function: Callable, description: str):\n",
    "        \"\"\"Register tools for agent use\"\"\"\n",
    "        self.tools[name] = {\n",
    "            \"function\": function,\n",
    "            \"description\": description,\n",
    "            \"usage_count\": 0\n",
    "        }\n",
    "        print(f\"Registered tool: {name}\")\n",
    "    \n",
    "    def create_plan(self, task: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dynamic planning based on task complexity\n",
    "        Implements reasoning and planning capabilities\n",
    "        \"\"\"\n",
    "        self.state = AgentState.PLANNING\n",
    "        \n",
    "        planning_prompt = PromptTemplate(\n",
    "            input_variables=[\"task\", \"available_tools\", \"context\"],\n",
    "            template=\"\"\"You are an autonomous agent creating an execution plan.\n",
    "            \n",
    "            Task: {task}\n",
    "            \n",
    "            Available Tools: {available_tools}\n",
    "            \n",
    "            Current Context: {context}\n",
    "            \n",
    "            Create a detailed plan with steps, tools needed, and success criteria.\n",
    "            Format as JSON:\n",
    "            {{\n",
    "                \"plan_id\": \"unique_id\",\n",
    "                \"steps\": [\n",
    "                    {{\n",
    "                        \"step_id\": \"step_1\",\n",
    "                        \"action\": \"specific action to take\",\n",
    "                        \"tools_needed\": [\"tool1\", \"tool2\"],\n",
    "                        \"success_criteria\": \"how to verify success\",\n",
    "                        \"estimated_time\": \"time estimate\",\n",
    "                        \"dependencies\": [\"previous_step_ids\"]\n",
    "                    }}\n",
    "                ],\n",
    "                \"risks\": [\"potential issues\"],\n",
    "                \"checkpoints\": [\"human approval points\"]\n",
    "            }}\"\"\"\n",
    "        )\n",
    "        \n",
    "        tools_description = \"\\n\".join([\n",
    "            f\"- {name}: {info['description']}\" \n",
    "            for name, info in self.tools.items()\n",
    "        ])\n",
    "        \n",
    "        context = json.dumps(self.memory.current_context, indent=2)\n",
    "        \n",
    "        chain = planning_prompt | self.llm | StrOutputParser()\n",
    "        plan_result = chain.invoke({\n",
    "            \"task\": task,\n",
    "            \"available_tools\": tools_description,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        # Parse plan (simplified JSON extraction)\n",
    "        try:\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', plan_result, re.DOTALL)\n",
    "            if json_match:\n",
    "                plan_data = json.loads(json_match.group())\n",
    "                plan_steps = plan_data.get(\"steps\", [])\n",
    "                \n",
    "                # Add to memory\n",
    "                self.memory.task_history.append({\n",
    "                    \"task\": task,\n",
    "                    \"plan\": plan_data,\n",
    "                    \"created_at\": time.time()\n",
    "                })\n",
    "                \n",
    "                print(f\"Created plan with {len(plan_steps)} steps\")\n",
    "                return plan_steps\n",
    "        except Exception as e:\n",
    "            self.memory.error_log.append(f\"Planning error: {str(e)}\")\n",
    "            # Fallback simple plan\n",
    "            return [{\n",
    "                \"step_id\": \"fallback_1\",\n",
    "                \"action\": f\"Complete task: {task}\",\n",
    "                \"tools_needed\": [],\n",
    "                \"success_criteria\": \"Task completion\"\n",
    "            }]\n",
    "    \n",
    "    def execute_step(self, step: Dict) -> Dict:\n",
    "        \"\"\"Execute individual plan step with error recovery\"\"\"\n",
    "        step_id = step.get(\"step_id\", str(uuid.uuid4()))\n",
    "        print(f\"Executing step: {step_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if tools are needed\n",
    "            tools_needed = step.get(\"tools_needed\", [])\n",
    "            tool_results = {}\n",
    "            \n",
    "            for tool_name in tools_needed:\n",
    "                if tool_name in self.tools:\n",
    "                    print(f\"Using tool: {tool_name}\")\n",
    "                    # Simplified tool execution\n",
    "                    tool_results[tool_name] = f\"Tool {tool_name} executed successfully\"\n",
    "                    self.tools[tool_name][\"usage_count\"] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Tool {tool_name} not available\")\n",
    "            \n",
    "            # Execute main action\n",
    "            execution_prompt = PromptTemplate(\n",
    "                input_variables=[\"action\", \"tool_results\", \"success_criteria\"],\n",
    "                template=\"\"\"Execute this action step by step:\n",
    "                \n",
    "                Action: {action}\n",
    "                \n",
    "                Tool Results: {tool_results}\n",
    "                \n",
    "                Success Criteria: {success_criteria}\n",
    "                \n",
    "                Provide detailed execution results and verify success criteria.\"\"\"\n",
    "            )\n",
    "            \n",
    "            chain = execution_prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\n",
    "                \"action\": step[\"action\"],\n",
    "                \"tool_results\": json.dumps(tool_results, indent=2),\n",
    "                \"success_criteria\": step.get(\"success_criteria\", \"completion\")\n",
    "            })\n",
    "            \n",
    "            # Evaluate success\n",
    "            success = self.evaluate_step_success(step, result)\n",
    "            \n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"success\" if success else \"needs_retry\",\n",
    "                \"result\": result,\n",
    "                \"tool_usage\": tool_results,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step execution failed: {str(e)}\"\n",
    "            self.memory.error_log.append(error_msg)\n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "    \n",
    "    def evaluate_step_success(self, step: Dict, result: str) -> bool:\n",
    "        \"\"\"Evaluate if step was successful based on criteria\"\"\"\n",
    "        success_criteria = step.get(\"success_criteria\", \"\")\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"criteria\", \"result\"],\n",
    "            template=\"\"\"Evaluate if this result meets the success criteria.\n",
    "            \n",
    "            Success Criteria: {criteria}\n",
    "            \n",
    "            Actual Result: {result}\n",
    "            \n",
    "            Respond with just \"SUCCESS\" or \"FAILURE\" followed by brief reasoning.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation = chain.invoke({\n",
    "            \"criteria\": success_criteria,\n",
    "            \"result\": result\n",
    "        })\n",
    "        \n",
    "        return \"SUCCESS\" in evaluation.upper()\n",
    "    \n",
    "    def error_recovery(self, failed_step: Dict, error: str) -> Optional[Dict]:\n",
    "        \"\"\"Implement error recovery strategies\"\"\"\n",
    "        print(f\"Attempting error recovery for: {error}\")\n",
    "        \n",
    "        recovery_prompt = PromptTemplate(\n",
    "            input_variables=[\"failed_step\", \"error\", \"error_history\"],\n",
    "            template=\"\"\"Analyze this error and suggest recovery strategy:\n",
    "            \n",
    "            Failed Step: {failed_step}\n",
    "            \n",
    "            Error: {error}\n",
    "            \n",
    "            Previous Errors: {error_history}\n",
    "            \n",
    "            Suggest a modified approach or alternative strategy.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = recovery_prompt | self.llm | StrOutputParser()\n",
    "        recovery_suggestion = chain.invoke({\n",
    "            \"failed_step\": json.dumps(failed_step, indent=2),\n",
    "            \"error\": error,\n",
    "            \"error_history\": json.dumps(self.memory.error_log[-5:], indent=2)\n",
    "        })\n",
    "        \n",
    "        # Create modified step (simplified)\n",
    "        modified_step = failed_step.copy()\n",
    "        modified_step[\"action\"] = f\"RETRY: {modified_step['action']} (Modified based on: {recovery_suggestion[:100]})\"\n",
    "        \n",
    "        return modified_step\n",
    "    \n",
    "    def human_checkpoint(self, checkpoint_data: Dict) -> bool:\n",
    "        \"\"\"Simulate human-in-the-loop checkpoint\"\"\"\n",
    "        print(f\"🚨 HUMAN CHECKPOINT: {checkpoint_data}\")\n",
    "        print(\"In production, this would pause for human approval\")\n",
    "        \n",
    "        # Simulate human approval (always approve for demo)\n",
    "        approval = True\n",
    "        print(f\"✅ Human approval: {'Granted' if approval else 'Denied'}\")\n",
    "        return approval\n",
    "    \n",
    "    def autonomous_execution(self, task: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Main autonomous agent execution loop\n",
    "        Implements the complete agent pattern with all capabilities\n",
    "        \"\"\"\n",
    "        print(f\"🤖 AUTONOMOUS AGENT STARTING\")\n",
    "        print(f\"Task: {task}\")\n",
    "        \n",
    "        execution_log = {\n",
    "            \"task\": task,\n",
    "            \"start_time\": time.time(),\n",
    "            \"steps_completed\": 0,\n",
    "            \"errors_encountered\": 0,\n",
    "            \"human_interactions\": 0,\n",
    "            \"final_status\": \"in_progress\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: Planning\n",
    "            self.state = AgentState.PLANNING\n",
    "            plan = self.create_plan(task)\n",
    "            \n",
    "            if not plan:\n",
    "                raise Exception(\"Failed to create execution plan\")\n",
    "            \n",
    "            # Phase 2: Execution\n",
    "            self.state = AgentState.EXECUTING\n",
    "            completed_steps = []\n",
    "            \n",
    "            for iteration in range(self.max_iterations):\n",
    "                if not plan:\n",
    "                    break\n",
    "                    \n",
    "                current_step = plan.pop(0)\n",
    "                \n",
    "                # Check for human checkpoint\n",
    "                if \"checkpoint\" in current_step.get(\"action\", \"\").lower():\n",
    "                    if not self.human_checkpoint(current_step):\n",
    "                        self.state = AgentState.BLOCKED\n",
    "                        execution_log[\"final_status\"] = \"blocked_by_human\"\n",
    "                        break\n",
    "                    execution_log[\"human_interactions\"] += 1\n",
    "                \n",
    "                # Execute step\n",
    "                step_result = self.execute_step(current_step)\n",
    "                completed_steps.append(step_result)\n",
    "                execution_log[\"steps_completed\"] += 1\n",
    "                \n",
    "                if step_result[\"status\"] == \"failed\":\n",
    "                    execution_log[\"errors_encountered\"] += 1\n",
    "                    \n",
    "                    # Attempt error recovery\n",
    "                    recovered_step = self.error_recovery(\n",
    "                        current_step, \n",
    "                        step_result.get(\"error\", \"Unknown error\")\n",
    "                    )\n",
    "                    \n",
    "                    if recovered_step:\n",
    "                        plan.insert(0, recovered_step)  # Retry at front\n",
    "                    else:\n",
    "                        print(\"❌ Error recovery failed\")\n",
    "                        break\n",
    "                \n",
    "                elif step_result[\"status\"] == \"needs_retry\":\n",
    "                    plan.insert(0, current_step)  # Retry same step\n",
    "                \n",
    "                # Progress update\n",
    "                print(f\"Progress: {execution_log['steps_completed']} steps completed\")\n",
    "            \n",
    "            # Phase 3: Final evaluation\n",
    "            self.state = AgentState.EVALUATING\n",
    "            final_evaluation = self.evaluate_final_result(task, completed_steps)\n",
    "            \n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"total_duration\": time.time() - execution_log[\"start_time\"],\n",
    "                \"completed_steps\": completed_steps,\n",
    "                \"final_evaluation\": final_evaluation,\n",
    "                \"final_status\": \"completed\" if final_evaluation[\"success\"] else \"failed\"\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.COMPLETED if final_evaluation[\"success\"] else AgentState.FAILED\n",
    "            \n",
    "            print(f\"🎯 AUTONOMOUS EXECUTION {'COMPLETED' if final_evaluation['success'] else 'FAILED'}\")\n",
    "            print(f\"Duration: {execution_log['total_duration']:.2f}s\")\n",
    "            print(f\"Steps: {execution_log['steps_completed']}\")\n",
    "            print(f\"Errors: {execution_log['errors_encountered']}\")\n",
    "            \n",
    "            return execution_log\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"final_status\": \"system_error\",\n",
    "                \"system_error\": str(e)\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.FAILED\n",
    "            print(f\"💥 SYSTEM ERROR: {str(e)}\")\n",
    "            return execution_log\n",
    "    \n",
    "    def evaluate_final_result(self, original_task: str, completed_steps: List[Dict]) -> Dict:\n",
    "        \"\"\"Final evaluation of task completion\"\"\"\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"original_task\", \"steps_summary\"],\n",
    "            template=\"\"\"Evaluate if the original task was successfully completed.\n",
    "            \n",
    "            Original Task: {original_task}\n",
    "            \n",
    "            Completed Steps Summary: {steps_summary}\n",
    "            \n",
    "            Provide evaluation including:\n",
    "            1. Task completion status (SUCCESS/PARTIAL/FAILURE)\n",
    "            2. Quality assessment (1-10)\n",
    "            3. Areas of success\n",
    "            4. Areas for improvement\n",
    "            5. Overall confidence level\"\"\"\n",
    "        )\n",
    "        \n",
    "        steps_summary = \"\\n\".join([\n",
    "            f\"Step {i+1}: {step.get('result', 'No result')[:100]}...\"\n",
    "            for i, step in enumerate(completed_steps)\n",
    "        ])\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation_result = chain.invoke({\n",
    "            \"original_task\": original_task,\n",
    "            \"steps_summary\": steps_summary\n",
    "        })\n",
    "        \n",
    "        # Parse evaluation (simplified)\n",
    "        success = \"SUCCESS\" in evaluation_result.upper()\n",
    "        \n",
    "        return {\n",
    "            \"success\": success,\n",
    "            \"evaluation\": evaluation_result,\n",
    "            \"steps_count\": len(completed_steps),\n",
    "            \"quality_indicators\": {\n",
    "                \"completion_rate\": len([s for s in completed_steps if s.get(\"status\") == \"success\"]) / max(len(completed_steps), 1),\n",
    "                \"error_rate\": len([s for s in completed_steps if s.get(\"status\") == \"failed\"]) / max(len(completed_steps), 1)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstration of Autonomous Agent System\n",
    "class AutonomousAgentDemo:\n",
    "    \"\"\"Comprehensive demonstration of autonomous agent capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.agent = AdvancedAgentSystem(llm)\n",
    "        self.setup_demo_tools()\n",
    "    \n",
    "    def setup_demo_tools(self):\n",
    "        \"\"\"Register demonstration tools\"\"\"\n",
    "        \n",
    "        def web_search(query: str) -> str:\n",
    "            return f\"Search results for '{query}': [Simulated web search results]\"\n",
    "        \n",
    "        def file_manager(action: str, filename: str = \"\", content: str = \"\") -> str:\n",
    "            return f\"File operation '{action}' on '{filename}': Success\"\n",
    "        \n",
    "        def api_call(endpoint: str, data: Dict = None) -> str:\n",
    "            return f\"API call to '{endpoint}': Success (simulated)\"\n",
    "        \n",
    "        def data_analysis(dataset: str, analysis_type: str = \"summary\") -> str:\n",
    "            return f\"Analysis '{analysis_type}' on '{dataset}': Completed with insights\"\n",
    "        \n",
    "        # Register tools\n",
    "        self.agent.register_tool(\"web_search\", web_search, \"Search the web for information\")\n",
    "        self.agent.register_tool(\"file_manager\", file_manager, \"Create, read, update, delete files\")\n",
    "        self.agent.register_tool(\"api_call\", api_call, \"Make API calls to external services\")\n",
    "        self.agent.register_tool(\"data_analysis\", data_analysis, \"Analyze datasets and generate insights\")\n",
    "    \n",
    "    def demo_complex_research_task(self):\n",
    "        \"\"\"Demonstrate agent handling complex multi-step research task\"\"\"\n",
    "        print(\"🔬 AUTONOMOUS RESEARCH AGENT DEMONSTRATION\")\n",
    "        \n",
    "        complex_task = \"\"\"\n",
    "        Research the current state of quantum computing and create a comprehensive report including:\n",
    "        1. Recent breakthrough discoveries in quantum computing\n",
    "        2. Major companies and their quantum computing initiatives\n",
    "        3. Current limitations and challenges\n",
    "        4. Potential future applications\n",
    "        5. Timeline predictions for quantum supremacy achievements\n",
    "        \n",
    "        The report should be well-structured, factual, and include citations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(complex_task)\n",
    "        return result\n",
    "    \n",
    "    def demo_software_development_task(self):\n",
    "        \"\"\"Demonstrate agent handling software development workflow\"\"\"\n",
    "        print(\"💻 AUTONOMOUS DEVELOPMENT AGENT DEMONSTRATION\")\n",
    "        \n",
    "        dev_task = \"\"\"\n",
    "        Create a complete web application for a personal task management system including:\n",
    "        1. Backend API with user authentication\n",
    "        2. Database schema for tasks and users\n",
    "        3. Frontend interface with CRUD operations\n",
    "        4. Unit tests for core functionality\n",
    "        5. Deployment configuration\n",
    "        6. Documentation and README\n",
    "        \n",
    "        Use modern best practices and ensure security considerations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(dev_task)\n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_demo(self):\n",
    "        \"\"\"Run comprehensive autonomous agent demonstrations\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT SYSTEM DEMONSTRATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Demo 1: Research Task\n",
    "        results['research'] = self.demo_complex_research_task()\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        \n",
    "        # Demo 2: Development Task  \n",
    "        results['development'] = self.demo_software_development_task()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT DEMONSTRATIONS COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76645053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and demonstrate autonomous agents\n",
    "if 'autonomous_agent' not in tutorial_state:\n",
    "    agent_demo = AutonomousAgentDemo(memory_llm)\n",
    "    tutorial_state['autonomous_agent'] = agent_demo\n",
    "    \n",
    "    print(\"🚀 STARTING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = agent_demo.run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results\n",
    "else:\n",
    "    print(\"🔄 RUNNING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = tutorial_state['autonomous_agent'].run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143533c6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd518b",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b24f75",
   "metadata": {},
   "source": [
    "Now that we've mastered building intelligent agents and workflows, it's time to tackle one of the most important challenges in modern AI systems: how do we give our agents access to vast, specific, and up-to-date knowledge that wasn't included in their training data?\n",
    "\n",
    "This is where Retrieval-Augmented Generation (RAG) becomes essential. RAG is the bridge between the incredible reasoning capabilities of large language models and the specific, detailed knowledge that your applications need to be truly useful in real-world scenarios.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*WYv0_CaBmCTt7FXc\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e3e6c",
   "metadata": {},
   "source": [
    "### Why RAG Is Essential: The Knowledge Gap Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e18ff2",
   "metadata": {},
   "source": [
    "Let me paint a picture of why RAG matters. Imagine you've built a brilliant customer service agent using the workflows we just learned. It can route questions, use tools, and maintain conversation context perfectly. But then a customer asks about your company's specific return policy that was updated last week, or wants details about a product that was launched after the model's training cutoff.\n",
    "\n",
    "**The Fundamental Limitations of LLMs:**\n",
    "\n",
    "Even the most advanced language models face critical limitations when used alone:\n",
    "\n",
    "1. **Knowledge Cutoff**: Training data has a specific cutoff date, making models ignorant of recent information\n",
    "2. **Domain Specificity**: Models lack deep knowledge about your specific business, products, or internal processes  \n",
    "3. **Context Window Limits**: Even with large context windows, you can't fit entire knowledge bases into a single conversation\n",
    "4. **Hallucination Risk**: When models don't know something, they often generate plausible-sounding but incorrect information\n",
    "5. **Static Knowledge**: The information encoded during training can't be updated without retraining\n",
    "\n",
    "**Where Our Agent Workflows Hit the Wall:** The sophisticated agent workflows we've built are incredibly powerful for reasoning and decision-making, but they're only as good as the knowledge they have access to. Without RAG:\n",
    "\n",
    "- Your routing system might correctly identify that a question is about \"product specifications,\" but have no way to retrieve the actual, current specifications\n",
    "- Your memory system can remember what users have discussed, but can't recall relevant company knowledge or documentation\n",
    "- Your tools can calculate and process data, but can't access your proprietary knowledge base or recent updates\n",
    "\n",
    "**RAG as the Solution:** Retrieval-Augmented Generation solves these problems by creating a dynamic bridge between your agents and external knowledge sources. Instead of relying solely on the model's trained knowledge, RAG systems:\n",
    "\n",
    "- **Retrieve** relevant information from external knowledge bases in real-time\n",
    "- **Augment** the model's prompt with this retrieved context  \n",
    "- **Generate** responses that combine the model's reasoning abilities with specific, current, and accurate information\n",
    "\n",
    "This creates agents that maintain their sophisticated reasoning capabilities while having access to vast, specific, and up-to-date knowledge that makes them truly useful for real-world applications.\n",
    "\n",
    "**What We'll Build:** In this section, we'll explore how to integrate RAG into the agentic systems we've been building, creating agents that can seamlessly combine reasoning, tool use, memory, and knowledge retrieval into powerful, practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc169c2",
   "metadata": {},
   "source": [
    "### Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cb659",
   "metadata": {},
   "source": [
    "Document preprocessing is the foundation of any effective RAG system. Without proper structured, labeled data on database, no model can perform good. A good data preprocessing is crucial espcially in large scale production systems where we deal with millions of documents in real time, any small mistake or bug can lead to catastrophic failures. It's important to choose the right preprocessing techniques given requirements and to align well with business goal. \n",
    "\n",
    "**The Challenge:** Raw documents come in countless formats, structures, and sizes. A PDF might contain tables, images, and multi-column layouts. A web page includes navigation menus, advertisements, and dynamic content. A code repository has different file types with distinct syntaxes. Without proper preprocessing, even the most sophisticated retrieval system will struggle to find and present relevant information effectively.\n",
    "\n",
    "Document preprocessing involves several transformations that can be expressed mathematically:\n",
    "\n",
    "- **Information Density**: $\\rho = \\frac{\\text{Relevant Content}}{\\text{Total Content}}$ - maximizing signal-to-noise ratio\n",
    "- **Semantic Coherence**: $C(chunk) = \\frac{\\sum_{i,j} similarity(sent_i, sent_j)}{n(n-1)/2}$ - ensuring chunks maintain internal consistency  \n",
    "- **Optimal Chunk Size**: $size_{optimal} = \\arg\\max_{s} (retrieval\\_accuracy(s) - processing\\_cost(s))$\n",
    "\n",
    "<img src=\"https://chamomile.ai/reliable-rag-with-data-preprocessing/image6.png\" width=700>\n",
    "\n",
    "**The Preprocessing Pipeline:** Our approach follows a systematic four-stage pipeline:\n",
    "\n",
    "1. **Document Loading**: Extract content from various formats while preserving semantic structure\n",
    "2. **Splitting**: Break documents into manageable sections based on natural boundaries\n",
    "3. **Chunking**: Create optimally-sized pieces that balance context and specificity  \n",
    "4. **Embedding**: Transform text into vector representations for semantic search\n",
    "\n",
    "Each stage has multiple strategies optimized for different document types and use cases. Let's explore each in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d7a17",
   "metadata": {},
   "source": [
    "#### Document Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e08f9",
   "metadata": {},
   "source": [
    "Document loading is the critical first step in building effective RAG systems. Different document types require specialized loaders optimized for their unique structures and challenges. Let's explore the ecosystem of document loaders available in LangChain and understand when to use each one.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776c9a1",
   "metadata": {},
   "source": [
    "##### Web Content Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c7fc3",
   "metadata": {},
   "source": [
    "Web content presents unique challenges: dynamic JavaScript rendering, complex layouts, advertisements, navigation elements, and varying HTML structures. Choosing the right web loader depends on your specific requirements around speed, accuracy, and the complexity of target websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba584f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Loader** | **Best For** | **Key Features** | **Considerations** | **Type** |\n",
    "|------------|--------------|------------------|-------------------|----------|\n",
    "| [Web](https://python.langchain.com/docs/integrations/document_loaders/web_base) | Simple static pages | • Uses urllib + BeautifulSoup<br>• Fast and lightweight<br>• No external dependencies | • Struggles with JavaScript-heavy sites<br>• Basic HTML parsing only<br>• No dynamic content handling | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Complex layouts | • Advanced structure detection<br>• Preserves semantic hierarchy<br>• Handles tables and formatting | • Slower processing<br>• Heavier dependencies<br>• May need additional setup | Package |\n",
    "| [RecursiveURL](https://python.langchain.com/docs/integrations/document_loaders/recursive_url) | Documentation sites | • Automatically discovers child links<br>• Configurable depth control<br>• Maintains site structure | • Can retrieve too much data<br>• Requires careful depth limits<br>• May hit rate limits | Package |\n",
    "| [Sitemap](https://python.langchain.com/docs/integrations/document_loaders/sitemap) | Entire websites | • Uses sitemap.xml for discovery<br>• Efficient site crawling<br>• Respects site structure | • Requires valid sitemap<br>• May miss pages not in sitemap<br>• Large sites = long processing | Package |\n",
    "| [Spider](https://python.langchain.com/docs/integrations/document_loaders/spider) | Production crawling | • LLM-optimized output format<br>• Handles JavaScript rendering<br>• Anti-bot bypass capabilities | • Requires API key<br>• Usage-based pricing<br>• External service dependency | API |\n",
    "| [Firecrawl](https://python.langchain.com/docs/integrations/document_loaders/firecrawl) | Enterprise scraping | • Self-hostable option<br>• JavaScript execution<br>• Advanced content extraction | • Complex setup if self-hosted<br>• API costs if cloud-hosted<br>• Requires infrastructure | API |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Document-heavy sites | • Specialized for document extraction<br>• Format preservation<br>• Multi-format support | • Focused on document-centric sites<br>• May be overkill for simple pages<br>• Learning curve | Package |\n",
    "| [Hyperbrowser](https://python.langchain.com/docs/integrations/document_loaders/hyperbrowser) | Complex web apps | • Full browser automation<br>• JavaScript execution<br>• Session management | • Higher latency<br>• Resource intensive<br>• API-based pricing | API |\n",
    "| [AgentQL](https://python.langchain.com/docs/integrations/document_loaders/agentql) | Structured extraction | • Natural language queries<br>• Precise data targeting<br>• Schema-based extraction | • Best for specific data points<br>• Requires query design<br>• API costs | API |\n",
    "| [Oxylabs](https://python.langchain.com/docs/integrations/document_loaders/oxylabs) | Large-scale scraping | • Enterprise-grade infrastructure<br>• Geographic proxy support<br>• High success rates | • Premium pricing<br>• Overkill for small projects<br>• External dependency | API |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c74ea3",
   "metadata": {},
   "source": [
    "There's PDF content loaders as well \n",
    "\n",
    "| **Document Loader** | **Description** | **Package/API** |\n",
    "| --- | --- | --- |\n",
    "| [PyPDF](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader) | Uses `pypdf` to load and parse PDFs | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Uses Unstructured's open source library to load PDFs | Package |\n",
    "| [Amazon Textract](https://python.langchain.com/docs/integrations/document_loaders/amazon_textract) | Uses AWS API to load PDFs | API |\n",
    "| [MathPix](https://python.langchain.com/docs/integrations/document_loaders/mathpix) | Uses MathPix to load PDFs | Package |\n",
    "| [PDFPlumber](https://python.langchain.com/docs/integrations/document_loaders/pdfplumber) | Load PDF files using PDFPlumber | Package |\n",
    "| [PyPDFDirectry](https://python.langchain.com/docs/integrations/document_loaders/pypdfdirectory) | Load a directory with PDF files | Package |\n",
    "| [PyPDFium2](https://python.langchain.com/docs/integrations/document_loaders/pypdfium2) | Load PDF files using PyPDFium2 | Package |\n",
    "| [PyMuPDF](https://python.langchain.com/docs/integrations/document_loaders/pymupdf) | Load PDF files using PyMuPDF | Package |\n",
    "| [PyMuPDF4LLM](https://python.langchain.com/docs/integrations/document_loaders/pymupdf4llm) | Load PDF content to Markdown using PyMuPDF4LLM | Package |\n",
    "| [PDFMiner](https://python.langchain.com/docs/integrations/document_loaders/pdfminer) | Load PDF files using PDFMiner | Package |\n",
    "| [Upstage Document Parse Loader](https://python.langchain.com/docs/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader | Package |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Load PDF files using Docling | Package |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45978598",
   "metadata": {},
   "source": [
    "There's also a way you can build multimodal rag but basically what that means is we convert image to text and treat that document as normal vectorized doc\n",
    "\n",
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/rag-preprocessing-for-images.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cec56",
   "metadata": {},
   "source": [
    "For now let's just get a document loading function ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296718b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DocumentLoadingSystem:\n",
    "    \"\"\"\n",
    "    Comprehensive document loading system supporting multiple formats and sources\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_formats = {\n",
    "            'web': ['http://', 'https://'],\n",
    "            'pdf': ['.pdf'],\n",
    "            'text': ['.txt', '.md', '.rst'],\n",
    "            'csv': ['.csv'],\n",
    "            'html': ['.html', '.htm']\n",
    "        }\n",
    "        \n",
    "        # Store loaded documents in tutorial state\n",
    "        if 'loaded_documents' not in tutorial_state:\n",
    "            tutorial_state['loaded_documents'] = {}\n",
    "        \n",
    "        print(\"📚 Document Loading System initialized\")\n",
    "        print(f\"   Supported formats: {list(self.supported_formats.keys())}\")\n",
    "    \n",
    "    def load_web_content(self, urls: List[str], loader_type: str = \"basic\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load content from web URLs with different strategies\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to scrape\n",
    "            loader_type: Type of web loader to use\n",
    "        \"\"\"\n",
    "        print(f\"🌐 Loading web content from {len(urls)} URLs using {loader_type} loader\")\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        if loader_type == \"basic\":\n",
    "            # Simple web loader for static content\n",
    "            loader = WebBaseLoader(urls)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            for doc in docs:\n",
    "                documents.append({\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata,\n",
    "                    'source_type': 'web_basic',\n",
    "                    'loader_used': 'WebBaseLoader'\n",
    "                })\n",
    "                \n",
    "        elif loader_type == \"recursive\":\n",
    "            # Recursive loader for documentation sites\n",
    "            if len(urls) == 1:\n",
    "                loader = RecursiveUrlLoader(\n",
    "                    url=urls[0],\n",
    "                    max_depth=2,  # Limit depth to avoid too much content\n",
    "                    extractor=lambda x: x.get_text()\n",
    "                )\n",
    "                docs = loader.load()\n",
    "                \n",
    "                for doc in docs:\n",
    "                    documents.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata,\n",
    "                        'source_type': 'web_recursive',\n",
    "                        'loader_used': 'RecursiveUrlLoader'\n",
    "                    })\n",
    "            else:\n",
    "                print(\"⚠️ Recursive loader supports single URL only, using first URL\")\n",
    "                return self.load_web_content([urls[0]], loader_type)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(documents)} web documents\")\n",
    "        return documents\n",
    "    \n",
    "    def load_pdf_documents(self, pdf_paths: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load PDF documents with metadata extraction\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths: List of paths to PDF files\n",
    "        \"\"\"\n",
    "        print(f\"📄 Loading {len(pdf_paths)} PDF documents\")\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"⚠️ File not found: {pdf_path}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                for i, doc in enumerate(docs):\n",
    "                    documents.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': {\n",
    "                            **doc.metadata,\n",
    "                            'page_number': i + 1,\n",
    "                            'total_pages': len(docs),\n",
    "                            'file_path': pdf_path\n",
    "                        },\n",
    "                        'source_type': 'pdf',\n",
    "                        'loader_used': 'PyPDFLoader'\n",
    "                    })\n",
    "                    \n",
    "                print(f\"✅ Loaded {len(docs)} pages from {pdf_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {pdf_path}: {str(e)}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def load_text_documents(self, text_paths: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load plain text documents\n",
    "        \n",
    "        Args:\n",
    "            text_paths: List of paths to text files\n",
    "        \"\"\"\n",
    "        print(f\"📝 Loading {len(text_paths)} text documents\")\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for text_path in text_paths:\n",
    "            if not os.path.exists(text_path):\n",
    "                print(f\"⚠️ File not found: {text_path}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                loader = TextLoader(text_path, encoding='utf-8')\n",
    "                docs = loader.load()\n",
    "                \n",
    "                for doc in docs:\n",
    "                    documents.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': {\n",
    "                            **doc.metadata,\n",
    "                            'file_path': text_path,\n",
    "                            'file_size': os.path.getsize(text_path)\n",
    "                        },\n",
    "                        'source_type': 'text',\n",
    "                        'loader_used': 'TextLoader'\n",
    "                    })\n",
    "                    \n",
    "                print(f\"✅ Loaded text document: {text_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {text_path}: {str(e)}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def load_directory(self, directory_path: str, glob_pattern: str = \"**/*\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load all supported documents from a directory\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory\n",
    "            glob_pattern: Pattern to match files\n",
    "        \"\"\"\n",
    "        print(f\"📁 Loading documents from directory: {directory_path}\")\n",
    "        \n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"❌ Directory not found: {directory_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Create directory loader\n",
    "        loader = DirectoryLoader(\n",
    "            directory_path,\n",
    "            glob=glob_pattern,\n",
    "            show_progress=True,\n",
    "            use_multithreading=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            docs = loader.load()\n",
    "            documents = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                documents.append({\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata,\n",
    "                    'source_type': 'directory',\n",
    "                    'loader_used': 'DirectoryLoader'\n",
    "                })\n",
    "            \n",
    "            print(f\"✅ Loaded {len(documents)} documents from directory\")\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading directory: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def create_sample_documents(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create sample documents for demonstration\n",
    "        \"\"\"\n",
    "        print(\"📝 Creating sample documents for demonstration\")\n",
    "        \n",
    "        # Sample web content (simulated)\n",
    "        sample_web_docs = [\n",
    "            {\n",
    "                'content': \"\"\"\n",
    "                # AI and Machine Learning Guide\n",
    "                \n",
    "                Artificial Intelligence (AI) represents one of the most transformative technologies of our time. \n",
    "                Machine Learning, a subset of AI, enables systems to automatically learn and improve from experience \n",
    "                without being explicitly programmed.\n",
    "                \n",
    "                ## Key Concepts:\n",
    "                - **Supervised Learning**: Learning with labeled training data\n",
    "                - **Unsupervised Learning**: Finding patterns in unlabeled data  \n",
    "                - **Reinforcement Learning**: Learning through interaction and feedback\n",
    "                \n",
    "                ## Applications:\n",
    "                - Natural Language Processing\n",
    "                - Computer Vision\n",
    "                - Predictive Analytics\n",
    "                - Autonomous Systems\n",
    "                \"\"\",\n",
    "                'metadata': {\n",
    "                    'source': 'https://example.com/ai-guide',\n",
    "                    'title': 'AI and Machine Learning Guide',\n",
    "                    'language': 'en'\n",
    "                },\n",
    "                'source_type': 'web_sample',\n",
    "                'loader_used': 'SampleGenerator'\n",
    "            },\n",
    "            {\n",
    "                'content': \"\"\"\n",
    "                # RAG Systems Documentation\n",
    "                \n",
    "                Retrieval-Augmented Generation (RAG) combines the power of large language models \n",
    "                with external knowledge retrieval to provide more accurate and up-to-date responses.\n",
    "                \n",
    "                ## Core Components:\n",
    "                1. **Document Processing**: Loading, splitting, and chunking documents\n",
    "                2. **Vector Storage**: Embedding documents for semantic search\n",
    "                3. **Retrieval**: Finding relevant context for queries\n",
    "                4. **Generation**: Producing responses with retrieved context\n",
    "                \n",
    "                ## Benefits:\n",
    "                - Access to current information beyond training data\n",
    "                - Reduced hallucination through grounded responses\n",
    "                - Domain-specific knowledge integration\n",
    "                - Scalable knowledge updates without retraining\n",
    "                \"\"\",\n",
    "                'metadata': {\n",
    "                    'source': 'https://docs.example.com/rag',\n",
    "                    'title': 'RAG Systems Documentation',\n",
    "                    'category': 'technical'\n",
    "                },\n",
    "                'source_type': 'web_sample',\n",
    "                'loader_used': 'SampleGenerator'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Sample technical document\n",
    "        sample_tech_docs = [\n",
    "            {\n",
    "                'content': \"\"\"\n",
    "                # Python Best Practices for AI Development\n",
    "                \n",
    "                ## Code Organization\n",
    "                - Use virtual environments for dependency management\n",
    "                - Follow PEP 8 style guidelines\n",
    "                - Implement proper error handling\n",
    "                - Write comprehensive tests\n",
    "                \n",
    "                ## Performance Optimization\n",
    "                ```python\n",
    "                # Use vectorized operations with NumPy\n",
    "                import numpy as np\n",
    "                \n",
    "                # Instead of loops\n",
    "                result = np.array([x**2 for x in data])\n",
    "                \n",
    "                # Use vectorization\n",
    "                result = np.square(data)\n",
    "                ```\n",
    "                \n",
    "                ## Memory Management\n",
    "                - Use generators for large datasets\n",
    "                - Implement proper garbage collection\n",
    "                - Monitor memory usage during training\n",
    "                \"\"\",\n",
    "                'metadata': {\n",
    "                    'source': 'internal_docs/python_practices.md',\n",
    "                    'author': 'AI Team',\n",
    "                    'last_updated': '2024-10-23'\n",
    "                },\n",
    "                'source_type': 'markdown_sample',\n",
    "                'loader_used': 'SampleGenerator'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        all_samples = sample_web_docs + sample_tech_docs\n",
    "        print(f\"✅ Created {len(all_samples)} sample documents\")\n",
    "        \n",
    "        return all_samples\n",
    "\n",
    "# Initialize the document loading system\n",
    "doc_loader = DocumentLoadingSystem()\n",
    "\n",
    "# Store in tutorial state\n",
    "tutorial_state['doc_loader'] = doc_loader\n",
    "print(\"📚 Document Loading System ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2383753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Document Loading Capabilities\n",
    "\n",
    "print(\"🧪 TESTING DOCUMENT LOADING CAPABILITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Load sample documents (for demonstration)\n",
    "print(\"\\n📝 Test 1: Loading Sample Documents\")\n",
    "sample_docs = doc_loader.create_sample_documents()\n",
    "\n",
    "# Display sample document info\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    print(f\"\\n   Document {i+1}:\")\n",
    "    print(f\"   Title: {doc['metadata'].get('title', 'No title')}\")\n",
    "    print(f\"   Type: {doc['source_type']}\")\n",
    "    print(f\"   Content length: {len(doc['content'])} characters\")\n",
    "    print(f\"   Preview: {doc['content'][:100]}...\")\n",
    "\n",
    "# Test 2: Web content loading (simulated with sample URLs)\n",
    "print(f\"\\n🌐 Test 2: Web Content Loading Demo\")\n",
    "print(\"   Note: Using sample content to demonstrate web loading capabilities\")\n",
    "\n",
    "# Simulate web loading\n",
    "web_urls = [\n",
    "    \"https://docs.python.org/3/tutorial/\",\n",
    "    \"https://langchain.readthedocs.io/\"\n",
    "]\n",
    "\n",
    "print(f\"   Simulated loading from: {web_urls[0]}\")\n",
    "print(\"   In production, this would fetch live content from these URLs\")\n",
    "\n",
    "# Test 3: Document metadata analysis\n",
    "print(f\"\\n📊 Test 3: Document Metadata Analysis\")\n",
    "\n",
    "metadata_summary = {}\n",
    "for doc in sample_docs:\n",
    "    doc_type = doc['source_type']\n",
    "    if doc_type not in metadata_summary:\n",
    "        metadata_summary[doc_type] = {\n",
    "            'count': 0,\n",
    "            'total_chars': 0,\n",
    "            'loaders_used': set()\n",
    "        }\n",
    "    \n",
    "    metadata_summary[doc_type]['count'] += 1\n",
    "    metadata_summary[doc_type]['total_chars'] += len(doc['content'])\n",
    "    metadata_summary[doc_type]['loaders_used'].add(doc['loader_used'])\n",
    "\n",
    "print(\"   Document Type Summary:\")\n",
    "for doc_type, stats in metadata_summary.items():\n",
    "    print(f\"     {doc_type}:\")\n",
    "    print(f\"       Count: {stats['count']}\")\n",
    "    print(f\"       Total characters: {stats['total_chars']}\")\n",
    "    print(f\"       Loaders used: {', '.join(stats['loaders_used'])}\")\n",
    "\n",
    "# Store results\n",
    "tutorial_state['loaded_documents']['samples'] = sample_docs\n",
    "tutorial_state['loading_metadata'] = metadata_summary\n",
    "\n",
    "print(f\"\\n✅ DOCUMENT LOADING TESTS COMPLETE\")\n",
    "print(f\"   Loaded {len(sample_docs)} documents successfully\")\n",
    "print(\"   Ready to proceed to document splitting and chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbf6bd",
   "metadata": {},
   "source": [
    "#### Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7679573",
   "metadata": {},
   "source": [
    "explain different types of document splitting, the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfbca2",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab550fe4",
   "metadata": {},
   "source": [
    "explain different types of document chunking , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d351c",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39eed15",
   "metadata": {},
   "source": [
    "explain different types of document embedding. the math behind them if needed , their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d238e7",
   "metadata": {},
   "source": [
    "### Storing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb48998",
   "metadata": {},
   "source": [
    "introduce to storing documents, the math behind them if needed different ways of representing them and how different types of documents can be fed to rag and stuff etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477119b6",
   "metadata": {},
   "source": [
    "#### Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9f893",
   "metadata": {},
   "source": [
    "explain different types of vector database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608dc711",
   "metadata": {},
   "source": [
    "#### Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e19279",
   "metadata": {},
   "source": [
    "explain different types of vector database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090d616",
   "metadata": {},
   "source": [
    "#### SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b8e8c",
   "metadata": {},
   "source": [
    "explain different types of sql database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8555b",
   "metadata": {},
   "source": [
    "### Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dcc54",
   "metadata": {},
   "source": [
    "introduce to retreiver mechanisms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c40d32",
   "metadata": {},
   "source": [
    "explain different types of retreival mechanisms , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbce248",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b8387",
   "metadata": {},
   "source": [
    "introduce to evaluation mechniasms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd43da7",
   "metadata": {},
   "source": [
    "explain different types of evaluation methods , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f82ea2",
   "metadata": {},
   "source": [
    "## A Complete Agentic System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11448f82",
   "metadata": {},
   "source": [
    "## Limitations & Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ba15",
   "metadata": {},
   "source": [
    "#### RAPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a3d4e",
   "metadata": {},
   "source": [
    "#### Self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d098",
   "metadata": {},
   "source": [
    "#### CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d56e81",
   "metadata": {},
   "source": [
    "#### Adaptive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adba2b0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813de77",
   "metadata": {},
   "source": [
    "#### Workflow Pattern Selection Guide & Best Practices\n",
    "\n",
    "Choosing the right workflow pattern is crucial for building effective agentic systems. Here's a comprehensive guide based on production experience and Anthropic's research:\n",
    "\n",
    "**🔗 Prompt Chaining** - Use when:\n",
    "- Tasks can be cleanly decomposed into sequential steps\n",
    "- Each step benefits from focused attention\n",
    "- Quality is more important than latency\n",
    "- You need programmatic validation gates\n",
    "- Examples: Content generation → review → translation → cultural adaptation\n",
    "\n",
    "**📍 Routing** - Use when:\n",
    "- Input types have distinct handling requirements  \n",
    "- Specialized expertise improves outcomes significantly\n",
    "- Classification can be performed reliably\n",
    "- Different cost/performance tradeoffs exist per route\n",
    "- Examples: Customer service triage, query complexity routing\n",
    "\n",
    "**⚡ Parallelization** - Use when:\n",
    "- **Sectioning**: Independent subtasks can run simultaneously\n",
    "- **Voting**: Multiple perspectives improve decision confidence\n",
    "- Latency reduction is critical\n",
    "- Ensemble methods provide measurable accuracy gains\n",
    "- Examples: Multi-aspect analysis, content moderation, code review\n",
    "\n",
    "**🎯 Orchestrator-Workers** - Use when:\n",
    "- Task requirements can't be predicted in advance\n",
    "- Dynamic subtask generation is needed\n",
    "- Different specialists handle different aspects\n",
    "- Complex coordination is required\n",
    "- Examples: Software development, research synthesis, creative projects\n",
    "\n",
    "**🔄 Evaluator-Optimizer** - Use when:\n",
    "- Iterative refinement demonstrably improves quality\n",
    "- Clear evaluation criteria exist\n",
    "- The LLM can provide meaningful self-criticism\n",
    "- Quality improvement justifies additional latency\n",
    "- Examples: Creative writing, complex analysis, strategic planning\n",
    "\n",
    "**🤖 Autonomous Agents** - Use when:\n",
    "- Open-ended problems with unpredictable steps\n",
    "- Long-running tasks requiring persistence\n",
    "- Environment interaction and feedback loops exist\n",
    "- Human oversight can be incorporated at checkpoints\n",
    "- Trust level supports autonomous operation\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "1. **Start Simple**: Begin with the simplest pattern that meets requirements\n",
    "2. **Measure Performance**: Always evaluate accuracy, latency, and cost tradeoffs\n",
    "3. **Error Handling**: Implement robust error recovery and fallback strategies\n",
    "4. **Human Oversight**: Include checkpoints for critical decisions\n",
    "5. **Composability**: Patterns can be combined for sophisticated workflows\n",
    "6. **Tool Design**: Invest heavily in clear, well-documented tool interfaces\n",
    "7. **Testing**: Extensive testing in sandboxed environments before production\n",
    "\n",
    "\n",
    "### Memory Systems Quick Reference\n",
    "\n",
    "Now that we've seen memory systems in action, here's a practical guide for choosing the right approach:\n",
    "\n",
    "| Memory Type | Best Use Case | Pros | Cons | Complexity |\n",
    "|-------------|---------------|------|------|------------|\n",
    "| **ConversationBufferMemory** | Short, detail-critical conversations | Perfect recall, simple setup | Linear cost growth, token limits | O(n) |\n",
    "| **ConversationSummaryMemory** | Long-term relationships, key themes | Scales indefinitely, preserves important info | Loses detail, summarization overhead | O(log n) |\n",
    "| **ConversationBufferWindowMemory** | Task-oriented, recent context matters | Predictable performance, constant cost | Forgets older context completely | O(k) |\n",
    "| **ConversationTokenBufferMemory** | Production apps, cost control | Optimal context usage, never exceeds limits | Complex token counting logic | O(tokens) |\n",
    "| **ConversationEntityMemory** | Relationship tracking, complex scenarios | Maintains entity relationships, intelligent context | Requires entity extraction, higher complexity | O(entities) |\n",
    "| **CombinedMemory** | Sophisticated applications | Leverages multiple approaches, flexible | Complex setup, coordination overhead | O(combined) |\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "- 📝 **Need perfect recall?** → Buffer Memory\n",
    "- 🔄 **Long conversations?** → Summary Memory  \n",
    "- ⚡ **Recent context only?** → Window Memory\n",
    "- 💰 **Cost control critical?** → Token Memory\n",
    "- 👥 **Tracking relationships?** → Entity Memory\n",
    "- 🧠 **Multiple requirements?** → Combined Memory\n",
    "\n",
    "**Memory Performance Characteristics:**\n",
    "- **Buffer**: Grows with conversation length - great for short, detailed discussions\n",
    "- **Summary**: Logarithmic growth - ideal for ongoing relationships  \n",
    "- **Window**: Constant size - perfect for task-focused interactions\n",
    "- **Token**: Bounded growth - excellent for production cost control\n",
    "- **Entity**: Scales with entities - powerful for complex relationship tracking\n",
    "- **Combined**: Flexible scaling - adaptable to diverse requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6403629",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de317b91",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
