{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36be9c2",
   "metadata": {},
   "source": [
    "# Agents and RAG, A Technical Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92002c",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389560b9",
   "metadata": {},
   "source": [
    "I'll go through the fundamentals of Agents and rag with the help of langchain library \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/awan_getting_langchain_ecosystem_1-1024x574.png\" width=700>\n",
    "\n",
    "<img src=\"https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2023/10/12015949/LlamaIndex.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb5de",
   "metadata": {},
   "source": [
    "### Brief History\n",
    "\n",
    "Before we dive into building agents, let's take a moment to understand the journey that brought us to this exciting point in AI history. Understanding where agents came from will help you appreciate why the systems we're building today represent such a significant breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4eff09",
   "metadata": {},
   "source": [
    "Let me tell you a story about how we got here. The concept of intelligent agents has evolved dramatically over the past seven decades, transforming from simple rule-based systems to today's sophisticated AI companions that can reason, plan, and act autonomously. \n",
    "\n",
    "**The Early Days (1950s-1980s):** Understanding this progression is essential because it helps us appreciate why modern agentic systems represent such a breakthrough. The journey began in the 1950s when researchers like Allen Newell and Herbert Simon created the Logic Theorist, a program that could prove mathematical theorems by exploring different logical paths. These early agents were like skilled craftsmen—they could perform specific tasks very well, but only within narrow, pre-defined domains.\n",
    "\n",
    "The 1970s and 1980s brought expert systems like MYCIN for medical diagnosis and DENDRAL for chemical analysis. While impressive, these systems required months of manual knowledge engineering, where human experts had to explicitly encode their domain knowledge into rigid rule sets. Imagine trying to teach someone to be a doctor by writing down every possible symptom combination and treatment - that's essentially what early AI researchers had to do!\n",
    "\n",
    "**The Networking Era (1990s-2000s):** The 1990s marked a shift toward more flexible software agents that could operate in networked environments and coordinate with other agents. This period introduced the concept of multi-agent systems, where multiple specialized agents could collaborate to solve complex problems. However, these systems still required extensive manual programming and could only handle situations their creators had anticipated.\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*Ygen57Qiyrc8DXAFsjZLNA.gif\" width=700>\n",
    "\n",
    "**The Learning Revolution (2000s-2010s):** The real transformation began in the 2000s with machine learning advances. Agents could now learn from data rather than relying solely on hand-coded rules. Virtual assistants like Siri and Alexa brought agent technology to mainstream consumers, though they remained relatively narrow in scope—essentially sophisticated voice interfaces for search and simple task execution.\n",
    "\n",
    "**The LLM Breakthrough (2020s):** The breakthrough moment arrived with large language models starting around 2020. Systems like GPT-3 and GPT-4 combined vast knowledge with sophisticated reasoning abilities, creating agents that could understand natural language, maintain context across conversations, and tackle a wide variety of tasks without task-specific programming. \n",
    "\n",
    "Unlike their predecessors, these modern agents can break down complex problems into steps, use external tools when needed, and adapt to new situations they've never encountered before. This evolution represents a fundamental shift from automation to augmentation—where early agents automated specific, predefined tasks, today's agents can understand our goals and work as collaborative partners in problem-solving.\n",
    "\n",
    "**Why This History Matters for You:** Understanding this evolution helps us appreciate that we're not just building better chatbots—we're creating systems that can handle ambiguous instructions, incomplete information, and constantly changing contexts. These capabilities make them invaluable for building sophisticated applications like the retrieval-augmented generation systems we'll explore in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0100815",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b4ae",
   "metadata": {},
   "source": [
    "When we talk about agents in 2025, we're entering a landscape where the term has become both ubiquitous and somewhat ambiguous. Different organizations and researchers use \"agent\" to describe everything from simple chatbots to fully autonomous systems that can operate independently for weeks. \n",
    "\n",
    "Another confusion lies with reinforcement learning name conventions, the agent described in reinforcement learnign is different from the LLM agents that we deal with now, even though, they share similar vision.\n",
    "\n",
    "But don't let this confusion discourage you! This diversity in definition isn't just academic—it reflects fundamentally different architectural approaches that will determine how we build the next generation of AI applications. Let me help you navigate this landscape.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!A_Oy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3177e12-432e-4e41-814f-6febf7a35f68_1360x972.png\" width=700>\n",
    "\n",
    "**What Actually Makes an Agent?** At its core, an agent is a system that can perceive its environment, make decisions, and take actions to achieve specific goals. Sounds simple, right? But the way these capabilities are implemented varies dramatically.\n",
    "\n",
    "Some define agents as fully autonomous systems that operate independently over extended periods, using various tools and adapting their strategies based on feedback. Think of these like a personal assistant who can manage your entire schedule, book flights, handle emails, and make decisions on your behalf without constant supervision.\n",
    "\n",
    "Others use the term more broadly to describe any system that follows predefined workflows to accomplish tasks. These implementations are more like following a detailed recipe—each step is predetermined, and while the system can handle some variations, it operates within clearly defined boundaries.\n",
    "\n",
    "**Why This Distinction Matters to You:** The difference between these approaches is crucial because it affects everything from system reliability to development complexity. Understanding this spectrum will help you choose the right approach for your specific needs and avoid over-engineering solutions.\n",
    "\n",
    "**The Spectrum of Control:** The most useful way to think about this spectrum is through the lens of control and decision-making:\n",
    "\n",
    "- **Workflows** are systems where large language models and tools are orchestrated through predefined code paths. Every decision point is anticipated by the developer, and the system follows predetermined logic to handle different scenarios.\n",
    "\n",
    "- **Agents** are systems where the LLM dynamically directs its own processes and tool usage, maintaining control over how it accomplishes tasks. The model itself decides what to do next, which tools to use, and how to adapt when things don't go as planned.\n",
    "\n",
    "Think of workflows as following a GPS route—you know exactly where you're going and how to get there. Agents are more like having an experienced local guide who can adapt the route based on traffic, weather, or interesting stops along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069523fb",
   "metadata": {},
   "source": [
    "#### Simplicity Defines Perfectionism, Not Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db57831",
   "metadata": {},
   "source": [
    "Now, here's some advice that might surprise you: when building applications with LLMs, the fundamental principle should be finding the simplest solution that meets your requirements. This might mean not building agentic systems at all!\n",
    "\n",
    "Let me explain why this matters. Agentic systems inherently trade latency and cost for better task performance. Every additional decision point, tool call, and reasoning step adds time and expense to your application. You need to carefully consider when this tradeoff makes sense for your specific use case.\n",
    "\n",
    "**When to Choose Workflows:** Workflows offer predictability and consistency for well-defined tasks where you can anticipate most scenarios and edge cases. They're excellent for:\n",
    "- Standardized processes like data processing pipelines\n",
    "- Content moderation workflows\n",
    "- Structured analysis tasks\n",
    "- Any situation where you need reliable, repeatable results\n",
    "\n",
    "**When to Choose Agents:** Agents become the better choice when you need flexibility and model-driven decision-making at scale. This includes situations where:\n",
    "- The variety of inputs and required responses is too broad to predefine\n",
    "- The system needs to adapt to entirely new scenarios\n",
    "- You're dealing with open-ended problems that require creative problem-solving\n",
    "- The complexity of decision trees would make workflow programming impractical\n",
    "\n",
    "**The Simple Truth:** Here's what I've learned from building production AI systems: for many applications, the most effective approach involves optimizing single LLM calls with retrieval and in-context examples rather than building complex agentic systems. \n",
    "\n",
    "Before you architect a sophisticated multi-agent system with elaborate tool chains, ask yourself: \"Could I solve this with a well-crafted prompt and some good examples?\" You'd be surprised how often the answer is yes.\n",
    "\n",
    "**But When Complexity is Worth It:** However, as we'll explore throughout this tutorial, there are compelling scenarios where the additional complexity of agents becomes not just beneficial, but necessary for achieving your goals. Understanding when and how to make this transition is what separates effective AI system builders from those who over-engineer solutions to problems that could be solved more simply.\n",
    "\n",
    "The key is developing good judgment about when to add complexity. Start simple, measure performance, and only add complexity when you can clearly demonstrate that it improves outcomes for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bd7ab",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Let's start with the most fundamental skill you'll need as an agent builder: crafting effective prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb1b8e",
   "metadata": {},
   "source": [
    "Let's talk about the foundation of everything we'll build: prompts. Think of prompts as the bridge between human intent and AI capabilities—they're how we translate our natural language requests into structured instructions that language models can understand and act upon.\n",
    "\n",
    "But here's what makes prompts fascinating in agentic systems: they're not just about getting good answers to single questions. In the context of agents, prompts become the architectural blueprints that define not only *what* we want the agent to accomplish, but *how* the agent should approach problem-solving, what tools it can use, and how it should reason through complex tasks.\n",
    "\n",
    "**Why Prompts Are Your Most Important Tool:** I like to think of prompts as the instruction manual for your AI agent. Just as a well-written manual can make the difference between a novice successfully assembling furniture or ending up with a pile of confused parts, a well-crafted prompt determines whether your agent performs brilliantly or struggles to understand your intent.\n",
    "\n",
    "The quality and structure of your prompts directly influence:\n",
    "- The agent's reasoning capabilities\n",
    "- How it chooses and uses tools  \n",
    "- Its overall effectiveness in completing tasks\n",
    "- The consistency of results across different inputs\n",
    "\n",
    "<img src=\"https://www.datablist.com/_next/image?url=%2Fhowto_images%2Fhow-to-write-prompt-ai-agents%2Fstructured-ai-agent-prompt.png&w=3840&q=75\" width=700>\n",
    "\n",
    "**The Different Types of Prompts You'll Use:** As we build more sophisticated systems, you'll work with several types of prompts, each serving different purposes:\n",
    "\n",
    "- **System prompts** establish the agent's role, personality, and fundamental operating principles—these are like giving someone their job description and company handbook before they start work\n",
    "- **User prompts** contain the specific tasks or questions you want the agent to handle\n",
    "- **Few-shot prompts** provide examples of desired input-output patterns to guide the agent's responses\n",
    "- **Chain-of-thought prompts** encourage step-by-step reasoning, helping agents break down complex problems into manageable pieces\n",
    "\n",
    "**The Multi-Step Challenge:** In multi-step agentic workflows, prompt engineering becomes particularly sophisticated because you need to design prompts that not only solve individual tasks but also coordinate between different stages of processing. The agent needs to understand when to use specific tools, how to interpret tool outputs, and how to maintain context across multiple interaction cycles.\n",
    "\n",
    "This requires careful consideration of prompt structure, token efficiency, and the logical flow of information through your system. Don't worry—we'll practice all of this together as we build real systems.\n",
    "\n",
    "**Let's See It in Action:** Now that you understand why prompts matter so much, let's explore how to implement effective prompt templates using LangChain with Google's Gemini model. We'll start with basics and gradually work up to sophisticated multi-step prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal setup and imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "# Single shared LLM for the tutorial (reuse this everywhere)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.3,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Small shared state for examples\n",
    "tutorial_state = {\n",
    "    \"prompt_templates\": {},\n",
    "    \"chains\": {},\n",
    "    \"demo_data\": {},\n",
    "}\n",
    "\n",
    "print(\"Setup complete — shared LLM and tutorial_state are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dba25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the tutorial\n",
    "%pip install langchain langchain-google-genai langchain-core numpy\n",
    "\n",
    "print(\"📦 Installing packages for Agents and RAG tutorial...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416983e",
   "metadata": {},
   "source": [
    "we'll create some prompt examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create prompt templates that we'll reuse throughout the tutorial\n",
    "# These will work with our global llm instance\n",
    "\n",
    "def setup_prompt_templates():\n",
    "    \"\"\"Initialize reusable prompt templates for the tutorial\"\"\"\n",
    "    \n",
    "    # Basic instructional prompt - for general explanations\n",
    "    basic_template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"audience\"],\n",
    "        template=\"\"\"You are an expert educator who excels at explaining complex topics clearly.\n",
    "        \n",
    "        Topic: {topic}\n",
    "        Audience: {audience}\n",
    "        \n",
    "        Please provide a clear, engaging explanation that includes:\n",
    "        1. Core concept definition\n",
    "        2. Relevant examples or analogies  \n",
    "        3. Key takeaways for the audience level\n",
    "        \n",
    "        Keep your explanation appropriate for the specified audience.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Conversational prompt - for interactive discussions\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant with expertise in technology and science. \n",
    "        You provide accurate, clear explanations and engage in detailed discussions.\n",
    "        Always think step-by-step when solving problems and explain your reasoning.\"\"\"),\n",
    "        (\"human\", \"I need help understanding {concept}. Can you break it down for me?\"),\n",
    "        (\"ai\", \"I'd be happy to help explain {concept}! Let me break this down step by step.\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    \n",
    "    # Store templates in tutorial_state for reuse throughout the notebook\n",
    "    tutorial_state[\"prompt_templates\"] = {\n",
    "        \"basic\": basic_template,\n",
    "        \"chat\": chat_template\n",
    "    }\n",
    "    \n",
    "    # Create reusable chains with our global llm\n",
    "    tutorial_state[\"chains\"] = {\n",
    "        \"basic\": basic_template | llm | StrOutputParser(),\n",
    "        \"chat\": chat_template | llm | StrOutputParser()\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Prompt templates created and stored in tutorial_state\")\n",
    "    print(\"🔗 Chains connected to our global llm instance\")\n",
    "    print(\"📝 Templates available: basic, chat\")\n",
    "    return tutorial_state[\"prompt_templates\"]\n",
    "\n",
    "# Initialize our reusable prompt system\n",
    "prompt_templates = setup_prompt_templates()\n",
    "\n",
    "print(\"\\n\udca1 These templates will be reused throughout the tutorial\")\n",
    "print(\"\udd04 No need to recreate them - they're stored in tutorial_state\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacbea9",
   "metadata": {},
   "source": [
    "Great! now our LLM can respond to our questions, but how can we tweak it more to determine how much it weighs the prompt guideline while responding with it's own knowledge and reasoning? let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c858ca",
   "metadata": {},
   "source": [
    "###  Hyperparameters\n",
    "\n",
    "Once you've mastered basic prompting, the next level of control comes from understanding how to tune your model's behavior through hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d7ff9",
   "metadata": {},
   "source": [
    "Now let's dive into one of the most fascinating aspects of working with language models: hyperparameters. These are the control knobs that determine how a language model generates responses, acting like the settings on a sophisticated instrument that can dramatically change the output quality and behavior.\n",
    "\n",
    "**Why Understanding Hyperparameters Matters:** Understanding these parameters is crucial for building effective agents because they directly influence:\n",
    "- How the model balances following prompt instructions versus drawing on its pre-trained knowledge\n",
    "- How creative or conservative its responses are\n",
    "- How consistently it behaves across multiple interactions\n",
    "- Whether it takes safe, predictable paths or explores more novel solutions\n",
    "\n",
    "Let me walk you through the key parameters and show you the mathematical foundations that drive their behavior.\n",
    "\n",
    "**Temperature (τ) - The Creativity Knob:** Temperature controls the randomness in the model's token selection process through the softmax function. Here's how it works mathematically:\n",
    "\n",
    "Given logits $z_i$ for each possible token $i$, the probability distribution is calculated as:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{z_i/τ}}{\\sum_{j=1}^{V} e^{z_j/τ}}$$\n",
    "\n",
    "Where:\n",
    "- $τ$ (tau) is the temperature parameter\n",
    "- $V$ is the vocabulary size  \n",
    "- Lower $τ$ → sharper distribution (more deterministic)\n",
    "- Higher $τ$ → flatter distribution (more random)\n",
    "\n",
    "At $τ = 1$, we get the standard softmax. As $τ → 0$, the distribution approaches a one-hot encoding of the highest logit (very predictable). As $τ → ∞$, the distribution becomes uniform (completely random).\n",
    "\n",
    "**Top-p (Nucleus Sampling) - The Focus Control:** Top-p works by selecting the smallest set of tokens whose cumulative probability exceeds threshold $p$:\n",
    "\n",
    "$$\\text{Nucleus} = \\{i : \\sum_{j \\in \\text{top-k tokens}} P(token_j) \\leq p\\}$$\n",
    "\n",
    "This creates a dynamic vocabulary size—sometimes the model considers many options, sometimes just a few, depending on how confident it is.\n",
    "\n",
    "**Top-k - The Hard Limit:** Top-k simply restricts consideration to the $k$ highest-probability tokens, where $k$ is a fixed integer. It's simpler than top-p but less adaptive.\n",
    "\n",
    "**Practical Control Parameters:**\n",
    "- **Max tokens** provides an upper bound $N_{max}$ on sequence length\n",
    "- **Stop sequences** define termination conditions based on specific token patterns\n",
    "\n",
    "**The Art of Parameter Selection:** The key insight is that these parameters create fundamental tradeoffs. You're not just adjusting \"creativity\"—you're choosing between instruction-following precision and knowledge-bringing flexibility. \n",
    "\n",
    "For agents, this choice becomes critical: Do you want an agent that follows instructions exactly, or one that can creatively adapt its approach? The answer depends entirely on your use case.\n",
    "\n",
    "Let's explore how these parameters affect model behavior in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af59b",
   "metadata": {},
   "source": [
    "we'll have three types of model instances defined to differentiate between their creativity and max tokens as far as we can get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's explore how hyperparameters affect our existing LLM's behavior\n",
    "# We'll create variants using our global llm configuration as a template\n",
    "\n",
    "def demonstrate_temperature_effects(topic=\"quantum computing\"):\n",
    "    \"\"\"\n",
    "    Demonstrate how temperature affects the same LLM's responses\n",
    "    We'll use our existing llm instance and adjust only temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use our existing prompt template from tutorial_state\n",
    "    prompt = tutorial_state[\"prompt_templates\"][\"basic\"]\n",
    "    \n",
    "    # Create temperature variants using the same model as our global llm\n",
    "    temperatures = {\n",
    "        \"conservative\": 0.1,   # τ = 0.1 for high determinism\n",
    "        \"balanced\": 0.7,       # τ = 0.7 (same as our global llm)\n",
    "        \"creative\": 1.2        # τ = 1.2 for high creativity\n",
    "    }\n",
    "    \n",
    "    print(\"🌡️ TESTING TEMPERATURE EFFECTS ON RESPONSES\")\n",
    "    print(f\"Using the same model: {llm.model}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, temp_value in temperatures.items():\n",
    "        # Create a temporary llm instance with different temperature\n",
    "        temp_llm = ChatGoogleGenerativeAI(\n",
    "            model=llm.model,  # Same model as global llm\n",
    "            temperature=temp_value,\n",
    "            max_tokens=150,\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Use our existing chain pattern\n",
    "        chain = prompt | temp_llm | StrOutputParser()\n",
    "        response = chain.invoke({\n",
    "            \"topic\": topic,\n",
    "            \"audience\": \"technical professionals\"\n",
    "        })\n",
    "        \n",
    "        results[config_name] = response\n",
    "        print(f\"\\n{config_name.upper()} (τ={temp_value}):\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Store results in our tutorial state\n",
    "    tutorial_state[\"demo_data\"][\"hyperparameter_comparison\"] = results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with our reusable function\n",
    "print(\"\\n🧪 Demonstrating how temperature affects our LLM's behavior\")\n",
    "hyperparameter_results = demonstrate_temperature_effects()\n",
    "\n",
    "print(\"\\n✅ Temperature demonstration complete\")\n",
    "print(\"📊 Notice how the same model produces different outputs at different temperatures\")\n",
    "print(\"💡 Our global llm uses τ=0.3 for balanced results throughout the tutorial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfe18f",
   "metadata": {},
   "source": [
    "now let's see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24947aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our hyperparameter demonstrations and see the results\n",
    "print(\"🧪 Running hyperparameter demonstrations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test temperature effects using the function we defined\n",
    "print(\"\\n1️⃣ Testing Temperature Effects on the Same Query\")\n",
    "temp_results = demonstrate_temperature_effects(topic=\"neural networks\")\n",
    "\n",
    "print(\"\\n2️⃣ Analyzing the Results\")\n",
    "print(\"Notice how the same model at different temperatures produces:\")\n",
    "print(\"   • Conservative (τ=0.1): Focused, predictable responses\")\n",
    "print(\"   • Balanced (τ=0.7): Mix of consistency and variety\") \n",
    "print(\"   • Creative (τ=1.2): More diverse, exploratory responses\")\n",
    "\n",
    "print(\"\\n💡 Our global llm uses τ=0.3 throughout this tutorial\")\n",
    "print(\"   This gives us reliable, consistent behavior while allowing some flexibility\")\n",
    "\n",
    "print(\"\\n✅ Hyperparameter demonstrations complete\")\n",
    "print(\"📊 Results stored in tutorial_state['demo_data']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347026a1",
   "metadata": {},
   "source": [
    "**What We Just Discovered:** The examples above demonstrate something fundamental about how hyperparameters work in practice. They create a crucial tradeoff between instruction following and creative knowledge application. \n",
    "\n",
    "**Low Temperature Models:** Excel at following precise formatting requirements and maintaining consistency across multiple calls. This makes them ideal for:\n",
    "- Structured data extraction\n",
    "- API responses that need consistent formatting\n",
    "- Workflows where predictability is paramount\n",
    "- Any situation where you need the model to be a reliable, consistent executor\n",
    "\n",
    "**Higher Temperature Models:** Bring more of the model's training knowledge into play, generating more diverse responses and creative solutions. They're better for:\n",
    "- Creative writing and content generation\n",
    "- Problem-solving that benefits from novel approaches\n",
    "- Situations where you want the model to \"think outside the box\"\n",
    "- Applications where some variation in responses is actually beneficial\n",
    "\n",
    "**The Agent Design Choice:** This balance becomes critical in agentic systems where you need to decide whether your agent should be a precise executor of specific instructions or a creative problem-solver that can adapt its approach based on context. \n",
    "\n",
    "The choice often depends on your use case: customer service bots might need low-temperature consistency to ensure professional, predictable responses, while creative writing assistants might benefit from higher-temperature diversity to generate fresh ideas and varied approaches.\n",
    "\n",
    "**Moving Forward:** Now that we understand how to control our model's behavior through prompts and hyperparameters, we need to give our agents the ability to extend beyond their base knowledge and interact with the world. This is where tools come into play—they're what transform a language model from a sophisticated text generator into an active agent that can perform real actions and access current information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df627673",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "With prompts and hyperparameters mastered, it's time to give your agents the ability to interact with the world beyond their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf486d",
   "metadata": {},
   "source": [
    "Now we're getting to one of the most exciting parts of building agentic systems: tools! Tools are what transform language models from sophisticated text generators into active agents capable of performing real-world actions and accessing live information.\n",
    "\n",
    "**Think of Tools as Your Agent's Hands and Senses:** Without tools, even the most advanced language model is limited to working with only the knowledge it was trained on, which becomes stale the moment training ends. Tools bridge this gap by allowing agents to interact with databases, APIs, web services, file systems, and any other external systems your application needs to work with.\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGyFCaSY8w4Ag/article-cover_image-shrink_720_1280/B4DZYg8dDRHAAI-/0/1744309441965?e=1762992000&v=beta&t=NS3gCnYSTWkxVwnRpHX6tCG7wcXcGgEknNpowIVAo2k\" width=700>\n",
    "\n",
    "**How Tool Calling Actually Works:** The fundamental concept behind tools in agentic systems is function calling (also known as tool calling). Here's what makes this so powerful: modern language models like GPT-4, Claude, and Gemini have been specifically trained to understand when they need external information or capabilities, and can generate structured function calls with appropriate parameters.\n",
    "\n",
    "When an agent encounters a question about current weather, stock prices, or needs to perform calculations, it doesn't hallucinate an answer—instead, it recognizes the limitation and calls the appropriate tool. This is a game-changer for building reliable systems!\n",
    "\n",
    "**The Tool Execution Dance:** Let me walk you through how this works in practice:\n",
    "\n",
    "1. **Request Analysis:** The agent receives a user request and analyzes what information or actions are needed\n",
    "2. **Tool Selection:** It determines which tools to use based on the requirements  \n",
    "3. **Parameter Formatting:** It formats the tool calls with proper parameters\n",
    "4. **Execution:** The tools are executed and return results\n",
    "5. **Synthesis:** The agent receives the results and synthesizes a response using both its knowledge and the tool outputs\n",
    "\n",
    "**The Power of Tool Chaining:** This creates a powerful feedback loop where agents can chain multiple tool calls together, use the output of one tool as input to another, and dynamically adapt their approach based on intermediate results. Imagine an agent that searches the web for recent news, summarizes the findings, then generates a report—all in one coherent workflow!\n",
    "\n",
    "**Three Categories of Tools We'll Explore:**\n",
    "\n",
    "1. **Built-in tools** that come pre-integrated with language model providers\n",
    "2. **Explicit tools** that you define and implement yourself  \n",
    "3. **Model Context Protocol (MCP) tools** that provide standardized interfaces for complex integrations\n",
    "\n",
    "Each category serves different purposes and offers varying levels of customization and complexity. Let's start exploring them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233785",
   "metadata": {},
   "source": [
    "#### Starting Simple: Built-in Tools\n",
    "\n",
    "The easiest way to get started with agent tools is to use the capabilities that come built into your language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b29da",
   "metadata": {},
   "source": [
    "Let's start with the easiest way to give your agents powerful capabilities: built-in tools. These are native capabilities provided directly by language model providers, eliminating the need for external integrations or custom implementations.\n",
    "\n",
    "**Why Built-in Tools Are Awesome:** Google's Gemini models, for example, come with several powerful built-in tools including Google Search integration, code execution capabilities, and mathematical computation tools. These tools are particularly valuable because:\n",
    "\n",
    "- **Optimized Integration:** They're optimized for the specific model with minimal latency overhead\n",
    "- **No Extra Setup:** You don't need additional API keys or setup beyond your primary model access  \n",
    "- **Seamless Experience:** The model provider handles all the complexity of tool execution, result formatting, and error handling\n",
    "- **Reliability:** They're battle-tested and maintained by the model provider\n",
    "\n",
    "**Real-World Example:** When you enable Google Search for Gemini, the model can perform web searches and incorporate real-time information directly into its responses without any additional code on your part. It's like giving your agent instant access to the entire internet!\n",
    "\n",
    "Similarly, the code execution tool allows Gemini to write and run Python code in a sandboxed environment, making it excellent for data analysis, mathematical calculations, and generating visualizations. Imagine asking your agent to \"analyze this sales data and create a chart\" and having it actually execute the code to do so!\n",
    "\n",
    "**The Trade-off to Consider:** The main limitation of built-in tools is that you're constrained to what the provider offers. You can't customize their behavior or add your own specialized functionality. But for many use cases, the convenience and reliability make this a great starting point.\n",
    "\n",
    "Let's see how to use these powerful capabilities with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example showing how to attach tools to the same LLM (no multiple agent classes)\n",
    "def create_builtin_tools_demo():\n",
    "    \"\"\"Demonstrate reusing the same LLM with different tool-enabled chains.\"\"\"\n",
    "    # In production you might register tool-capable agents; here we show direct chains\n",
    "    search_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"When useful, search the web for up-to-date info.\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    code_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"When asked, produce small, safe Python snippets to compute answers.\"),\n",
    "        (\"human\", \"{analysis_request}\")\n",
    "    ])\n",
    "    # store templates for reuse\n",
    "    tutorial_state[\"prompt_templates\"].update({\"search\": search_prompt, \"code\": code_prompt})\n",
    "    return {\"search_prompt\": search_prompt, \"code_prompt\": code_prompt}\n",
    "\n",
    "builtins = create_builtin_tools_demo()\n",
    "print('Built-in tool templates created (reused later).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's demonstrate our built-in tool agents\n",
    "# These extend our global llm with additional capabilities\n",
    "\n",
    "print(\"🔧 Testing Built-in Tool Capabilities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our agents from tutorial_state (they use the same base llm)\n",
    "agents = tutorial_state.get(\"builtin_agents\", {})\n",
    "\n",
    "if agents:\n",
    "    print(\"\\n✅ Using pre-configured agents with built-in tools\")\n",
    "    print(f\"   Available agents: {list(agents.keys())}\")\n",
    "    \n",
    "    # Get our chains that use these agents\n",
    "    chains = tutorial_state.get(\"chains\", {})\n",
    "    \n",
    "    if \"search_chain\" in chains:\n",
    "        print(\"\\n🔍 Example: Search-enabled agent\")\n",
    "        print(\"   This agent can search for current information when needed\")\n",
    "        print(\"   💡 It uses our same base llm but with Google Search capability\")\n",
    "    \n",
    "    if \"code_chain\" in chains:\n",
    "        print(\"\\n💻 Example: Code execution agent\")\n",
    "        print(\"   This agent can write and execute Python code\")\n",
    "        print(\"   💡 It uses our same base llm but with code execution capability\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Built-in agents not yet initialized\")\n",
    "    print(\"   They will be created when needed using our global llm\")\n",
    "\n",
    "print(\"\\n💡 Key insight: All these agents share the same base LLM\")\n",
    "print(\"   We're just adding different tool capabilities on top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f08dc",
   "metadata": {},
   "source": [
    "#### Explicit Tools : Building Agent Memory\n",
    "\n",
    "As we build more sophisticated agents, we quickly run into a fundamental challenge: how do we help our agents remember important information across conversations and interactions? This is where memory systems become crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a351b4",
   "metadata": {},
   "source": [
    "**Why Memory Matters:** Think about how frustrating it would be to work with a colleague who forgot everything you discussed after each meeting. That's essentially what happens with stateless language models—each interaction starts fresh, with no memory of previous conversations or learned preferences.\n",
    "\n",
    "Memory systems solve this by allowing agents to:\n",
    "- **Maintain Context**: Remember what you've discussed previously\n",
    "- **Learn Preferences**: Adapt to your communication style and needs over time  \n",
    "- **Build Relationships**: Create more natural, ongoing conversations\n",
    "- **Accumulate Knowledge**: Learn from interactions to become more effective\n",
    "\n",
    "**The Challenge:** The tricky part is deciding what to remember, how long to keep it, and how to retrieve relevant memories when needed. Different memory strategies work better for different types of applications.\n",
    "\n",
    "Let's explore the various memory systems available and learn when to use each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a couple of focused example tools using the @tool decorator\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Return a tiny mocked weather JSON for teaching purposes.\"\"\"\n",
    "    condition = random.choice([\"sunny\", \"cloudy\", \"rainy\"])\n",
    "    return json.dumps({\"city\": city, \"condition\": condition, \"temp_c\": random.randint(0,30)})\n",
    "\n",
    "@tool\n",
    "def compound_interest(principal: float, rate: float, years: int) -> str:\n",
    "    \"\"\"Minimal compound interest calculation; returns JSON string.\"\"\"\n",
    "    amount = principal * (1 + rate) ** years\n",
    "    return json.dumps({\"principal\": principal, \"rate\": rate, \"years\": years, \"final\": round(amount,2)})\n",
    "\n",
    "# Expose tools list for agents/examples\n",
    "example_tools = [get_weather, compound_interest]\n",
    "print('Example tools defined: get_weather, compound_interest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127adc",
   "metadata": {},
   "source": [
    "great now we'll create the armed agent and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf42e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an agent that uses our custom tools\n",
    "# This agent will use our existing global llm instance\n",
    "\n",
    "def create_custom_tool_agent():\n",
    "    \"\"\"\n",
    "    Create an agent with custom tools using our existing llm instance\n",
    "    This shows how to extend our base agent with specific capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the tools we created earlier\n",
    "    custom_tools = tutorial_state.get(\"tools\", {}).get(\"custom_tools\", create_custom_tools())\n",
    "    \n",
    "    # Create a prompt that works with our existing llm\n",
    "    tool_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant with access to several specialized tools:\n",
    "        \n",
    "        🌤️  get_weather: Get current weather for any city\n",
    "        💰 calculate_compound_interest: Calculate investment returns with compound interest\n",
    "        👥 search_user_database: Look up customer information in database\n",
    "        \n",
    "        Use these tools when needed to provide accurate, helpful responses.\n",
    "        Always explain which tool you're using and why.\n",
    "        Format JSON data nicely for users.\"\"\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    \n",
    "    # Create agent using our global llm\n",
    "    agent = create_tool_calling_agent(llm, custom_tools, tool_prompt)\n",
    "    \n",
    "    # Create executor\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=custom_tools, \n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    # Store in tutorial state for reuse\n",
    "    tutorial_state[\"agents\"] = tutorial_state.get(\"agents\", {})\n",
    "    tutorial_state[\"agents\"][\"custom_tool_agent\"] = agent_executor\n",
    "    \n",
    "    print(\"\udd16 Custom tool agent created using our global llm\")\n",
    "    print(f\"🔧 Tools available: {len(custom_tools)}\")\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "# Create our reusable agent\n",
    "tool_agent = create_custom_tool_agent()\n",
    "\n",
    "print(\"✅ Agent ready and stored in tutorial_state['agents']\")\n",
    "print(\"💡 We can reuse this agent for multiple queries without recreating it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3328",
   "metadata": {},
   "source": [
    "#### Model Context Protocol (MCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56fff",
   "metadata": {},
   "source": [
    "Model Context Protocol (MCP) represents the next evolution in AI tool integration, providing a standardized way for AI applications to securely connect to data sources and tools. Think of MCP as a universal translator that allows any AI system to communicate with any external service through a common protocol, eliminating the need for custom integrations for each tool or data source.\n",
    "\n",
    "<img src=\"https://mintcdn.com/mcp/bEUxYpZqie0DsluH/images/mcp-simple-diagram.png?w=1100&fit=max&auto=format&n=bEUxYpZqie0DsluH&q=85&s=341b88d6308188ab06bf05748c80a494\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/tweet_video_thumb/Gl7C44tXYAAdDSJ.jpg\" width=700>\n",
    "\n",
    "<img src=\"https://miro.medium.com/0*qtnzILuhG39c2DML.jpeg\" width=700>\n",
    "\n",
    "\n",
    "\n",
    "MCP was developed by Anthropic to solve the fragmentation problem in AI tool ecosystems. Before MCP, every AI application had to implement its own custom integrations for databases, APIs, file systems, and other external resources. This led to duplicated effort, security inconsistencies, and tools that only worked with specific AI platforms. MCP standardizes these interactions through a client-server architecture where MCP servers expose resources (like databases or file systems) and tools (like calculators or API clients) through a uniform interface.\n",
    "\n",
    "The protocol operates on JSON-RPC 2.0, enabling real-time, bidirectional communication between AI applications (MCP clients) and external resources (MCP servers). This means your agent can not only call tools but also receive real-time updates, notifications, and streaming data from external systems. The security model is built around explicit capability declarations and sandboxed execution, ensuring that agents can only access resources they've been explicitly granted permission to use.\n",
    "\n",
    "What makes MCP particularly powerful for RAG and agentic systems is its ability to provide **contextual data access**. Instead of just calling functions, MCP servers can expose rich contextual information about resources - like database schemas, file structures, or API capabilities - allowing agents to make more informed decisions about how to interact with external systems.\n",
    "\n",
    "Let's explore how to integrate MCP servers with LangChain and Gemini. For this example, we'll use the MCP SDK to create a simple server and then connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio loops for Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a tiny MCP-like resource collection (synchronous, minimal)\n",
    "simulated_mcp = {\n",
    "    \"customer_db\": {\"customers\": [{\"id\": \"001\", \"name\": \"Alice\", \"tier\": \"premium\"}]},\n",
    "    \"inventory\": {\"items\": [{\"sku\": \"A001\", \"name\": \"Widget\", \"quantity\": 10}]},\n",
    "    \"analytics\": {\"sales\": {\"month\": 12000, \"trend\": \"up\"}},\n",
    "}\n",
    "\n",
    "def mcp_read_resource_sync(resource_name: str) -> str:\n",
    "    \"\"\"Simple synchronous read from the simulated MCP resources.\"\"\"\n",
    "    key = resource_name.lower()\n",
    "    if key in simulated_mcp:\n",
    "        return json.dumps(simulated_mcp[key])\n",
    "    return json.dumps({\"error\": f\"resource '{resource_name}' not found\"})\n",
    "\n",
    "def mcp_call_tool_sync(name: str, arguments: dict) -> str:\n",
    "    \"\"\"Very small dispatcher for simulated tools (e.g., update inventory).\"\"\"\n",
    "    if name == \"query_analytics\":\n",
    "        metric = arguments.get(\"metric\", \"sales\")\n",
    "        return json.dumps(simulated_mcp.get(\"analytics\", {}).get(metric, {}))\n",
    "    if name == \"update_inventory\":\n",
    "        sku = arguments.get(\"sku\")\n",
    "        qty = arguments.get(\"quantity\", 0)\n",
    "        items = simulated_mcp[\"inventory\"][\"items\"]\n",
    "        item = next((i for i in items if i[\"sku\"] == sku), None)\n",
    "        if item:\n",
    "            item[\"quantity\"] = qty\n",
    "            return json.dumps({\"sku\": sku, \"new_quantity\": item[\"quantity\"]})\n",
    "        return json.dumps({\"error\": \"sku not found\"})\n",
    "    return json.dumps({\"error\": f\"unknown tool {name}\"})\n",
    "\n",
    "print('Simulated MCP available for local examples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small @tool wrappers that call the synchronous simulated MCP helpers\n",
    "@tool\n",
    "def mcp_read_resource(resource_name: str) -> str:\n",
    "    return mcp_read_resource_sync(resource_name)\n",
    "\n",
    "@tool\n",
    "def mcp_query_analytics(metric: str = \"sales\", period: str = \"month\") -> str:\n",
    "    return mcp_call_tool_sync(\"query_analytics\", {\"metric\": metric, \"period\": period})\n",
    "\n",
    "@tool\n",
    "def mcp_update_inventory(sku: str, quantity: int) -> str:\n",
    "    return mcp_call_tool_sync(\"update_inventory\", {\"sku\": sku, \"quantity\": quantity})\n",
    "\n",
    "mcp_tools = [mcp_read_resource, mcp_query_analytics, mcp_update_inventory]\n",
    "\n",
    "# Build a minimal agent demonstrating tool usage\n",
    "mcp_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a business assistant with access to simple MCP tools.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "mcp_agent = create_tool_calling_agent(llm, mcp_tools, mcp_prompt)\n",
    "print('Minimal MCP-enabled agent created (uses simulated resources).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the REAL MCP-enabled agent with comprehensive business scenarios\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🧪 TESTING REAL MCP SERVER INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n=== Test 1: Customer Data Analysis via MCP ===\")\n",
    "print(\"🔍 Using MCP resource: customer_db\")\n",
    "customer_analysis = mcp_executor.invoke({\n",
    "    \"input\": \"Analyze our customer data. Show me the customer information, tier distribution, and total customer value.\"\n",
    "})\n",
    "print(\"📋 Response:\", customer_analysis['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 2: Real-time Business Analytics via MCP Tools ===\") \n",
    "print(\"📊 Using MCP tool: query_analytics\")\n",
    "analytics_query = mcp_executor.invoke({\n",
    "    \"input\": \"Get our current sales and revenue metrics for this month. Also check user growth trends.\"\n",
    "})\n",
    "print(\"📈 Response:\", analytics_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 3: Inventory Management via MCP ===\")\n",
    "print(\"📦 Using MCP resource and tools: inventory + update_inventory\")\n",
    "inventory_management = mcp_executor.invoke({\n",
    "    \"input\": \"Check our current inventory levels, then update the laptop inventory by adding 25 units. Also check if we're low on any items.\"\n",
    "})\n",
    "print(\"🏪 Response:\", inventory_management['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 4: Business Operations - Notification System ===\")\n",
    "print(\"📢 Using MCP tool: send_notification\")\n",
    "notification_test = mcp_executor.invoke({\n",
    "    \"input\": \"Send a high-priority notification to the warehouse manager about low stock levels for any items under 100 units.\"\n",
    "})\n",
    "print(\"🔔 Response:\", notification_test['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 5: Comprehensive Business Dashboard ===\")\n",
    "print(\"🎯 Using multiple MCP resources and tools\")\n",
    "dashboard_query = mcp_executor.invoke({\n",
    "    \"input\": \"\"\"Create a comprehensive business dashboard showing:\n",
    "    1. Customer tier distribution and total value\n",
    "    2. Current sales performance and trends  \n",
    "    3. Inventory status with any low-stock alerts\n",
    "    4. Send a summary notification to the CEO\n",
    "    \n",
    "    Use all available MCP resources and tools to gather this information.\"\"\"\n",
    "})\n",
    "print(\"📊 Dashboard Response:\", dashboard_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ REAL MCP INTEGRATION TESTS COMPLETED\")\n",
    "print(\"🎉 Model Context Protocol successfully integrated!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8b567",
   "metadata": {},
   "source": [
    "\n",
    "This real MCP implementation demonstrates how modern AI systems can safely and efficiently integrate with enterprise systems using standardized protocols rather than ad-hoc custom integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cleanup MCP Server Resources\n",
    "# Run this when you're done with the MCP server to clean up resources\n",
    "\n",
    "async def cleanup_mcp_server():\n",
    "    \"\"\"Cleanup MCP server resources\"\"\"\n",
    "    try:\n",
    "        await business_mcp.cleanup()\n",
    "        print(\"✅ MCP server resources cleaned up successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning: {e}\")\n",
    "\n",
    "# Uncomment the line below if you want to cleanup the MCP server\n",
    "# await cleanup_mcp_server()\n",
    "\n",
    "print(\"💡 MCP server is ready for use!\")\n",
    "print(\"🧹 Run cleanup_mcp_server() when finished to release resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56c6e2",
   "metadata": {},
   "source": [
    "The examples above demonstrate the power of tools in transforming language models into capable agents. We've seen how **built-in tools** provide immediate capabilities with minimal setup, **explicit tools** offer complete customization for your specific needs, and **MCP tools** enable standardized integration with complex systems while maintaining security and scalability.\n",
    "\n",
    "The key insight is that tools are what bridge the gap between language model intelligence and real-world utility. Without tools, even the most sophisticated language model is limited to generating text based on its training data. With tools, agents become active participants in your business processes, capable of querying databases, performing calculations, calling APIs, and taking actions in response to user needs.\n",
    "\n",
    "As we design agentic systems, the choice between different tool types depends on your specific requirements:\n",
    "- Use **built-in tools** when the model provider offers functionality that meets your needs\n",
    "- Create **explicit tools** when you need custom integration with your specific systems  \n",
    "- Implement **MCP tools** when you need standardized, scalable integrations across multiple AI applications\n",
    "\n",
    "Now that our agents can take actions in the world through tools, we need to ensure they can maintain context and remember information across interactions. This is where memory and context management become crucial for building agents that can handle complex, multi-step workflows and maintain coherent conversations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0edd85",
   "metadata": {},
   "source": [
    "### Context Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7cd88",
   "metadata": {},
   "source": [
    "Context management is the cognitive backbone of sophisticated agents, determining how they maintain awareness of ongoing conversations, remember past interactions, and build upon previous knowledge to provide coherent, contextually relevant responses. Without proper context management, even the most capable agents become like individuals with severe short-term memory loss—they might excel at individual tasks but fail to maintain meaningful, coherent interactions over time.\n",
    "\n",
    "Think of context management as the difference between having a conversation with a knowledgeable expert who remembers your entire discussion versus repeatedly starting fresh with someone who has no recollection of what you've already covered. The former builds understanding progressively, references earlier points, and adapts their communication based on your evolving needs. The latter, while potentially knowledgeable, forces you to repeat yourself and cannot build on the conversational foundation you've established.\n",
    "\n",
    "In agentic systems, context management becomes even more critical because agents need to coordinate information across multiple tool calls, maintain state during complex workflows, and remember important details that influence future decisions. An agent helping with financial planning needs to remember your risk tolerance, investment timeline, and previous decisions to provide consistent advice. A customer service agent should recall your account history, previous issues, and preferences to deliver personalized support.\n",
    "\n",
    "The challenge lies in balancing several competing factors: **memory capacity** (how much information can be retained), **relevance** (what information is most important to keep), **efficiency** (managing token limits and processing costs), and **persistence** (maintaining memory across sessions). Different memory strategies excel in different scenarios, and the best approach often involves combining multiple memory types to create a comprehensive context management system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b3a57",
   "metadata": {},
   "source": [
    "<img src=\"https://substackcdn.com/image/fetch/$s_!AyLS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0e3c002-0841-4d5f-9171-3eb63c321824_1600x1224.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592d71",
   "metadata": {},
   "source": [
    "Memory systems in agentic applications serve different purposes and have distinct strengths and limitations. Understanding these differences is crucial for selecting the right memory strategy for your specific use case. Let's explore the major categories of memory available in LangChain and how they can be effectively utilized.\n",
    "\n",
    "**Buffer-based memories** store raw conversation history up to certain limits, providing complete fidelity but consuming significant token space. **Summary-based memories** compress conversation history into concise summaries, trading some detail for efficiency. **Window-based memories** maintain only recent interactions, ensuring relevance while discarding older context. **Token-aware memories** dynamically manage content based on token consumption, balancing completeness with cost constraints.\n",
    "\n",
    "Each memory type excels in specific scenarios: use buffer memory for short conversations where every detail matters, summary memory for long-running sessions where themes and key decisions need tracking, window memory for task-oriented interactions where only recent context is relevant, and token buffer memory for cost-sensitive applications with unpredictable conversation lengths.\n",
    "\n",
    "- **Buffer Memory**: Stores everything - perfect recall but grows indefinitely\n",
    "- **Summary Memory**: Compresses older content - manageable size with key information preserved  \n",
    "- **Window Memory**: Only recent context - predictable size but limited history\n",
    "- **Token Memory**: Smart pruning based on token limits - cost-controlled with intelligent truncation\n",
    "- **Entity Memory**: Relationship tracking - maintains entity awareness across conversations\n",
    "\n",
    "\n",
    "Let's implement and compare these different memory systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create different memory systems that work with our global llm\n",
    "# These will help our agent remember conversations in different ways\n",
    "\n",
    "def setup_memory_systems():\n",
    "    \"\"\"\n",
    "    Initialize various memory systems for our agent\n",
    "    All will use the same global llm but with different memory strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dedicated LLM for memory operations (slightly lower temperature for consistency)\n",
    "    memory_llm = ChatGoogleGenerativeAI(\n",
    "        model=llm.model,  # Same model as global llm\n",
    "        temperature=0.2,  # Lower temperature for more consistent memory operations\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Store the memory llm in tutorial_state\n",
    "    tutorial_state[\"memory_llm\"] = memory_llm\n",
    "    \n",
    "    # Initialize our memory systems\n",
    "    memory_systems = {\n",
    "        \"Buffer (Complete)\": ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Summary (Compressed)\": ConversationSummaryMemory(\n",
    "            llm=memory_llm,\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Window (Last 3)\": ConversationBufferWindowMemory(\n",
    "            k=3,  # Keep last 3 conversation pairs\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Token Limited\": ConversationTokenBufferMemory(\n",
    "            llm=memory_llm,\n",
    "            max_token_limit=500,\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        ),\n",
    "        \"Entity Tracking\": ConversationEntityMemory(\n",
    "            llm=memory_llm,\n",
    "            entity_store=InMemoryEntityStore(),\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Store in tutorial_state for reuse\n",
    "    tutorial_state[\"memory_systems\"] = memory_systems\n",
    "    \n",
    "    # Create conversation chains for each memory type\n",
    "    memory_chains = {}\n",
    "    for name, memory_system in memory_systems.items():\n",
    "        memory_chains[name] = ConversationChain(\n",
    "            llm=memory_llm,  # Use our consistent memory llm\n",
    "            memory=memory_system,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    tutorial_state[\"memory_chains\"] = memory_chains\n",
    "    \n",
    "    print(\"🧠 Memory Systems Initialized\")\n",
    "    print(f\"   📊 {len(memory_systems)} different memory strategies\")\n",
    "    print(f\"   🔗 All using consistent memory LLM (τ=0.2)\")\n",
    "    print(f\"   💾 Available types: {list(memory_systems.keys())}\")\n",
    "    \n",
    "    return memory_systems, memory_chains\n",
    "\n",
    "# Initialize our memory systems\n",
    "memory_systems, memory_chains = setup_memory_systems()\n",
    "\n",
    "print(\"\\n✅ Memory systems ready for use throughout the tutorial\")\n",
    "print(\"💡 These will help our agent remember conversations in different ways\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095169",
   "metadata": {},
   "source": [
    "##### Comparing Memory Systems Side-by-Side:\n",
    "\n",
    "Now that we understand each memory type individually, let's create a direct comparison to see how they behave differently with the same input. This will help you understand when to choose each approach:\n",
    "\n",
    "\n",
    "Let's test them all with the same business conversation scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our memory systems with a business scenario\n",
    "# We'll use the chains we already created in tutorial_state\n",
    "\n",
    "print(\"🧪 Testing Memory Systems with Business Scenario\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our pre-configured memory chains\n",
    "memory_chains = tutorial_state.get(\"memory_chains\", {})\n",
    "\n",
    "if not memory_chains:\n",
    "    print(\"⚠️ Memory chains not initialized, setting them up now...\")\n",
    "    memory_systems, memory_chains = setup_memory_systems()\n",
    "\n",
    "# Define a realistic conversation scenario\n",
    "test_scenario = [\n",
    "    \"Hi, I'm working on the TechCorp project with a $2M budget.\",\n",
    "    \"The project manager is Sarah Chen, and we're targeting Q4 launch.\", \n",
    "    \"We need to coordinate with the development team led by Mike Rodriguez.\",\n",
    "    \"The main deliverable is a cloud migration to Azure platform.\",\n",
    "    \"Sarah mentioned the timeline is aggressive - only 3 months to complete.\",\n",
    "    \"What are the key risks we should be monitoring for this project?\"\n",
    "]\n",
    "\n",
    "print(f\"📝 Testing with {len(test_scenario)} conversation turns\")\n",
    "print(\"\\n\udca1 Each memory system will process the same conversation\")\n",
    "print(\"   Watch how they handle context differently\")\n",
    "\n",
    "# Test each memory system\n",
    "scenario_results = {}\n",
    "\n",
    "for memory_name, chain in memory_chains.items():\n",
    "    print(f\"\\n--- Testing {memory_name} ---\")\n",
    "    \n",
    "    # Process all conversation turns with this memory system\n",
    "    for i, user_input in enumerate(test_scenario, 1):\n",
    "        response = chain.predict(input=user_input)\n",
    "        print(f\"Turn {i}: ✅\")\n",
    "    \n",
    "    # Store the final response for comparison\n",
    "    final_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    scenario_results[memory_name] = {\n",
    "        \"final_response\": final_response,\n",
    "        \"memory_type\": memory_name\n",
    "    }\n",
    "    \n",
    "    # Clear memory for next test\n",
    "    chain.memory.clear()\n",
    "    print(f\"✓ Completed and cleared\")\n",
    "\n",
    "# Store results\n",
    "tutorial_state[\"memory_test_results\"] = scenario_results\n",
    "\n",
    "print(f\"\\n🏁 Completed testing all {len(memory_chains)} memory systems!\")\n",
    "print(\"📊 Results stored in tutorial_state for analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee12e43",
   "metadata": {},
   "source": [
    "Real-world applications often benefit from combining multiple memory strategies to create sophisticated context management systems that leverage the strengths of different approaches while mitigating their individual limitations. CombinedMemory allows you to orchestrate multiple memory systems simultaneously, creating layered context awareness that can handle both immediate needs and long-term relationship building.\n",
    "\n",
    "For example, you might combine ConversationBufferWindowMemory for immediate context with ConversationEntityMemory for long-term entity tracking, plus a custom memory component for domain-specific information. This creates a multi-layered memory architecture where recent interactions provide immediate context, entity memory maintains relationship continuity, and specialized memory components handle domain-specific requirements like user preferences or system configurations.\n",
    "\n",
    "Let's implement a combined memory system that demonstrates this architectural approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build a sophisticated combined memory system\n",
    "# This will use our existing memory_llm from tutorial_state\n",
    "\n",
    "print(\"🏗️ Building Combined Memory Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm (created earlier with consistent settings)\n",
    "memory_llm = tutorial_state.get(\"memory_llm\")\n",
    "\n",
    "if not memory_llm:\n",
    "    print(\"⚠️ Memory LLM not found, creating it...\")\n",
    "    memory_llm = ChatGoogleGenerativeAI(\n",
    "        model=llm.model,\n",
    "        temperature=0.2,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    tutorial_state[\"memory_llm\"] = memory_llm\n",
    "\n",
    "# Create individual memory components\n",
    "print(\"\\n1️⃣ Setting up memory components...\")\n",
    "\n",
    "# Recent Memory - immediate context\n",
    "recent_memory = ConversationBufferWindowMemory(\n",
    "    k=2,\n",
    "    memory_key=\"recent_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "print(\"   ✅ Recent Memory (last 2 turns)\")\n",
    "\n",
    "# Entity Tracker - long-term relationships\n",
    "entity_tracker = ConversationEntityMemory(\n",
    "    llm=memory_llm,  # Reusing our memory_llm\n",
    "    entity_store=InMemoryEntityStore(),\n",
    "    memory_key=\"entities\",\n",
    "    return_messages=False\n",
    ")\n",
    "print(\"   ✅ Entity Tracker (people, projects, companies)\")\n",
    "\n",
    "# Preferences - user settings\n",
    "preferences_memory = SimpleMemory(\n",
    "    memories={\"user_preferences\": \"No specific preferences set yet\"}\n",
    ")\n",
    "print(\"   ✅ Preferences Memory (user settings)\")\n",
    "\n",
    "# Combine them all\n",
    "print(\"\\n2️⃣ Combining into unified system...\")\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Create custom prompt for combined memory\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the conversation chain using our memory_llm\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,  # Reusing our existing memory_llm\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Store in tutorial_state for reuse\n",
    "tutorial_state[\"combined_memory\"] = combined_memory\n",
    "tutorial_state[\"combined_chain\"] = combined_chain\n",
    "\n",
    "print(\"\\n✅ Combined Memory System Created!\")\n",
    "print(\"   🔄 Orchestrates: Recent context + Entity tracking + Preferences\")\n",
    "print(\"   🧠 Uses our existing memory_llm (consistent with other memory ops)\")\n",
    "print(\"   💾 Stored in tutorial_state for reuse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980c322",
   "metadata": {},
   "source": [
    "**Understanding the Architecture:** \n",
    "\n",
    "What we just created is a three-layer memory system:\n",
    "\n",
    "1. **Recent Memory** provides immediate conversational context - what was just said in the last few exchanges\n",
    "2. **Entity Tracker** maintains long-term awareness of important entities (people, companies, projects) mentioned throughout the conversation\n",
    "3. **Preferences Memory** stores user-specific settings and preferences that should persist across conversations\n",
    "\n",
    "This architecture mirrors how human memory works - we have immediate working memory for current context, long-term memory for important relationships and facts, and persistent preferences that guide our behavior.\n",
    "\n",
    "Next, let's combine these systems into a unified memory architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Memory Systems\n",
    "# Now let's orchestrate all three memory types into a unified system\n",
    "\n",
    "# Create the combined memory that coordinates all components\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Create a prompt template that utilizes all memory types\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the conversation chain with our combined memory\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"🧠 Combined Memory System Created!\")\n",
    "print(\"   🔄 Orchestrates: Recent context + Entity tracking + User preferences\")\n",
    "print(\"   📋 Custom prompt template utilizes all memory types\")\n",
    "print(\"   ⚙️  Ready for sophisticated context-aware conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c5c33",
   "metadata": {},
   "source": [
    "**How Combined Memory Works:**\n",
    "\n",
    "The `CombinedMemory` system is like having a team of specialists working together:\n",
    "\n",
    "- **Recent Memory** acts as the \"immediate context specialist\" - always aware of what just happened\n",
    "- **Entity Tracker** serves as the \"relationship specialist\" - remembering who's who and what's what across conversations  \n",
    "- **Preferences Memory** functions as the \"personalization specialist\" - maintaining user-specific settings and preferences\n",
    "\n",
    "When you ask a question, all three systems contribute their expertise:\n",
    "1. Recent memory provides immediate conversational context\n",
    "2. Entity tracker identifies relevant relationships and entities \n",
    "3. Preferences memory ensures responses align with user preferences\n",
    "\n",
    "The custom prompt template weaves all this information together, creating responses that are both contextually aware and personally relevant.\n",
    "\n",
    "Let's test this system with a realistic conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80218763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our combined memory system\n",
    "# We'll use the chain we just created and stored in tutorial_state\n",
    "\n",
    "print(\"🧪 Testing Combined Memory System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our combined chain\n",
    "combined_chain = tutorial_state.get(\"combined_chain\")\n",
    "\n",
    "if not combined_chain:\n",
    "    print(\"⚠️ Combined chain not found, please run the previous cell first\")\n",
    "else:\n",
    "    # Define test conversation\n",
    "    test_conversation = [\n",
    "        \"Hi, I'm Sarah and I prefer concise responses. I'm working on a Python project.\",\n",
    "        \"I need help with data analysis using pandas. Can you recommend some techniques?\", \n",
    "        \"Actually, I'm working with customer data for my company TechFlow Solutions.\",\n",
    "        \"Our CEO Mike Johnson wants insights on customer retention patterns.\",\n",
    "        \"Can you suggest a visualization approach for this data?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"📝 Running {len(test_conversation)} conversation turns\")\n",
    "    print(\"💡 Watch how the combined memory system:\")\n",
    "    print(\"   • Remembers Sarah prefers concise responses\")\n",
    "    print(\"   • Tracks entities (Sarah, TechFlow, Mike Johnson)\")\n",
    "    print(\"   • Maintains recent context\")\n",
    "    print()\n",
    "\n",
    "    # Process each conversation turn\n",
    "    for i, user_input in enumerate(test_conversation, 1):\n",
    "        print(f\"\\n--- Turn {i} ---\")\n",
    "        print(f\"User: {user_input}\")\n",
    "        \n",
    "        # Use our combined memory chain\n",
    "        response = combined_chain.predict(input=user_input)\n",
    "        \n",
    "        # Show brief preview\n",
    "        preview = response[:100] + \"...\" if len(response) > 100 else response\n",
    "        print(f\"Preview: {preview}\")\n",
    "        print(f\"✅ Turn {i} processed\")\n",
    "\n",
    "    print(f\"\\n🎯 Completed {len(test_conversation)} turns with combined memory!\")\n",
    "    print(\"💾 All context preserved across the conversation\")\n",
    "    \n",
    "    # Store conversation in tutorial_state\n",
    "    tutorial_state[\"combined_memory_conversation\"] = test_conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a057b",
   "metadata": {},
   "source": [
    "**Analyzing What Just Happened:**\n",
    "\n",
    "In this conversation, watch how the combined memory system demonstrated all three memory types working together:\n",
    "\n",
    "1. **Turn 1**: Sarah introduces herself and sets preferences (concise responses) - captured by preferences memory\n",
    "2. **Turn 2**: Discusses pandas and data analysis - entity memory starts tracking \"pandas\" and \"data analysis\"  \n",
    "3. **Turn 3**: Introduces \"TechFlow Solutions\" - entity memory now tracks this company\n",
    "4. **Turn 4**: Mentions \"Mike Johnson\" as CEO - entity memory connects him to TechFlow Solutions\n",
    "5. **Turn 5**: Asks about visualization - recent memory provides immediate context while entity memory maintains awareness of all the players and context\n",
    "\n",
    "This creates a conversation experience where the agent:\n",
    "- Remembers Sarah prefers concise responses (preferences)\n",
    "- Knows she works at TechFlow Solutions with CEO Mike Johnson (entities)  \n",
    "- Understands the current conversation is about customer retention visualization (recent context)\n",
    "\n",
    "Let's examine what our memory systems captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what our memory systems captured\n",
    "# And see the full picture of our reusable agent components\n",
    "\n",
    "print(\"\\n🧠 MEMORY SYSTEM ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check recent memory\n",
    "recent_memory = tutorial_state.get(\"combined_memory\")\n",
    "if recent_memory:\n",
    "    print(\"✅ Combined Memory System Active\")\n",
    "    print(\"   Components working together:\")\n",
    "    print(\"   • Recent Memory (last 2 turns)\")\n",
    "    print(\"   • Entity Tracker (relationships)\")\n",
    "    print(\"   • Preferences (user settings)\")\n",
    "\n",
    "print(\"\\n📊 REUSABLE COMPONENTS INVENTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show all our reusable components\n",
    "components_summary = {\n",
    "    \"LLM Instances\": 0,\n",
    "    \"Prompt Templates\": 0,\n",
    "    \"Chains\": 0,\n",
    "    \"Memory Systems\": 0,\n",
    "    \"Agents\": 0,\n",
    "    \"Tools\": 0\n",
    "}\n",
    "\n",
    "if \"memory_llm\" in tutorial_state:\n",
    "    components_summary[\"LLM Instances\"] += 1\n",
    "    \n",
    "if \"prompt_templates\" in tutorial_state:\n",
    "    components_summary[\"Prompt Templates\"] = len(tutorial_state[\"prompt_templates\"])\n",
    "    \n",
    "if \"chains\" in tutorial_state:\n",
    "    components_summary[\"Chains\"] = len(tutorial_state[\"chains\"])\n",
    "    \n",
    "if \"memory_systems\" in tutorial_state:\n",
    "    components_summary[\"Memory Systems\"] = len(tutorial_state[\"memory_systems\"])\n",
    "    \n",
    "if \"agents\" in tutorial_state:\n",
    "    components_summary[\"Agents\"] = len(tutorial_state[\"agents\"])\n",
    "    \n",
    "if \"tools\" in tutorial_state:\n",
    "    if \"custom_tools\" in tutorial_state[\"tools\"]:\n",
    "        components_summary[\"Tools\"] = len(tutorial_state[\"tools\"][\"custom_tools\"])\n",
    "\n",
    "print(\"\\n📈 Component Summary:\")\n",
    "for component_type, count in components_summary.items():\n",
    "    print(f\"   {component_type}: {count}\")\n",
    "\n",
    "print(\"\\n✅ Memory tutorial section completed!\")\n",
    "print(f\"💾 All components stored in tutorial_state\")\n",
    "print(f\"\udd04 Ready to be reused in subsequent sections\")\n",
    "\n",
    "print(\"\\n💡 TUTORIAL PHILOSOPHY:\")\n",
    "print(\"   Instead of creating new instances everywhere,\")\n",
    "print(\"   we build components once and reuse them throughout.\")\n",
    "print(\"   This mirrors real-world development practices!\")\n",
    "\n",
    "# Update tutorial state\n",
    "tutorial_state['memory_systems_tested'] = [\n",
    "    'ConversationBufferMemory', \n",
    "    'ConversationSummaryMemory',\n",
    "    'ConversationBufferWindowMemory', \n",
    "    'ConversationTokenBufferMemory',\n",
    "    'ConversationEntityMemory',\n",
    "    'CombinedMemory'\n",
    "]\n",
    "tutorial_state['current_section'] = 'memory_complete'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6f83a",
   "metadata": {},
   "source": [
    "The examples above demonstrate the spectrum of memory management strategies available for agentic systems. Each approach serves different purposes and excels in specific scenarios:\n",
    "\n",
    "**ConversationBufferMemory** provides perfect recall for short conversations where every detail matters, but becomes expensive in extended interactions. **ConversationSummaryMemory** enables indefinitely long conversations by maintaining key themes while sacrificing some detail. **ConversationBufferWindowMemory** offers predictable performance by keeping only recent context, ideal for task-oriented interactions. **ConversationTokenBufferMemory** provides optimal context utilization with cost control, perfect for production applications.\n",
    "\n",
    "**ConversationEntityMemory** excels at tracking relationships and building long-term understanding, while **CombinedMemory** allows sophisticated orchestration of multiple memory strategies. The choice depends on your specific requirements: conversation length, cost constraints, detail requirements, and the importance of long-term relationship building.\n",
    "\n",
    "In practice, most production agentic systems benefit from combining multiple memory approaches, using recent memory for immediate context, entity memory for relationship continuity, and token-aware management for cost control. This creates robust context management that adapts to different conversation patterns while maintaining performance and reliability.\n",
    "\n",
    "\n",
    "\n",
    "Now that our agents have sophisticated memory capabilities, let's explore how they can develop and refine specialized skills that make them even more effective at specific tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729477c1",
   "metadata": {},
   "source": [
    "#### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e400",
   "metadata": {},
   "source": [
    "### Skills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22156a",
   "metadata": {},
   "source": [
    "As we build more sophisticated agents, we quickly discover that while general-purpose language models are incredibly versatile, they often lack the specialized expertise needed for complex, domain-specific tasks. This is where the concept of \"skills\" becomes crucial—they're like giving your agent professional training in specific areas.\n",
    "\n",
    "**What Are Agent Skills?** Think of skills as specialized capabilities that combine prompts, tools, memory patterns, and domain knowledge to excel at specific types of problems. Just like a human expert develops specialized skills over years of practice, we can build focused capabilities that allow our agents to perform at expert levels in particular domains.\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fddd7e6e572ad0b6a943cacefe957248455f6d522-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F191bf5dd4b6f8cfe6f1ebafe6243dd1641ed231c-1650x1069.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F441b9f6cc0d2337913c1f41b05357f16f51f702e-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "**Real-World Examples:**\n",
    "- A **financial analysis skill** might combine market data tools, statistical calculation capabilities, and specialized prompts for interpreting economic indicators\n",
    "- A **creative writing skill** could integrate research tools, style guidelines, and iterative refinement processes  \n",
    "- A **technical debugging skill** might include code analysis tools, documentation search, and systematic troubleshooting approaches\n",
    "\n",
    "**Why Skills Matter for Your Agents:**\n",
    "\n",
    "- **Specialization**: Agents can develop deep expertise in specific areas rather than being mediocre generalists\n",
    "- **Consistency**: Similar problems are approached with proven, refined techniques that improve over time\n",
    "- **Reusability**: Successful skill patterns can be applied across different contexts and even shared between agents\n",
    "- **Composability**: Complex workflows where multiple skills collaborate to solve multifaceted problems\n",
    "\n",
    "**The Challenges to Consider:** Skills also introduce challenges you need to be aware of:\n",
    "- **Over-specialization** where agents become inflexible outside their trained domains\n",
    "- **Complexity** that makes systems harder to debug and maintain\n",
    "- **Coordination overhead** when multiple skills need to work together effectively\n",
    "\n",
    "The key is finding the right balance between specialization and flexibility for your specific use case. Let's build a practical skills system to see these concepts in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal skills system — simple registry of callable skills (didactic)\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class SkillResult:\n",
    "    success: bool\n",
    "    output: str\n",
    "    confidence: float\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "# Simple registry helpers\n",
    "def register_skill(name: str, func: Callable[[str], SkillResult], description: str = \"\"):\n",
    "    if \"skills\" not in tutorial_state:\n",
    "        tutorial_state[\"skills\"] = {}\n",
    "    tutorial_state[\"skills\"][name] = {\"func\": func, \"description\": description}\n",
    "\n",
    "def run_skill(name: str, input_text: str) -> SkillResult:\n",
    "    entry = tutorial_state.get(\"skills\", {}).get(name)\n",
    "    if not entry:\n",
    "        return SkillResult(False, f\"Skill '{name}' not found\", 0.0)\n",
    "    try:\n",
    "        return entry[\"func\"](input_text)\n",
    "    except Exception as e:\n",
    "        return SkillResult(False, str(e), 0.0)\n",
    "\n",
    "# Example skill implementation (very small and deterministic for teaching)\n",
    "def financial_analysis_skill(input_text: str) -> SkillResult:\n",
    "    # Tiny illustrative logic: summarize and return a fixed confidence\n",
    "    summary = f\"[financial_analysis] summary for input (len={len(input_text)}): {input_text[:60]}...\"\n",
    "    return SkillResult(True, summary, 0.8)\n",
    "\n",
    "# Register the example skill\n",
    "register_skill(\"financial_analysis\", financial_analysis_skill, \"Tiny example financial skill\")\n",
    "print(\"Minimal skill registry ready — 'financial_analysis' registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beac902",
   "metadata": {},
   "source": [
    "### Workflows and Chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372add07",
   "metadata": {},
   "source": [
    "Now that we've mastered the building blocks of agentic systems—prompts, tools, memory, and skills—it's time to explore how we orchestrate these components into sophisticated workflows.\n",
    "\n",
    "**Think of Workflows as Choreography:** I like to think of workflows as the \"choreography\" of your agentic system. Just like a ballet performance, they define how different components interact, when they execute, and how information flows between them. Without good choreography, even the most talented individual performers can't create something beautiful together.\n",
    "\n",
    "**The Transformation:** Workflows transform simple LLM interactions into powerful, multi-step reasoning systems. Instead of asking an LLM to solve a complex problem in one shot (which often leads to mediocre results), workflows break down tasks into manageable pieces, allowing for specialization, validation, and iterative improvement.\n",
    "\n",
    "Here's why this matters so much:\n",
    "\n",
    "**Why Workflows Are Game-Changers:**\n",
    "\n",
    "1. **Task Decomposition**: Complex problems become manageable when broken into smaller, focused steps. Instead of \"write a marketing campaign,\" you might have \"research audience → generate concepts → create copy → review and refine.\"\n",
    "\n",
    "2. **Specialization**: Different parts of your system can excel at different aspects of the problem. Your research specialist can be different from your creative writer, each optimized for their specific role.\n",
    "\n",
    "3. **Quality Control**: You can add validation and error checking at each step. If the research step fails, you catch it before moving to content generation.\n",
    "\n",
    "4. **Scalability**: Parallel execution and efficient resource utilization mean you can handle more complex tasks without proportional increases in time.\n",
    "\n",
    "5. **Maintainability**: It's easier to debug, test, and improve individual components rather than trying to fix one monolithic prompt.\n",
    "\n",
    "**Understanding the Spectrum:** Workflows exist on a spectrum from simple sequential chains to fully autonomous agents:\n",
    "\n",
    "```\n",
    "Simple → Sequential → Parallel → Dynamic → Autonomous\n",
    "Chain     Routing     Execution   Orchestration   Agents\n",
    "```\n",
    "\n",
    "Each level adds complexity but also capability. The key is choosing the right level for your specific use case—sometimes a simple chain is perfect, other times you need full autonomy.\n",
    "\n",
    "**What We'll Build Together:** We'll start with basic prompt chaining, then work our way up to intelligent routing systems, parallel execution patterns, and eventually full autonomous agents. Each step builds on the previous one, so you'll understand not just how to build these systems, but when and why to use each approach.\n",
    "\n",
    "Building on Anthropic's foundational patterns, we can implement more sophisticated agentic systems that combine multiple workflows and demonstrate emergent behaviors. These advanced patterns represent the cutting edge of production agentic systems.\n",
    "\n",
    "##### Mathematical Foundations of Workflow Optimization\n",
    "\n",
    "**Error Propagation in Chains**: In prompt chaining, if each step has error rate ε, the cumulative error follows: \n",
    "$$E_{total} = 1 - \\prod_{i=1}^{n}(1-\\varepsilon_i)$$\n",
    "\n",
    "For identical error rates: $E_{total} = 1 - (1-\\varepsilon)^n$\n",
    "\n",
    "**Parallel Processing Speedup**: Theoretical speedup from parallelization follows Amdahl's Law:\n",
    "$$S = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where P is the parallelizable fraction and N is the number of processors.\n",
    "\n",
    "**Consensus Accuracy**: For voting systems with individual accuracy p, ensemble accuracy follows:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Iterative Improvement**: Quality improvement in evaluator-optimizer workflows can be modeled as:\n",
    "$$Q_n = Q_0 \\cdot (1 + \\alpha \\cdot \\beta^n)$$\n",
    "\n",
    "Where α is the improvement factor and β is the diminishing returns coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45774676",
   "metadata": {},
   "source": [
    "#### 1. Prompt Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7d88c",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ff0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Our First Workflow: Prompt Chaining System\n",
    "# We'll use our existing LLM to create a sequential workflow\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import SequentialChain\n",
    "import time\n",
    "\n",
    "print(\"🔗 PROMPT CHAINING WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what LLM instances we have available\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "class PromptChain:\n",
    "    \"\"\"\n",
    "    Sequential workflow system using our existing LLM\n",
    "    \n",
    "    This is like a factory assembly line where each step:\n",
    "    - Takes output from the previous step\n",
    "    - Performs focused transformation\n",
    "    - Passes result to next step\n",
    "    \n",
    "    Key insight: We reuse the same LLM throughout the chain,\n",
    "    just with different prompts for each step!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        self.llm = llm_instance\n",
    "        self.steps_executed = 0\n",
    "        print(f\"🏗️ Prompt Chain initialized\")\n",
    "        print(f\"   Using LLM: {self.llm.model}\")\n",
    "        print(f\"   Temperature: {self.llm.temperature}\")\n",
    "        \n",
    "    def create_step(self, name: str, instruction: str, gate_check=None):\n",
    "        \"\"\"\n",
    "        Define a step in our chain\n",
    "        \n",
    "        Args:\n",
    "            name: Step identifier for tracking\n",
    "            instruction: What this step should do\n",
    "            gate_check: Optional validation function\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"instruction\": instruction,\n",
    "            \"gate_check\": gate_check\n",
    "        }\n",
    "    \n",
    "    def execute_step(self, step, input_text):\n",
    "        \"\"\"\n",
    "        Execute a single step using our LLM\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Executing: {step['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Quality gate check\n",
    "        if step.get('gate_check') and not step['gate_check'](input_text):\n",
    "            print(f\"❌ Gate check failed for {step['name']}\")\n",
    "            return None\n",
    "            \n",
    "        # Create prompt for this step\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"instruction\"],\n",
    "            template=\"\"\"Task: {instruction}\n",
    "\n",
    "Input: {input}\n",
    "\n",
    "Provide a clear, focused response that can be used as input for the next step in the workflow.\n",
    "Be thorough but concise - the next step depends on your output quality.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Execute using our LLM\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"input\": input_text,\n",
    "            \"instruction\": step[\"instruction\"]\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        self.steps_executed += 1\n",
    "        \n",
    "        print(f\"✅ Completed in {execution_time:.2f}s\")\n",
    "        print(f\"   Output: {len(result)} characters\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create or reuse prompt chain\n",
    "if 'prompt_chain' not in tutorial_state:\n",
    "    prompt_chain = PromptChain(memory_llm)\n",
    "    tutorial_state['prompt_chain'] = prompt_chain\n",
    "    print(\"\\n✅ New Prompt Chain created\")\n",
    "else:\n",
    "    prompt_chain = tutorial_state['prompt_chain']\n",
    "    print(f\"\\n✅ Reusing existing Prompt Chain\")\n",
    "    print(f\"   Steps executed so far: {prompt_chain.steps_executed}\")\n",
    "\n",
    "print(\"\\n💡 This chain will reuse our memory_llm for all steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our prompt chain is already initialized in the previous cell\n",
    "# Let's verify it's ready and show what we have\n",
    "\n",
    "print(\"🔍 Verifying Workflow System Status\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if prompt_chain:\n",
    "    print(\"✅ Prompt Chain System Ready\")\n",
    "    print(f\"   LLM Model: {prompt_chain.llm.model}\")\n",
    "    print(f\"   Temperature: {prompt_chain.llm.temperature}\")\n",
    "    print(f\"   Steps executed: {prompt_chain.steps_executed}\")\n",
    "    print(f\"   Status: Ready for sequential workflows\")\n",
    "else:\n",
    "    print(\"⚠️ Prompt chain not found, please run previous cell\")\n",
    "\n",
    "print(\"\\n💡 Ready to build and execute sequential workflows!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ba278",
   "metadata": {},
   "source": [
    "Now Let's Build and Test Our First Chain\n",
    "We'll create a practical workflow for marketing copy that demonstrates all the key concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build and execute a marketing workflow using our existing prompt chain\n",
    "# Notice how we reuse the chain we created earlier\n",
    "\n",
    "print(\"📝 BUILDING MARKETING WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our prompt chain from tutorial_state\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if not prompt_chain:\n",
    "    print(\"⚠️ Prompt chain not initialized. Please run previous cells.\")\n",
    "else:\n",
    "    print(f\"✅ Using existing Prompt Chain (executed {prompt_chain.steps_executed} steps so far)\")\n",
    "    \n",
    "    # Define our workflow steps\n",
    "    print(\"\\n🔧 Defining workflow steps...\")\n",
    "    \n",
    "    # Step 1: Content Creation\n",
    "    content_step = prompt_chain.create_step(\n",
    "        \"content_creation\",\n",
    "        \"Create compelling marketing copy for a new AI productivity tool. Focus on benefits for busy professionals and include a strong call-to-action.\"\n",
    "    )\n",
    "    print(\"   1. Content Creation ○\")\n",
    "    \n",
    "    # Step 2: Quality Review with Gate Check\n",
    "    quality_step = prompt_chain.create_step(\n",
    "        \"quality_review\", \n",
    "        \"Review this marketing copy for clarity, persuasiveness, and professional tone. Improve grammar and strengthen the value proposition.\",\n",
    "        gate_check=lambda x: len(x) > 50 and len(x.split()) > 10\n",
    "    )\n",
    "    print(\"   2. Quality Review ✓ (with gate check)\")\n",
    "    \n",
    "    # Step 3: Translation\n",
    "    translation_step = prompt_chain.create_step(\n",
    "        \"translation\",\n",
    "        \"Translate this marketing copy to Spanish while maintaining tone and persuasiveness.\"\n",
    "    )\n",
    "    print(\"   3. Translation ○\")\n",
    "    \n",
    "    # Execute the workflow\n",
    "    steps = [content_step, quality_step, translation_step]\n",
    "    print(f\"\\n\ude80 EXECUTING WORKFLOW ({len(steps)} steps)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    current_input = \"AI productivity tool for busy professionals\"\n",
    "    results = []\n",
    "    \n",
    "    for i, step in enumerate(steps, 1):\n",
    "        print(f\"\\n--- Step {i}: {step['name']} ---\")\n",
    "        print(f\"Input: {current_input[:60]}...\")\n",
    "        \n",
    "        # Execute using our reusable prompt chain\n",
    "        result = prompt_chain.execute_step(step, current_input)\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"❌ Chain terminated due to step failure\")\n",
    "            break\n",
    "        \n",
    "        results.append({\n",
    "            \"step_number\": i,\n",
    "            \"step_name\": step['name'],\n",
    "            \"input_length\": len(current_input),\n",
    "            \"output_length\": len(result),\n",
    "            \"output_preview\": result[:100] + \"...\"\n",
    "        })\n",
    "        \n",
    "        # Output becomes next input\n",
    "        current_input = result\n",
    "    \n",
    "    # Store results\n",
    "    tutorial_state['chain_results'] = results\n",
    "    tutorial_state['latest_workflow_output'] = current_input\n",
    "    \n",
    "    print(f\"\\n✅ Workflow completed: {len(results)} steps executed\")\n",
    "    print(f\"📊 Total steps by this chain: {prompt_chain.steps_executed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze our workflow execution results\n",
    "print(\"📊 WORKFLOW EXECUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get results from tutorial_state\n",
    "results = tutorial_state.get('chain_results', [])\n",
    "prompt_chain = tutorial_state.get('prompt_chain')\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n✅ Successfully completed {len(results)} steps\")\n",
    "    \n",
    "    print(\"\\n📈 Content Evolution:\")\n",
    "    for result in results:\n",
    "        print(f\"  Step {result['step_number']} - {result['step_name']}:\")\n",
    "        print(f\"    Input → Output: {result['input_length']} → {result['output_length']} chars\")\n",
    "        print(f\"    Preview: {result['output_preview']}\")\n",
    "    \n",
    "    # Show final output\n",
    "    final_output = tutorial_state.get('latest_workflow_output')\n",
    "    if final_output:\n",
    "        print(f\"\\n📝 Final Output Preview:\")\n",
    "        print(f\"   {final_output[:150]}...\")\n",
    "    \n",
    "    # Show chain stats\n",
    "    if prompt_chain:\n",
    "        print(f\"\\n📊 Chain Statistics:\")\n",
    "        print(f\"   Total steps executed by this chain: {prompt_chain.steps_executed}\")\n",
    "        print(f\"   LLM reused throughout: {prompt_chain.llm.model}\")\n",
    "    \n",
    "    print(\"\\n💡 Key Insight: One LLM, multiple transformations!\")\n",
    "    print(\"   We didn't create new LLM instances for each step\")\n",
    "    print(\"   We reused the same one with different prompts\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No workflow results found. Please run previous cell.\")\n",
    "\n",
    "print(\"\\n✅ Results stored in tutorial_state for further analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e0d2b",
   "metadata": {},
   "source": [
    "#### 2. Routing Workflows - Intelligent Task Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fcf46",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d6c86",
   "metadata": {},
   "source": [
    "Now let's explore routing workflows, which intelligently classify inputs and direct them to specialized handlers. Think of it as a smart switchboard that sends different types of requests to the most appropriate specialist.\n",
    "\n",
    "**The Problem Routing Solves:**\n",
    "\n",
    "Imagine building a customer service system. You could create one massive prompt that tries to handle all types of inquiries, but this leads to:\n",
    "- Generic responses that aren't specialized enough\n",
    "- Conflicting optimization (improving billing support might hurt technical support)\n",
    "- Difficulty in maintaining and improving specific areas\n",
    "\n",
    "**How Routing Works:**\n",
    "\n",
    "1. **Classification**: Analyze the input to determine its type/category\n",
    "2. **Route Selection**: Choose the appropriate specialized handler\n",
    "3. **Execution**: Process using the selected specialist\n",
    "4. **Response**: Return the specialized result\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "Routing leverages the principle of **specialization gains**. If we have accuracy A_general for a general system and A_specialized for specialists, routing achieves:\n",
    "\n",
    "$$Accuracy_{routed} = \\sum_{i} P(category_i) \\times A_{specialist_i}$$\n",
    "\n",
    "Where P(category_i) is the probability of correct classification.\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Specialization**: Each route can be optimized for specific input types\n",
    "- **Maintainability**: Update one route without affecting others\n",
    "- **Performance**: Use different models/strategies per route (fast vs. accurate)\n",
    "- **Cost Optimization**: Route simple queries to cheaper models\n",
    "\n",
    "Let's build a routing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an Intelligent Routing System\n",
    "\n",
    "class IntelligentRouter:\n",
    "    \"\"\"\n",
    "    An intelligent routing system that acts like a smart receptionist.\n",
    "    \n",
    "    and extend our existing prompt patterns instead of creating everything from scratch.\n",
    "    \n",
    "    This approach shows:\n",
    "    - How to build upon existing components\n",
    "    - Maintaining consistency across the codebase\n",
    "    - Reducing memory usage and initialization time\n",
    "    - Making the tutorial flow more logical and connected\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance=None):\n",
    "        self.llm = llm_instance or llm  # Falls back to global llm\n",
    "        self.routes = {}\n",
    "        print(\"🎯 Initializing intelligent routing system using existing LLM...\")\n",
    "        \n",
    "        # Notice how we're extending the structure we already established\n",
    "        self.router_prompt = PromptTemplate(\n",
    "            input_variables=[\"input_text\", \"available_routes\"],\n",
    "            template=\"\"\"You are an intelligent classification system. Your job is to analyze the input and determine which specialist should handle it.\n",
    "\n",
    "Input to classify: {input_text}\n",
    "\n",
    "Available specialists:\n",
    "{available_routes}\n",
    "\n",
    "CRITICAL: Respond with ONLY the route name that best matches the input type. \n",
    "No explanation, no extra text - just the exact route name.\n",
    "If unsure, choose the most general route available.\"\"\"\n",
    "        )\n",
    "        \n",
    "        tutorial_state[\"routers\"] = tutorial_state.get(\"routers\", {})\n",
    "        tutorial_state[\"routers\"][\"main_router\"] = self\n",
    "        \n",
    "        print(\"🔄 Router initialized and stored in tutorial_state\")\n",
    "    \n",
    "    def register_route(self, name, description, template=None, confidence=0.8):\n",
    "        \"\"\"\n",
    "        Register a new specialist route.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if template is None:\n",
    "            # Check if we have a suitable existing template\n",
    "            existing_templates = tutorial_state.get(\"prompt_templates\", {})\n",
    "            if \"basic\" in existing_templates:\n",
    "                print(f\"🔄 Reusing existing basic template for route '{name}'\")\n",
    "                template = existing_templates[\"basic\"]\n",
    "            else:\n",
    "                # Fallback: create a simple template\n",
    "                template = PromptTemplate(\n",
    "                    input_variables=[\"input\"],\n",
    "                    template=\"Handle this request: {input}\"\n",
    "                )\n",
    "        \n",
    "        self.routes[name] = {\n",
    "            \"description\": description,\n",
    "            \"template\": template,\n",
    "            \"confidence\": confidence,\n",
    "            \"usage_count\": 0  # Track how often this route is used\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def route(self, input_text: str):\n",
    "        \"\"\"\n",
    "        Route input to the appropriate specialist\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.routes:\n",
    "            return \"No routes registered. Please register routes first.\"\n",
    "        \n",
    "        # Build available routes description for the classifier\n",
    "        routes_desc = \"\\n\".join([\n",
    "            f\"- {name}: {route['description']}\" \n",
    "            for name, route in self.routes.items()\n",
    "        ])\n",
    "        \n",
    "        router_chain = self.router_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # Get the route decision\n",
    "            chosen_route = router_chain.invoke({\n",
    "                \"input_text\": input_text,\n",
    "                \"available_routes\": routes_desc\n",
    "            }).strip()\n",
    "            \n",
    "            # Validate the route exists\n",
    "            if chosen_route in self.routes:\n",
    "                # Update usage stats\n",
    "                self.routes[chosen_route][\"usage_count\"] += 1\n",
    "                return chosen_route\n",
    "            else:\n",
    "                # Fallback to first available route\n",
    "                fallback_route = list(self.routes.keys())[0]\n",
    "                print(f\"⚠️ Route '{chosen_route}' not found, using fallback: {fallback_route}\")\n",
    "                return fallback_route\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Routing error: {e}\")\n",
    "            return list(self.routes.keys())[0] if self.routes else None\n",
    "\n",
    "print(\"🚀 Creating Intelligent Router using existing components...\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our global LLM instead of creating a new one\n",
    "intelligent_router = IntelligentRouter(llm_instance=llm)\n",
    "\n",
    "# Register some routes reusing our existing templates\n",
    "print(\"\\n📝 Registering routes with existing templates...\")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"general_chat\",\n",
    "    description=\"General conversation and questions\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"chat\"],\n",
    "    confidence=0.7\n",
    ")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"explanation\", \n",
    "    description=\"Detailed explanations of concepts and topics\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"basic\"],\n",
    "    confidence=0.9\n",
    ")\n",
    "\n",
    "# Register a specialized route (will create new template only if needed)\n",
    "intelligent_router.register_route(\n",
    "    name=\"technical_analysis\",\n",
    "    description=\"Technical analysis and code-related questions\",\n",
    "    confidence=0.8\n",
    ")\n",
    "\n",
    "print(\"\\n✅ ROUTING SYSTEM READY\")\n",
    "print(\"📦 Router stored in tutorial_state for future use\")\n",
    "print(f\"🎯 {len(intelligent_router.routes)} routes registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c21fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our routing system using the global llm\n",
    "# This router will intelligently direct queries to specialists\n",
    "\n",
    "print(\"🎯 Initializing Intelligent Router\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if router already exists\n",
    "if 'router' not in tutorial_state:\n",
    "    # Create router using our global llm\n",
    "    router = IntelligentRouter(llm_instance=llm)\n",
    "    tutorial_state['router'] = router\n",
    "    print(\"✅ New router created using global llm\")\n",
    "else:\n",
    "    router = tutorial_state['router']\n",
    "    print(\"✅ Using existing router from tutorial_state\")\n",
    "\n",
    "print(f\"🔧 Router uses: {router.llm.model}\")\n",
    "print(f\"🌡️  Temperature: {router.llm.temperature}\")\n",
    "print(\"💡 Ready to register specialist routes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal router for teaching (keyword-based, synchronous)\n",
    "class SimpleRouter:\n",
    "    def __init__(self, llm_instance=None):\n",
    "        # use provided llm or global one\n",
    "        self.llm = llm_instance or globals().get('llm')\n",
    "        self.routes = {}\n",
    "\n",
    "    def register_route(self, name, description, template, confidence=0.5):\n",
    "        self.routes[name] = {\"description\": description, \"template\": template, \"confidence\": confidence, \"usage_count\": 0}\n",
    "\n",
    "    def route(self, text: str):\n",
    "        tl = text.lower()\n",
    "        if any(k in tl for k in (\"error\", \"crash\", \"bug\", \"failed\")):\n",
    "            return \"technical_support\"\n",
    "        if any(k in tl for k in (\"charge\", \"refund\", \"billing\", \"invoice\")):\n",
    "            return \"billing_support\"\n",
    "        return \"general_inquiry\"\n",
    "\n",
    "# Instantiate a simple router and register three concise specialist routes\n",
    "router = SimpleRouter(llm_instance=llm)\n",
    "router.register_route(\"technical_support\", \"Troubleshoot technical issues\", \"Technical troubleshooting template\", confidence=0.9)\n",
    "router.register_route(\"billing_support\", \"Handle billing questions\", \"Billing support template\", confidence=0.85)\n",
    "router.register_route(\"general_inquiry\", \"General customer questions\", \"General response template\", confidence=0.75)\n",
    "\n",
    "# expose router to tutorial_state for reuse\n",
    "tutorial_state[\"router\"] = router\n",
    "print(\"Minimal router configured with 3 routes (technical_support, billing_support, general_inquiry).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display our registered team (simple view)\n",
    "print(f\"\\n✅ SPECIALIST TEAM ASSEMBLED\")\n",
    "print(f\"Total specialists registered: {len(tutorial_state['router'].routes)}\")\n",
    "\n",
    "print(f\"\\n📊 TEAM ROSTER:\")\n",
    "for route_name, route_info in tutorial_state['router'].routes.items():\n",
    "    print(f\"   🎯 {route_name}\")\n",
    "    print(f\"      Confidence: {route_info['confidence']}\")\n",
    "    print(f\"      Usage: {route_info['usage_count']}\")\n",
    "    print(f\"      Specialty: {route_info['description']}\")\n",
    "    print()\n",
    "\n",
    "print(\"🚀 Ready to start routing customer inquiries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b28df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal routing processor — uses the simple router and returns simulated responses\n",
    "\n",
    "def route_and_process(input_text: str):\n",
    "    router = tutorial_state.get('router')\n",
    "    if not router or not router.routes:\n",
    "        return {\"route\": \"unhandled\", \"result\": \"Router not initialized\", \"confidence\": 0.0}\n",
    "\n",
    "    selected_route = router.route(input_text)\n",
    "    router.routes[selected_route][\"usage_count\"] += 1\n",
    "\n",
    "    # For teaching, avoid a full LLM call here; produce a clear simulated response\n",
    "    template = router.routes[selected_route][\"template\"]\n",
    "    simulated_response = f\"(simulated) {selected_route} handled the query. Template used: {template}\"\n",
    "\n",
    "    return {\n",
    "        \"route\": selected_route,\n",
    "        \"result\": simulated_response,\n",
    "        \"confidence\": router.routes[selected_route][\"confidence\"],\n",
    "        \"specialist_usage\": router.routes[selected_route][\"usage_count\"]\n",
    "    }\n",
    "\n",
    "print(\"✅ Routing function ready (simulated responses for teaching).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f28f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Suite: Real Customer Inquiries\n",
    "# Let's test our routing system with realistic customer service scenarios\n",
    "print(f\"\\n🧪 COMPREHENSIVE ROUTING TEST SUITE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# These are real-world examples that show different types of customer inquiries\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Technical Issue\",\n",
    "        \"query\": \"My app keeps crashing every time I try to export a file. I get error code 500 and then it just closes. This happens on both Windows and Mac versions.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Billing Problem\", \n",
    "        \"query\": \"I was charged twice for my subscription this month and I need a refund for the duplicate charge. My card ending in 1234 shows two charges on October 15th.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Product Question\",\n",
    "        \"query\": \"What's the difference between your premium and enterprise plans? I'm trying to decide which one would be best for a team of 15 people.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Mixed Technical/Billing\",\n",
    "        \"query\": \"I upgraded to premium but I'm still seeing ads and getting limited features. Did my payment go through? How can I check my account status?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simplified routing tests and capture results\n",
    "routing_results = []\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n--- TEST {i}: {scenario['scenario']} ---\")\n",
    "    result = route_and_process(scenario['query'])\n",
    "\n",
    "    result['test_scenario'] = scenario['scenario']\n",
    "    result['original_query'] = scenario['query']\n",
    "    routing_results.append(result)\n",
    "\n",
    "    print(f\"🎯 Route: {result['route']}\")\n",
    "    print(f\"📊 Confidence: {result['confidence']}\")\n",
    "    print(f\"📝 Response preview: {result['result'][:120]}...\")\n",
    "    print(f\"📈 Specialist usage count: {result.get('specialist_usage')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b426786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Performance Analysis (simple metrics)\n",
    "print(f\"\\n📊 ROUTING SYSTEM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "route_distribution = {}\n",
    "for result in routing_results:\n",
    "    route = result['route']\n",
    "    route_distribution[route] = route_distribution.get(route, 0) + 1\n",
    "\n",
    "print(f\"📈 ROUTING DISTRIBUTION:\")\n",
    "for route_name, count in route_distribution.items():\n",
    "    percentage = (count / len(routing_results)) * 100\n",
    "    print(f\"   {route_name}: {count} queries ({percentage:.1f}%)\")\n",
    "\n",
    "avg_confidence = sum(r['confidence'] for r in routing_results) / len(routing_results)\n",
    "print(f\"\\n🎯 SYSTEM METRICS:\")\n",
    "print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "print(f\"   Successful routes: {len([r for r in routing_results if r['route'] != 'unhandled'])}/{len(routing_results)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea4419",
   "metadata": {},
   "source": [
    "#### 3. Parallelization Workflows - Speed and Consensus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd24e58",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec847a",
   "metadata": {},
   "source": [
    "Parallelization is where things get interesting. Instead of processing sequentially, we can execute multiple tasks simultaneously, either to **divide the work** (sectioning) or to get **multiple perspectives** (voting). This is crucial for production systems where speed and accuracy both matter.\n",
    "\n",
    "**Two Flavors of Parallelization:**\n",
    "\n",
    "1. **Sectioning**: Break a large task into independent parts that can run simultaneously\n",
    "   - Example: Analyzing a document from financial, legal, and technical perspectives\n",
    "   - Benefit: Speed (total time = max individual time, not sum)\n",
    "\n",
    "2. **Voting**: Run the same task multiple times to reach consensus  \n",
    "   - Example: Multiple models evaluating content safety\n",
    "   - Benefit: Accuracy through ensemble effects\n",
    "\n",
    "**Mathematical Foundation - Amdahl's Law:**\n",
    "\n",
    "The theoretical speedup from parallelization follows:\n",
    "$$Speedup = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where:\n",
    "- P = fraction of work that can be parallelized  \n",
    "- N = number of parallel processors\n",
    "\n",
    "**Voting Accuracy (Condorcet's Jury Theorem):**\n",
    "\n",
    "If individual classifiers have accuracy p > 0.5, ensemble accuracy with n classifiers is:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "This means ensemble accuracy increases with more voters (if individual accuracy > 50%).\n",
    "\n",
    "**When to Use Parallelization:**\n",
    "- **Sectioning**: When you can identify independent subtasks\n",
    "- **Voting**: When you need high-confidence decisions\n",
    "- **Speed Requirements**: When latency is critical\n",
    "- **Quality Requirements**: When accuracy is paramount\n",
    "\n",
    "Let's implement both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9896afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing with our existing LLM\n",
    "# We'll reuse our memory_llm for parallel task execution\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "print(\"⚡ PARALLEL PROCESSING SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ParallelProcessor:\n",
    "    \"\"\"\n",
    "    Execute multiple tasks in parallel using our existing LLM\n",
    "    \n",
    "    Key insight: We use the SAME LLM for all parallel tasks,\n",
    "    but execute them simultaneously in different threads!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        self.llm = llm_instance\n",
    "        self.tasks_executed = 0\n",
    "        print(f\"⚡ Parallel Processor initialized\")\n",
    "        print(f\"   Using LLM: {self.llm.model}\")\n",
    "        print(f\"   Temperature: {self.llm.temperature}\")\n",
    "        \n",
    "    def create_section_task(self, name, focus_area, analysis_prompt):\n",
    "        \"\"\"Define a parallel task section\"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"focus\": focus_area,\n",
    "            \"prompt_template\": analysis_prompt\n",
    "        }\n",
    "    \n",
    "    def execute_section(self, task, input_data):\n",
    "        \"\"\"Execute one section using our LLM\"\"\"\n",
    "        print(f\"🔄 Processing: {task['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create prompt for this section\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"focus\"],\n",
    "            template=task[\"prompt_template\"]\n",
    "        )\n",
    "        \n",
    "        # Execute using our shared LLM\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"data\": input_data,\n",
    "            \"focus\": task[\"focus\"]\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        self.tasks_executed += 1\n",
    "        \n",
    "        print(f\"✅ '{task['name']}' done in {execution_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            \"section\": task[\"name\"],\n",
    "            \"focus\": task[\"focus\"],\n",
    "            \"result\": result,\n",
    "            \"execution_time\": execution_time\n",
    "        }\n",
    "\n",
    "# Get our memory_llm for consistent parallel processing\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "# Create or reuse parallel processor\n",
    "if 'parallel_processor' not in tutorial_state:\n",
    "    parallel_processor = ParallelProcessor(memory_llm)\n",
    "    tutorial_state['parallel_processor'] = parallel_processor\n",
    "    print(\"\\n✅ New Parallel Processor created\")\n",
    "else:\n",
    "    parallel_processor = tutorial_state['parallel_processor']\n",
    "    print(f\"\\n✅ Reusing existing Parallel Processor\")\n",
    "    print(f\"   Tasks executed so far: {parallel_processor.tasks_executed}\")\n",
    "\n",
    "print(\"\\n💡 All parallel tasks will use the SAME LLM instance\")\n",
    "print(\"   Parallelization happens at the execution level, not LLM level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our parallel processor using the existing memory_llm\n",
    "print(\"⚡ Initializing Parallel Processor\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm (which we use for consistent processing)\n",
    "memory_llm = tutorial_state.get(\"memory_llm\")\n",
    "\n",
    "if not memory_llm:\n",
    "    print(\"⚠️ Memory LLM not found, using global llm\")\n",
    "    memory_llm = llm\n",
    "\n",
    "# Check if parallel processor exists\n",
    "if 'parallel_processor' not in tutorial_state:\n",
    "    parallel_processor = ParallelProcessor(memory_llm)\n",
    "    tutorial_state['parallel_processor'] = parallel_processor\n",
    "    print(\"✅ New parallel processor created\")\n",
    "else:\n",
    "    parallel_processor = tutorial_state['parallel_processor']\n",
    "    print(\"✅ Using existing parallel processor\")\n",
    "\n",
    "print(f\"🔧 Processor uses same LLM as memory operations\")\n",
    "print(\"💡 Ready for parallel task execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parallel analysis tasks using our existing processor\n",
    "print(\"🏗️ BUILDING PARALLEL BUSINESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our parallel processor\n",
    "parallel_processor = tutorial_state.get('parallel_processor')\n",
    "\n",
    "if not parallel_processor:\n",
    "    print(\"⚠️ Parallel processor not initialized\")\n",
    "else:\n",
    "    print(f\"✅ Using existing Parallel Processor\")\n",
    "    print(f\"   Tasks executed: {parallel_processor.tasks_executed}\")\n",
    "    \n",
    "    # Define parallel sections for business analysis\n",
    "    print(\"\\n📊 Defining analysis sections...\")\n",
    "    \n",
    "    section_tasks = [\n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Financial Analysis\",\n",
    "            focus_area=\"financial metrics and projections\",\n",
    "            analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus specifically on financial health, revenue trends, profitability, and financial risks. \n",
    "Provide key metrics, insights, and recommendations.\"\"\"\n",
    "        ),\n",
    "        \n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Market Analysis\", \n",
    "            focus_area=\"market position and competitive landscape\",\n",
    "            analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on market opportunity, competitive advantages, market risks, and positioning.\n",
    "Provide market insights and strategic recommendations.\"\"\"\n",
    "        ),\n",
    "        \n",
    "        parallel_processor.create_section_task(\n",
    "            name=\"Operational Analysis\",\n",
    "            focus_area=\"operational efficiency and scalability\", \n",
    "            analysis_prompt=\"\"\"Analyze this business data from an {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on operational strengths, efficiency metrics, scalability factors, and operational risks.\n",
    "Provide operational insights and improvement recommendations.\"\"\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Created {len(section_tasks)} parallel analysis tasks\")\n",
    "    for task in section_tasks:\n",
    "        print(f\"   • {task['name']}\")\n",
    "    \n",
    "    # Store tasks for execution in next cell\n",
    "    tutorial_state['parallel_tasks'] = section_tasks\n",
    "    \n",
    "    print(\"\\n💡 Each task uses the SAME LLM but runs in parallel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel business analysis using our existing processor\n",
    "print(\"🚀 EXECUTING PARALLEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our processor and tasks\n",
    "parallel_processor = tutorial_state.get('parallel_processor')\n",
    "section_tasks = tutorial_state.get('parallel_tasks', [])\n",
    "\n",
    "if not parallel_processor or not section_tasks:\n",
    "    print(\"⚠️ Prerequisites not ready. Please run previous cells.\")\n",
    "else:\n",
    "    # Business data to analyze\n",
    "    business_data = \"\"\"\n",
    "TechStartup Inc. Q3 2024 Summary:\n",
    "- Revenue: $2.5M (up 150% YoY)\n",
    "- Monthly Active Users: 50,000 (up 200% YoY) \n",
    "- Customer Acquisition Cost: $45\n",
    "- Monthly Churn Rate: 3.2%\n",
    "- Burn Rate: $300K/month\n",
    "- Cash Runway: 18 months\n",
    "- Team Size: 25 employees\n",
    "- Market Size: $10B TAM\n",
    "- Top 3 competitors: BigCorp, StartupX, TechGiant\n",
    "- Key Features: AI automation, real-time collaboration, mobile-first\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"📊 Analyzing business data with {len(section_tasks)} parallel sections\")\n",
    "    \n",
    "    # Execute all sections in parallel\n",
    "    def run_parallel_analysis(tasks, data):\n",
    "        \"\"\"Run sections in parallel using ThreadPoolExecutor\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_task = {\n",
    "                executor.submit(parallel_processor.execute_section, task, data): task\n",
    "                for task in tasks\n",
    "            }\n",
    "            \n",
    "            # Collect results\n",
    "            results = []\n",
    "            for future in as_completed(future_to_task):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return results, total_time\n",
    "    \n",
    "    # Run the parallel analysis\n",
    "    results, total_time = run_parallel_analysis(section_tasks, business_data)\n",
    "    \n",
    "    # Calculate speedup\n",
    "    sequential_time = sum(r[\"execution_time\"] for r in results)\n",
    "    speedup = sequential_time / total_time\n",
    "    \n",
    "    print(f\"\\n⚡ PARALLEL EXECUTION COMPLETE\")\n",
    "    print(f\"   Wall-clock time: {total_time:.2f}s\")\n",
    "    print(f\"   Sequential time would be: {sequential_time:.2f}s\")\n",
    "    print(f\"   Speedup achieved: {speedup:.1f}x\")\n",
    "    \n",
    "    print(f\"\\n📋 Sections completed:\")\n",
    "    for result in results:\n",
    "        print(f\"  • {result['section']}: {result['execution_time']:.2f}s\")\n",
    "    \n",
    "    # Store results\n",
    "    tutorial_state['parallel_results'] = results\n",
    "    tutorial_state['parallel_speedup'] = speedup\n",
    "    \n",
    "    print(f\"\\n💡 Same LLM, parallel execution = {speedup:.1f}x faster!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parallel analysis results\n",
    "print(\"📊 PARALLEL ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = tutorial_state.get('parallel_results', [])\n",
    "speedup = tutorial_state.get('parallel_speedup', 0)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n✅ Completed {len(results)} parallel analyses\")\n",
    "    print(f\"⚡ Speedup: {speedup:.1f}x faster than sequential\")\n",
    "    \n",
    "    print(f\"\\n\udcc8 Results by section:\")\n",
    "    for result in results:\n",
    "        print(f\"\\n  {result['section']}:\")\n",
    "        print(f\"    Time: {result['execution_time']:.2f}s\")\n",
    "        print(f\"    Focus: {result['focus']}\")\n",
    "        print(f\"    Output: {len(result['result'])} characters\")\n",
    "    \n",
    "    print(f\"\\n💡 Key Insight:\")\n",
    "    print(f\"   We used ONE LLM instance for all {len(results)} tasks\")\n",
    "    print(f\"   Parallel execution happened at the thread level\")\n",
    "    print(f\"   This is more efficient than creating multiple LLM instances!\")\n",
    "else:\n",
    "    print(\"⚠️ No results found. Please run previous cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parallel Voting - Consensus Through Multiple Perspectives\n",
    "\n",
    "class VotingSystem:\n",
    "    \"\"\"Implement parallel voting for consensus decisions\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance):\n",
    "        \"\"\"\n",
    "        Initialize voting system with existing LLM instance\n",
    "        \n",
    "        TUTORIAL NOTE: We receive the LLM instance instead of creating a new one.\n",
    "        This follows our principle of reusing components for efficiency.\n",
    "        \"\"\"\n",
    "        self.llm = llm_instance\n",
    "        self.votes_cast = 0  # Track voting activity\n",
    "        \n",
    "    def create_vote_prompt(self, base_instruction, perspective_twist=\"\"):\n",
    "        \"\"\"Create a voting prompt with slight variation for diversity\"\"\"\n",
    "        return f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "{perspective_twist}\n",
    "\n",
    "Analyze carefully and provide your assessment. End your response with a clear decision:\n",
    "DECISION: [YES/NO/UNCERTAIN]\n",
    "CONFIDENCE: [1-10]\n",
    "\"\"\"\n",
    "    \n",
    "    def cast_vote(self, vote_id, content, instruction, perspective=\"\"):\n",
    "        \"\"\"Cast a single vote in the voting process\"\"\"\n",
    "        prompt_text = self.create_vote_prompt(instruction, perspective)\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"content\"],\n",
    "            template=prompt_text + \"\\n\\nContent to evaluate: {content}\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\"content\": content})\n",
    "        \n",
    "        self.votes_cast += 1  # Track each vote\n",
    "        \n",
    "        # Extract decision (simplified parsing)\n",
    "        decision = \"UNCERTAIN\"\n",
    "        confidence = 5\n",
    "        \n",
    "        if \"DECISION: YES\" in response:\n",
    "            decision = \"YES\"\n",
    "        elif \"DECISION: NO\" in response:\n",
    "            decision = \"NO\"\n",
    "            \n",
    "        # Try to extract confidence\n",
    "        if \"CONFIDENCE:\" in response:\n",
    "            try:\n",
    "                conf_line = [line for line in response.split('\\n') if 'CONFIDENCE:' in line][0]\n",
    "                confidence = int(conf_line.split(':')[1].strip().split()[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            \"vote_id\": vote_id,\n",
    "            \"decision\": decision,\n",
    "            \"confidence\": confidence,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    \n",
    "    def parallel_voting(self, content, base_instruction, num_votes=3):\n",
    "        \"\"\"Execute parallel voting with multiple perspectives\"\"\"\n",
    "        \n",
    "        # Create diverse perspectives for voting\n",
    "        perspectives = [\n",
    "            \"Consider this from a conservative, risk-averse viewpoint.\",\n",
    "            \"Evaluate this from an optimistic, opportunity-focused angle.\", \n",
    "            \"Analyze this from a balanced, neutral perspective.\"\n",
    "        ]\n",
    "        \n",
    "        # Ensure we have enough perspectives\n",
    "        while len(perspectives) < num_votes:\n",
    "            perspectives.append(f\"Provide perspective #{len(perspectives) + 1} evaluation.\")\n",
    "        \n",
    "        print(f\"🗳️ Conducting parallel voting with {num_votes} voters\")\n",
    "        print(f\"   Using existing LLM instance (votes cast so far: {self.votes_cast})\")\n",
    "        \n",
    "        # Execute votes in parallel\n",
    "        with ThreadPoolExecutor(max_workers=num_votes) as executor:\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    self.cast_vote, \n",
    "                    f\"voter_{i+1}\", \n",
    "                    content, \n",
    "                    base_instruction,\n",
    "                    perspectives[i]\n",
    "                )\n",
    "                for i in range(num_votes)\n",
    "            ]\n",
    "            \n",
    "            votes = [future.result() for future in futures]\n",
    "        \n",
    "        # Calculate consensus\n",
    "        decisions = [vote[\"decision\"] for vote in votes]\n",
    "        confidences = [vote[\"confidence\"] for vote in votes]\n",
    "        \n",
    "        yes_votes = decisions.count(\"YES\")\n",
    "        no_votes = decisions.count(\"NO\") \n",
    "        uncertain_votes = decisions.count(\"UNCERTAIN\")\n",
    "        \n",
    "        # Determine consensus\n",
    "        if yes_votes > no_votes and yes_votes > uncertain_votes:\n",
    "            consensus = \"YES\"\n",
    "        elif no_votes > yes_votes and no_votes > uncertain_votes:\n",
    "            consensus = \"NO\"\n",
    "        else:\n",
    "            consensus = \"NO CONSENSUS\"\n",
    "            \n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        \n",
    "        return {\n",
    "            \"votes\": votes,\n",
    "            \"consensus\": consensus,\n",
    "            \"vote_breakdown\": {\n",
    "                \"YES\": yes_votes,\n",
    "                \"NO\": no_votes, \n",
    "                \"UNCERTAIN\": uncertain_votes\n",
    "            },\n",
    "            \"average_confidence\": avg_confidence\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b81d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize voting system using our existing memory_llm\n",
    "print(\"🗳️ Initializing Voting System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get our memory_llm for consistent voting\n",
    "memory_llm = tutorial_state.get(\"memory_llm\", llm)\n",
    "\n",
    "# Check if voting system exists - reuse if available\n",
    "if 'voting_system' not in tutorial_state:\n",
    "    voting_system = VotingSystem(memory_llm)\n",
    "    tutorial_state['voting_system'] = voting_system\n",
    "    print(\"✅ New voting system created with existing LLM\")\n",
    "    print(f\"   LLM instance: memory_llm (temp: 0.2)\")\n",
    "else:\n",
    "    voting_system = tutorial_state['voting_system']\n",
    "    print(\"✅ Reusing existing voting system\")\n",
    "    print(f\"   Total votes cast: {voting_system.votes_cast}\")\n",
    "\n",
    "print(f\"\\n🔧 Configuration:\")\n",
    "print(f\"   • Uses same LLM as memory operations\")\n",
    "print(f\"   • Enables parallel consensus voting\")\n",
    "print(f\"   • Tracks voting activity across sessions\")\n",
    "print(\"\\n💡 Ready for consensus-based decision making\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Voting Demo - Content Safety Assessment\n",
    "\n",
    "# Get our voting system from tutorial state\n",
    "voting_system = tutorial_state['voting_system']\n",
    "\n",
    "# Content to evaluate for safety\n",
    "test_content = \"\"\"\n",
    "Check out this amazing new productivity method that will totally change your life! \n",
    "It's based on scientific research and has helped thousands of people achieve their goals faster than ever before.\n",
    "Join our community today and start your transformation journey!\n",
    "\"\"\"\n",
    "\n",
    "safety_instruction = \"\"\"\n",
    "Evaluate whether this content is appropriate for a professional platform.\n",
    "\n",
    "Consider these factors:\n",
    "- Is it truthful and not misleading?\n",
    "- Does it avoid excessive hype or unrealistic claims?\n",
    "- Is it suitable for a professional audience?\n",
    "- Does it comply with content guidelines?\n",
    "\n",
    "Provide detailed reasoning for your assessment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🛡️ CONTENT SAFETY VOTING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluating content: '{test_content[:60]}...'\")\n",
    "print(f\"Using voting system with {voting_system.votes_cast} votes cast previously\\n\")\n",
    "\n",
    "# Conduct the vote using our existing voting system\n",
    "voting_result = voting_system.parallel_voting(\n",
    "    content=test_content,\n",
    "    base_instruction=safety_instruction,\n",
    "    num_votes=5\n",
    ")\n",
    "\n",
    "# Store results for later reference\n",
    "tutorial_state['voting_results'] = voting_result\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📊 VOTING RESULTS:\")\n",
    "print(f\"Consensus: {voting_result['consensus']}\")\n",
    "print(f\"Average Confidence: {voting_result['average_confidence']:.1f}/10\")\n",
    "print(f\"Vote Breakdown:\")\n",
    "for decision, count in voting_result['vote_breakdown'].items():\n",
    "    print(f\"  {decision}: {count} votes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show individual votes and summary\n",
    "voting_result = tutorial_state['voting_results']\n",
    "voting_system = tutorial_state['voting_system']\n",
    "\n",
    "print(f\"🗳️ INDIVIDUAL VOTES:\")\n",
    "print(\"=\" * 60)\n",
    "for vote in voting_result['votes']:\n",
    "    print(f\"  {vote['vote_id']}: {vote['decision']} (confidence: {vote['confidence']}/10)\")\n",
    "\n",
    "print(f\"\\n📊 VOTING SYSTEM STATISTICS:\")\n",
    "print(f\"   Total votes cast: {voting_system.votes_cast}\")\n",
    "print(f\"   Votes in this session: {len(voting_result['votes'])}\")\n",
    "\n",
    "print(f\"\\n✅ PARALLELIZATION WORKFLOWS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   ✓ Sectioning: Parallel task decomposition for speed\")\n",
    "print(\"   ✓ Voting: Consensus-based decision making for accuracy\")\n",
    "print(\"   ✓ Mathematical foundations: Amdahl's Law & Condorcet's Theorem\")\n",
    "print(f\"\\n💡 All parallel workflows used the SAME LLM instance\")\n",
    "print(f\"   • ParallelProcessor tasks: {tutorial_state['parallel_processor'].tasks_executed}\")\n",
    "print(f\"   • VotingSystem votes: {voting_system.votes_cast}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Summary - Review All Components Built\n",
    "\n",
    "print(\"📊 WORKFLOW PATTERNS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll workflow components built and stored in tutorial_state:\\n\")\n",
    "\n",
    "# 1. Prompt Chaining\n",
    "if 'prompt_chain' in tutorial_state:\n",
    "    chain = tutorial_state['prompt_chain']\n",
    "    print(\"✅ 1. PROMPT CHAINING\")\n",
    "    print(f\"   • Sequential step-by-step processing\")\n",
    "    print(f\"   • Steps executed: {chain.steps_executed}\")\n",
    "    print(f\"   • Uses: memory_llm (consistent LLM instance)\")\n",
    "\n",
    "# 2. Routing System\n",
    "if 'router' in tutorial_state:\n",
    "    router = tutorial_state['router']\n",
    "    print(\"\\n✅ 2. INTELLIGENT ROUTING\")\n",
    "    print(f\"   • Dynamic query classification and routing\")\n",
    "    print(f\"   • Routes registered: {len(router.routes)}\")\n",
    "    print(f\"   • Uses: global llm\")\n",
    "\n",
    "# 3. Parallel Processing\n",
    "if 'parallel_processor' in tutorial_state:\n",
    "    processor = tutorial_state['parallel_processor']\n",
    "    print(\"\\n✅ 3. PARALLEL PROCESSING\")\n",
    "    print(f\"   • Concurrent task execution\")\n",
    "    print(f\"   • Tasks executed: {processor.tasks_executed}\")\n",
    "    print(f\"   • Uses: memory_llm (shared across threads)\")\n",
    "\n",
    "# 4. Voting System\n",
    "if 'voting_system' in tutorial_state:\n",
    "    voting = tutorial_state['voting_system']\n",
    "    print(\"\\n✅ 4. CONSENSUS VOTING\")\n",
    "    print(f\"   • Parallel voting for consensus decisions\")\n",
    "    print(f\"   • Votes cast: {voting.votes_cast}\")\n",
    "    print(f\"   • Uses: memory_llm (same as parallel processor)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"💡 KEY INSIGHT: LLM Instance Reuse\")\n",
    "print(\"=\" * 80)\n",
    "print(\"All workflows share TWO LLM instances:\")\n",
    "print(\"  1. `llm` (temp: 0.3) - Main LLM for general tasks & routing\")\n",
    "print(\"  2. `memory_llm` (temp: 0.2) - Used for memory, chains, parallel work, voting\")\n",
    "print(\"\\n🎯 Benefits of this architecture:\")\n",
    "print(\"   • Reduced memory footprint\")\n",
    "print(\"   • Faster initialization\")\n",
    "print(\"   • Consistent behavior across workflows\")\n",
    "print(\"   • More efficient resource utilization\")\n",
    "print(\"   • Easier to manage and debug\")\n",
    "print(\"\\n✨ This is how production systems should be built!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflows Section Complete - Check Tutorial State\n",
    "\n",
    "print(\"🎉 WORKFLOWS AND CHAINS SECTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show all workflow components in tutorial_state\n",
    "print(\"\\n📦 Components available in tutorial_state:\")\n",
    "workflow_components = ['prompt_chain', 'router', 'parallel_processor', 'voting_system']\n",
    "for component in workflow_components:\n",
    "    if component in tutorial_state:\n",
    "        print(f\"   ✓ {component}\")\n",
    "    else:\n",
    "        print(f\"   ✗ {component} (not found)\")\n",
    "\n",
    "print(\"\\n🔧 LLM Instances:\")\n",
    "print(f\"   • llm (global): {type(llm).__name__} (temp: 0.3)\")\n",
    "if 'memory_llm' in tutorial_state:\n",
    "    print(f\"   • memory_llm: {type(tutorial_state['memory_llm']).__name__} (temp: 0.2)\")\n",
    "\n",
    "print(\"\\n📊 Usage Statistics:\")\n",
    "if 'prompt_chain' in tutorial_state:\n",
    "    print(f\"   • Prompt chain steps: {tutorial_state['prompt_chain'].steps_executed}\")\n",
    "if 'parallel_processor' in tutorial_state:\n",
    "    print(f\"   • Parallel tasks: {tutorial_state['parallel_processor'].tasks_executed}\")\n",
    "if 'voting_system' in tutorial_state:\n",
    "    print(f\"   • Votes cast: {tutorial_state['voting_system'].votes_cast}\")\n",
    "\n",
    "print(\"\\n💡 Ready to proceed to Advanced Agent Systems and RAG!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc688ff4",
   "metadata": {},
   "source": [
    "#### 4. Advanced Workflow Patterns & Agent Systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa093e3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Agentic Systems - Autonomous Agents and Meta-Workflows\n",
    "\n",
    "import time\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "import uuid\n",
    "\n",
    "class AgentState(Enum):\n",
    "    \"\"\"Agent execution states\"\"\"\n",
    "    IDLE = \"idle\"\n",
    "    PLANNING = \"planning\" \n",
    "    EXECUTING = \"executing\"\n",
    "    EVALUATING = \"evaluating\"\n",
    "    BLOCKED = \"blocked\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AgentMemory:\n",
    "    \"\"\"Agent working memory and context\"\"\"\n",
    "    task_history: List[Dict] = field(default_factory=list)\n",
    "    current_context: Dict = field(default_factory=dict)\n",
    "    learned_patterns: Dict = field(default_factory=dict)\n",
    "    error_log: List[str] = field(default_factory=list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedAgentSystem:\n",
    "    \"\"\"\n",
    "    Autonomous agent system implementing Anthropic's agent patterns\n",
    "    Features: Dynamic planning, error recovery, learning, human-in-the-loop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_iterations: int = 10):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.state = AgentState.IDLE\n",
    "        self.memory = AgentMemory()\n",
    "        self.tools = {}\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def register_tool(self, name: str, function: Callable, description: str):\n",
    "        \"\"\"Register tools for agent use\"\"\"\n",
    "        self.tools[name] = {\n",
    "            \"function\": function,\n",
    "            \"description\": description,\n",
    "            \"usage_count\": 0\n",
    "        }\n",
    "        print(f\"Registered tool: {name}\")\n",
    "    \n",
    "    def create_plan(self, task: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dynamic planning based on task complexity\n",
    "        Implements reasoning and planning capabilities\n",
    "        \"\"\"\n",
    "        self.state = AgentState.PLANNING\n",
    "        \n",
    "        planning_prompt = PromptTemplate(\n",
    "            input_variables=[\"task\", \"available_tools\", \"context\"],\n",
    "            template=\"\"\"You are an autonomous agent creating an execution plan.\n",
    "            \n",
    "            Task: {task}\n",
    "            \n",
    "            Available Tools: {available_tools}\n",
    "            \n",
    "            Current Context: {context}\n",
    "            \n",
    "            Create a detailed plan with steps, tools needed, and success criteria.\n",
    "            Format as JSON:\n",
    "            {{\n",
    "                \"plan_id\": \"unique_id\",\n",
    "                \"steps\": [\n",
    "                    {{\n",
    "                        \"step_id\": \"step_1\",\n",
    "                        \"action\": \"specific action to take\",\n",
    "                        \"tools_needed\": [\"tool1\", \"tool2\"],\n",
    "                        \"success_criteria\": \"how to verify success\",\n",
    "                        \"estimated_time\": \"time estimate\",\n",
    "                        \"dependencies\": [\"previous_step_ids\"]\n",
    "                    }}\n",
    "                ],\n",
    "                \"risks\": [\"potential issues\"],\n",
    "                \"checkpoints\": [\"human approval points\"]\n",
    "            }}\"\"\"\n",
    "        )\n",
    "        \n",
    "        tools_description = \"\\n\".join([\n",
    "            f\"- {name}: {info['description']}\" \n",
    "            for name, info in self.tools.items()\n",
    "        ])\n",
    "        \n",
    "        context = json.dumps(self.memory.current_context, indent=2)\n",
    "        \n",
    "        chain = planning_prompt | self.llm | StrOutputParser()\n",
    "        plan_result = chain.invoke({\n",
    "            \"task\": task,\n",
    "            \"available_tools\": tools_description,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        # Parse plan (simplified JSON extraction)\n",
    "        try:\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', plan_result, re.DOTALL)\n",
    "            if json_match:\n",
    "                plan_data = json.loads(json_match.group())\n",
    "                plan_steps = plan_data.get(\"steps\", [])\n",
    "                \n",
    "                # Add to memory\n",
    "                self.memory.task_history.append({\n",
    "                    \"task\": task,\n",
    "                    \"plan\": plan_data,\n",
    "                    \"created_at\": time.time()\n",
    "                })\n",
    "                \n",
    "                print(f\"Created plan with {len(plan_steps)} steps\")\n",
    "                return plan_steps\n",
    "        except Exception as e:\n",
    "            self.memory.error_log.append(f\"Planning error: {str(e)}\")\n",
    "            # Fallback simple plan\n",
    "            return [{\n",
    "                \"step_id\": \"fallback_1\",\n",
    "                \"action\": f\"Complete task: {task}\",\n",
    "                \"tools_needed\": [],\n",
    "                \"success_criteria\": \"Task completion\"\n",
    "            }]\n",
    "    \n",
    "    def execute_step(self, step: Dict) -> Dict:\n",
    "        \"\"\"Execute individual plan step with error recovery\"\"\"\n",
    "        step_id = step.get(\"step_id\", str(uuid.uuid4()))\n",
    "        print(f\"Executing step: {step_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if tools are needed\n",
    "            tools_needed = step.get(\"tools_needed\", [])\n",
    "            tool_results = {}\n",
    "            \n",
    "            for tool_name in tools_needed:\n",
    "                if tool_name in self.tools:\n",
    "                    print(f\"Using tool: {tool_name}\")\n",
    "                    # Simplified tool execution\n",
    "                    tool_results[tool_name] = f\"Tool {tool_name} executed successfully\"\n",
    "                    self.tools[tool_name][\"usage_count\"] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Tool {tool_name} not available\")\n",
    "            \n",
    "            # Execute main action\n",
    "            execution_prompt = PromptTemplate(\n",
    "                input_variables=[\"action\", \"tool_results\", \"success_criteria\"],\n",
    "                template=\"\"\"Execute this action step by step:\n",
    "                \n",
    "                Action: {action}\n",
    "                \n",
    "                Tool Results: {tool_results}\n",
    "                \n",
    "                Success Criteria: {success_criteria}\n",
    "                \n",
    "                Provide detailed execution results and verify success criteria.\"\"\"\n",
    "            )\n",
    "            \n",
    "            chain = execution_prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\n",
    "                \"action\": step[\"action\"],\n",
    "                \"tool_results\": json.dumps(tool_results, indent=2),\n",
    "                \"success_criteria\": step.get(\"success_criteria\", \"completion\")\n",
    "            })\n",
    "            \n",
    "            # Evaluate success\n",
    "            success = self.evaluate_step_success(step, result)\n",
    "            \n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"success\" if success else \"needs_retry\",\n",
    "                \"result\": result,\n",
    "                \"tool_usage\": tool_results,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step execution failed: {str(e)}\"\n",
    "            self.memory.error_log.append(error_msg)\n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "    \n",
    "    def evaluate_step_success(self, step: Dict, result: str) -> bool:\n",
    "        \"\"\"Evaluate if step was successful based on criteria\"\"\"\n",
    "        success_criteria = step.get(\"success_criteria\", \"\")\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"criteria\", \"result\"],\n",
    "            template=\"\"\"Evaluate if this result meets the success criteria.\n",
    "            \n",
    "            Success Criteria: {criteria}\n",
    "            \n",
    "            Actual Result: {result}\n",
    "            \n",
    "            Respond with just \"SUCCESS\" or \"FAILURE\" followed by brief reasoning.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation = chain.invoke({\n",
    "            \"criteria\": success_criteria,\n",
    "            \"result\": result\n",
    "        })\n",
    "        \n",
    "        return \"SUCCESS\" in evaluation.upper()\n",
    "    \n",
    "    def error_recovery(self, failed_step: Dict, error: str) -> Optional[Dict]:\n",
    "        \"\"\"Implement error recovery strategies\"\"\"\n",
    "        print(f\"Attempting error recovery for: {error}\")\n",
    "        \n",
    "        recovery_prompt = PromptTemplate(\n",
    "            input_variables=[\"failed_step\", \"error\", \"error_history\"],\n",
    "            template=\"\"\"Analyze this error and suggest recovery strategy:\n",
    "            \n",
    "            Failed Step: {failed_step}\n",
    "            \n",
    "            Error: {error}\n",
    "            \n",
    "            Previous Errors: {error_history}\n",
    "            \n",
    "            Suggest a modified approach or alternative strategy.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = recovery_prompt | self.llm | StrOutputParser()\n",
    "        recovery_suggestion = chain.invoke({\n",
    "            \"failed_step\": json.dumps(failed_step, indent=2),\n",
    "            \"error\": error,\n",
    "            \"error_history\": json.dumps(self.memory.error_log[-5:], indent=2)\n",
    "        })\n",
    "        \n",
    "        # Create modified step (simplified)\n",
    "        modified_step = failed_step.copy()\n",
    "        modified_step[\"action\"] = f\"RETRY: {modified_step['action']} (Modified based on: {recovery_suggestion[:100]})\"\n",
    "        \n",
    "        return modified_step\n",
    "    \n",
    "    def human_checkpoint(self, checkpoint_data: Dict) -> bool:\n",
    "        \"\"\"Simulate human-in-the-loop checkpoint\"\"\"\n",
    "        print(f\"🚨 HUMAN CHECKPOINT: {checkpoint_data}\")\n",
    "        print(\"In production, this would pause for human approval\")\n",
    "        \n",
    "        # Simulate human approval (always approve for demo)\n",
    "        approval = True\n",
    "        print(f\"✅ Human approval: {'Granted' if approval else 'Denied'}\")\n",
    "        return approval\n",
    "    \n",
    "    def autonomous_execution(self, task: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Main autonomous agent execution loop\n",
    "        Implements the complete agent pattern with all capabilities\n",
    "        \"\"\"\n",
    "        print(f\"🤖 AUTONOMOUS AGENT STARTING\")\n",
    "        print(f\"Task: {task}\")\n",
    "        \n",
    "        execution_log = {\n",
    "            \"task\": task,\n",
    "            \"start_time\": time.time(),\n",
    "            \"steps_completed\": 0,\n",
    "            \"errors_encountered\": 0,\n",
    "            \"human_interactions\": 0,\n",
    "            \"final_status\": \"in_progress\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: Planning\n",
    "            self.state = AgentState.PLANNING\n",
    "            plan = self.create_plan(task)\n",
    "            \n",
    "            if not plan:\n",
    "                raise Exception(\"Failed to create execution plan\")\n",
    "            \n",
    "            # Phase 2: Execution\n",
    "            self.state = AgentState.EXECUTING\n",
    "            completed_steps = []\n",
    "            \n",
    "            for iteration in range(self.max_iterations):\n",
    "                if not plan:\n",
    "                    break\n",
    "                    \n",
    "                current_step = plan.pop(0)\n",
    "                \n",
    "                # Check for human checkpoint\n",
    "                if \"checkpoint\" in current_step.get(\"action\", \"\").lower():\n",
    "                    if not self.human_checkpoint(current_step):\n",
    "                        self.state = AgentState.BLOCKED\n",
    "                        execution_log[\"final_status\"] = \"blocked_by_human\"\n",
    "                        break\n",
    "                    execution_log[\"human_interactions\"] += 1\n",
    "                \n",
    "                # Execute step\n",
    "                step_result = self.execute_step(current_step)\n",
    "                completed_steps.append(step_result)\n",
    "                execution_log[\"steps_completed\"] += 1\n",
    "                \n",
    "                if step_result[\"status\"] == \"failed\":\n",
    "                    execution_log[\"errors_encountered\"] += 1\n",
    "                    \n",
    "                    # Attempt error recovery\n",
    "                    recovered_step = self.error_recovery(\n",
    "                        current_step, \n",
    "                        step_result.get(\"error\", \"Unknown error\")\n",
    "                    )\n",
    "                    \n",
    "                    if recovered_step:\n",
    "                        plan.insert(0, recovered_step)  # Retry at front\n",
    "                    else:\n",
    "                        print(\"❌ Error recovery failed\")\n",
    "                        break\n",
    "                \n",
    "                elif step_result[\"status\"] == \"needs_retry\":\n",
    "                    plan.insert(0, current_step)  # Retry same step\n",
    "                \n",
    "                # Progress update\n",
    "                print(f\"Progress: {execution_log['steps_completed']} steps completed\")\n",
    "            \n",
    "            # Phase 3: Final evaluation\n",
    "            self.state = AgentState.EVALUATING\n",
    "            final_evaluation = self.evaluate_final_result(task, completed_steps)\n",
    "            \n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"total_duration\": time.time() - execution_log[\"start_time\"],\n",
    "                \"completed_steps\": completed_steps,\n",
    "                \"final_evaluation\": final_evaluation,\n",
    "                \"final_status\": \"completed\" if final_evaluation[\"success\"] else \"failed\"\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.COMPLETED if final_evaluation[\"success\"] else AgentState.FAILED\n",
    "            \n",
    "            print(f\"🎯 AUTONOMOUS EXECUTION {'COMPLETED' if final_evaluation['success'] else 'FAILED'}\")\n",
    "            print(f\"Duration: {execution_log['total_duration']:.2f}s\")\n",
    "            print(f\"Steps: {execution_log['steps_completed']}\")\n",
    "            print(f\"Errors: {execution_log['errors_encountered']}\")\n",
    "            \n",
    "            return execution_log\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"final_status\": \"system_error\",\n",
    "                \"system_error\": str(e)\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.FAILED\n",
    "            print(f\"💥 SYSTEM ERROR: {str(e)}\")\n",
    "            return execution_log\n",
    "    \n",
    "    def evaluate_final_result(self, original_task: str, completed_steps: List[Dict]) -> Dict:\n",
    "        \"\"\"Final evaluation of task completion\"\"\"\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"original_task\", \"steps_summary\"],\n",
    "            template=\"\"\"Evaluate if the original task was successfully completed.\n",
    "            \n",
    "            Original Task: {original_task}\n",
    "            \n",
    "            Completed Steps Summary: {steps_summary}\n",
    "            \n",
    "            Provide evaluation including:\n",
    "            1. Task completion status (SUCCESS/PARTIAL/FAILURE)\n",
    "            2. Quality assessment (1-10)\n",
    "            3. Areas of success\n",
    "            4. Areas for improvement\n",
    "            5. Overall confidence level\"\"\"\n",
    "        )\n",
    "        \n",
    "        steps_summary = \"\\n\".join([\n",
    "            f\"Step {i+1}: {step.get('result', 'No result')[:100]}...\"\n",
    "            for i, step in enumerate(completed_steps)\n",
    "        ])\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation_result = chain.invoke({\n",
    "            \"original_task\": original_task,\n",
    "            \"steps_summary\": steps_summary\n",
    "        })\n",
    "        \n",
    "        # Parse evaluation (simplified)\n",
    "        success = \"SUCCESS\" in evaluation_result.upper()\n",
    "        \n",
    "        return {\n",
    "            \"success\": success,\n",
    "            \"evaluation\": evaluation_result,\n",
    "            \"steps_count\": len(completed_steps),\n",
    "            \"quality_indicators\": {\n",
    "                \"completion_rate\": len([s for s in completed_steps if s.get(\"status\") == \"success\"]) / max(len(completed_steps), 1),\n",
    "                \"error_rate\": len([s for s in completed_steps if s.get(\"status\") == \"failed\"]) / max(len(completed_steps), 1)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstration of Autonomous Agent System\n",
    "class AutonomousAgentDemo:\n",
    "    \"\"\"Comprehensive demonstration of autonomous agent capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.agent = AdvancedAgentSystem(llm)\n",
    "        self.setup_demo_tools()\n",
    "    \n",
    "    def setup_demo_tools(self):\n",
    "        \"\"\"Register demonstration tools\"\"\"\n",
    "        \n",
    "        def web_search(query: str) -> str:\n",
    "            return f\"Search results for '{query}': [Simulated web search results]\"\n",
    "        \n",
    "        def file_manager(action: str, filename: str = \"\", content: str = \"\") -> str:\n",
    "            return f\"File operation '{action}' on '{filename}': Success\"\n",
    "        \n",
    "        def api_call(endpoint: str, data: Dict = None) -> str:\n",
    "            return f\"API call to '{endpoint}': Success (simulated)\"\n",
    "        \n",
    "        def data_analysis(dataset: str, analysis_type: str = \"summary\") -> str:\n",
    "            return f\"Analysis '{analysis_type}' on '{dataset}': Completed with insights\"\n",
    "        \n",
    "        # Register tools\n",
    "        self.agent.register_tool(\"web_search\", web_search, \"Search the web for information\")\n",
    "        self.agent.register_tool(\"file_manager\", file_manager, \"Create, read, update, delete files\")\n",
    "        self.agent.register_tool(\"api_call\", api_call, \"Make API calls to external services\")\n",
    "        self.agent.register_tool(\"data_analysis\", data_analysis, \"Analyze datasets and generate insights\")\n",
    "    \n",
    "    def demo_complex_research_task(self):\n",
    "        \"\"\"Demonstrate agent handling complex multi-step research task\"\"\"\n",
    "        print(\"🔬 AUTONOMOUS RESEARCH AGENT DEMONSTRATION\")\n",
    "        \n",
    "        complex_task = \"\"\"\n",
    "        Research the current state of quantum computing and create a comprehensive report including:\n",
    "        1. Recent breakthrough discoveries in quantum computing\n",
    "        2. Major companies and their quantum computing initiatives\n",
    "        3. Current limitations and challenges\n",
    "        4. Potential future applications\n",
    "        5. Timeline predictions for quantum supremacy achievements\n",
    "        \n",
    "        The report should be well-structured, factual, and include citations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(complex_task)\n",
    "        return result\n",
    "    \n",
    "    def demo_software_development_task(self):\n",
    "        \"\"\"Demonstrate agent handling software development workflow\"\"\"\n",
    "        print(\"💻 AUTONOMOUS DEVELOPMENT AGENT DEMONSTRATION\")\n",
    "        \n",
    "        dev_task = \"\"\"\n",
    "        Create a complete web application for a personal task management system including:\n",
    "        1. Backend API with user authentication\n",
    "        2. Database schema for tasks and users\n",
    "        3. Frontend interface with CRUD operations\n",
    "        4. Unit tests for core functionality\n",
    "        5. Deployment configuration\n",
    "        6. Documentation and README\n",
    "        \n",
    "        Use modern best practices and ensure security considerations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(dev_task)\n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_demo(self):\n",
    "        \"\"\"Run comprehensive autonomous agent demonstrations\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT SYSTEM DEMONSTRATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Demo 1: Research Task\n",
    "        results['research'] = self.demo_complex_research_task()\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        \n",
    "        # Demo 2: Development Task  \n",
    "        results['development'] = self.demo_software_development_task()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT DEMONSTRATIONS COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76645053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and demonstrate autonomous agents\n",
    "if 'autonomous_agent' not in tutorial_state:\n",
    "    agent_demo = AutonomousAgentDemo(memory_llm)\n",
    "    tutorial_state['autonomous_agent'] = agent_demo\n",
    "    \n",
    "    print(\"🚀 STARTING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = agent_demo.run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results\n",
    "else:\n",
    "    print(\"🔄 RUNNING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = tutorial_state['autonomous_agent'].run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143533c6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd518b",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b24f75",
   "metadata": {},
   "source": [
    "Now that we've mastered building intelligent agents and workflows, it's time to tackle one of the most important challenges in modern AI systems: how do we give our agents access to vast, specific, and up-to-date knowledge that wasn't included in their training data?\n",
    "\n",
    "This is where Retrieval-Augmented Generation (RAG) becomes essential. RAG is the bridge between the incredible reasoning capabilities of large language models and the specific, detailed knowledge that your applications need to be truly useful in real-world scenarios.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*WYv0_CaBmCTt7FXc\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e3e6c",
   "metadata": {},
   "source": [
    "### Why RAG Is Essential: The Knowledge Gap Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68357078",
   "metadata": {},
   "source": [
    "LlamaIndex is highly specialized for data ingestion and retrieval, while LangChain is better suited for building complex, multi-step AI workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e18ff2",
   "metadata": {},
   "source": [
    "Let me paint a picture of why RAG matters. Imagine you've built a brilliant customer service agent using the workflows we just learned. It can route questions, use tools, and maintain conversation context perfectly. But then a customer asks about your company's specific return policy that was updated last week, or wants details about a product that was launched after the model's training cutoff.\n",
    "\n",
    "**The Fundamental Limitations of LLMs:**\n",
    "\n",
    "Even the most advanced language models face critical limitations when used alone:\n",
    "\n",
    "1. **Knowledge Cutoff**: Training data has a specific cutoff date, making models ignorant of recent information\n",
    "2. **Domain Specificity**: Models lack deep knowledge about your specific business, products, or internal processes  \n",
    "3. **Context Window Limits**: Even with large context windows, you can't fit entire knowledge bases into a single conversation\n",
    "4. **Hallucination Risk**: When models don't know something, they often generate plausible-sounding but incorrect information\n",
    "5. **Static Knowledge**: The information encoded during training can't be updated without retraining\n",
    "\n",
    "**Where Our Agent Workflows Hit the Wall:** The sophisticated agent workflows we've built are incredibly powerful for reasoning and decision-making, but they're only as good as the knowledge they have access to. Without RAG:\n",
    "\n",
    "- Your routing system might correctly identify that a question is about \"product specifications,\" but have no way to retrieve the actual, current specifications\n",
    "- Your memory system can remember what users have discussed, but can't recall relevant company knowledge or documentation\n",
    "- Your tools can calculate and process data, but can't access your proprietary knowledge base or recent updates\n",
    "\n",
    "**RAG as the Solution:** Retrieval-Augmented Generation solves these problems by creating a dynamic bridge between your agents and external knowledge sources. Instead of relying solely on the model's trained knowledge, RAG systems:\n",
    "\n",
    "- **Retrieve** relevant information from external knowledge bases in real-time\n",
    "- **Augment** the model's prompt with this retrieved context  \n",
    "- **Generate** responses that combine the model's reasoning abilities with specific, current, and accurate information\n",
    "\n",
    "This creates agents that maintain their sophisticated reasoning capabilities while having access to vast, specific, and up-to-date knowledge that makes them truly useful for real-world applications.\n",
    "\n",
    "**The Most Popular RAG Approaches Right Now:**\n",
    "\n",
    "**1. GraphRAG** \n",
    "- Microsoft's breakthrough approach that creates knowledge graphs from documents\n",
    "- Builds hierarchical community summaries for better context understanding\n",
    "- Excels at answering complex, multi-hop questions that span multiple documents\n",
    "- Game-changer for enterprise knowledge bases and research applications\n",
    "\n",
    "**2. Agentic RAG**\n",
    "- Combines RAG with autonomous agents that can reason about when and what to retrieve\n",
    "- Uses sophisticated routing to decide between different knowledge sources\n",
    "- Self-correcting retrieval based on generated content quality\n",
    "- Perfect for complex workflows requiring dynamic knowledge access\n",
    "\n",
    "**3. Multi-Modal RAG**\n",
    "- Retrieves and processes images, tables, charts alongside text\n",
    "- Essential for technical documentation, financial reports, and visual content\n",
    "- Uses vision-language models for comprehensive document understanding\n",
    "\n",
    "**4. Conversational RAG** \n",
    "- Maintains conversation history and context across multiple turns\n",
    "- Intelligently decides when to retrieve new information vs. use conversation memory\n",
    "- Critical for chatbots and customer service applications\n",
    "\n",
    "**5. Self-RAG**\n",
    "- Models self-evaluate when retrieval is needed and assess information quality\n",
    "- Reduces hallucination by checking factual consistency\n",
    "- Adaptive retrieval triggers based on confidence and uncertainty\n",
    "\n",
    "**6. Hybrid Dense-Sparse RAG**\n",
    "- Combines traditional keyword search (BM25) with semantic embeddings\n",
    "- Best of both worlds: exact matches + semantic similarity\n",
    "- Most production systems use this approach for robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc169c2",
   "metadata": {},
   "source": [
    "### Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cb659",
   "metadata": {},
   "source": [
    "Document preprocessing is the foundation of any effective RAG system. Without proper structured, labeled data on database, no model can perform good. A good data preprocessing is crucial espcially in large scale production systems where we deal with millions of documents in real time, any small mistake or bug can lead to catastrophic failures. It's important to choose the right preprocessing techniques given requirements and to align well with business goal. \n",
    "\n",
    "**The Challenge:** Raw documents come in countless formats, structures, and sizes. A PDF might contain tables, images, and multi-column layouts. A web page includes navigation menus, advertisements, and dynamic content. A code repository has different file types with distinct syntaxes. Without proper preprocessing, even the most sophisticated retrieval system will struggle to find and present relevant information effectively.\n",
    "\n",
    "Document preprocessing involves several transformations that can be expressed mathematically:\n",
    "\n",
    "- **Information Density**: $\\rho = \\frac{\\text{Relevant Content}}{\\text{Total Content}}$ - maximizing signal-to-noise ratio\n",
    "- **Semantic Coherence**: $C(chunk) = \\frac{\\sum_{i,j} similarity(sent_i, sent_j)}{n(n-1)/2}$ - ensuring chunks maintain internal consistency  \n",
    "- **Optimal Chunk Size**: $size_{optimal} = \\arg\\max_{s} (retrieval\\_accuracy(s) - processing\\_cost(s))$\n",
    "\n",
    "<img src=\"https://chamomile.ai/reliable-rag-with-data-preprocessing/image6.png\" width=700>\n",
    "\n",
    "**The Preprocessing Pipeline:** Our approach follows a systematic four-stage pipeline:\n",
    "\n",
    "1. **Document Loading**: Extract content from various formats while preserving semantic structure\n",
    "2. **Splitting**: Break documents into manageable sections based on natural boundaries\n",
    "3. **Chunking**: Create optimally-sized pieces that balance context and specificity  \n",
    "4. **Embedding**: Transform text into vector representations for semantic search\n",
    "\n",
    "Each stage has multiple strategies optimized for different document types and use cases. Let's explore each in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d7a17",
   "metadata": {},
   "source": [
    "#### Document Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e08f9",
   "metadata": {},
   "source": [
    "Document loading is the critical first step in building effective RAG systems. Different document types require specialized loaders optimized for their unique structures and challenges. Let's explore the ecosystem of document loaders available in LangChain and understand when to use each one.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776c9a1",
   "metadata": {},
   "source": [
    "##### Web Content Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c7fc3",
   "metadata": {},
   "source": [
    "Web content presents unique challenges: dynamic JavaScript rendering, complex layouts, advertisements, navigation elements, and varying HTML structures. Choosing the right web loader depends on your specific requirements around speed, accuracy, and the complexity of target websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba584f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Loader** | **Best For** | **Key Features** | **Considerations** | **Type** |\n",
    "|------------|--------------|------------------|-------------------|----------|\n",
    "| [Web](https://python.langchain.com/docs/integrations/document_loaders/web_base) | Simple static pages | • Uses urllib + BeautifulSoup<br>• Fast and lightweight<br>• No external dependencies | • Struggles with JavaScript-heavy sites<br>• Basic HTML parsing only<br>• No dynamic content handling | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Complex layouts | • Advanced structure detection<br>• Preserves semantic hierarchy<br>• Handles tables and formatting | • Slower processing<br>• Heavier dependencies<br>• May need additional setup | Package |\n",
    "| [RecursiveURL](https://python.langchain.com/docs/integrations/document_loaders/recursive_url) | Documentation sites | • Automatically discovers child links<br>• Configurable depth control<br>• Maintains site structure | • Can retrieve too much data<br>• Requires careful depth limits<br>• May hit rate limits | Package |\n",
    "| [Sitemap](https://python.langchain.com/docs/integrations/document_loaders/sitemap) | Entire websites | • Uses sitemap.xml for discovery<br>• Efficient site crawling<br>• Respects site structure | • Requires valid sitemap<br>• May miss pages not in sitemap<br>• Large sites = long processing | Package |\n",
    "| [Spider](https://python.langchain.com/docs/integrations/document_loaders/spider) | Production crawling | • LLM-optimized output format<br>• Handles JavaScript rendering<br>• Anti-bot bypass capabilities | • Requires API key<br>• Usage-based pricing<br>• External service dependency | API |\n",
    "| [Firecrawl](https://python.langchain.com/docs/integrations/document_loaders/firecrawl) | Enterprise scraping | • Self-hostable option<br>• JavaScript execution<br>• Advanced content extraction | • Complex setup if self-hosted<br>• API costs if cloud-hosted<br>• Requires infrastructure | API |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Document-heavy sites | • Specialized for document extraction<br>• Format preservation<br>• Multi-format support | • Focused on document-centric sites<br>• May be overkill for simple pages<br>• Learning curve | Package |\n",
    "| [Hyperbrowser](https://python.langchain.com/docs/integrations/document_loaders/hyperbrowser) | Complex web apps | • Full browser automation<br>• JavaScript execution<br>• Session management | • Higher latency<br>• Resource intensive<br>• API-based pricing | API |\n",
    "| [AgentQL](https://python.langchain.com/docs/integrations/document_loaders/agentql) | Structured extraction | • Natural language queries<br>• Precise data targeting<br>• Schema-based extraction | • Best for specific data points<br>• Requires query design<br>• API costs | API |\n",
    "| [Oxylabs](https://python.langchain.com/docs/integrations/document_loaders/oxylabs) | Large-scale scraping | • Enterprise-grade infrastructure<br>• Geographic proxy support<br>• High success rates | • Premium pricing<br>• Overkill for small projects<br>• External dependency | API |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c74ea3",
   "metadata": {},
   "source": [
    "There's PDF content loaders as well \n",
    "\n",
    "| **Document Loader** | **Description** | **Package/API** |\n",
    "| --- | --- | --- |\n",
    "| [PyPDF](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader) | Uses `pypdf` to load and parse PDFs | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Uses Unstructured's open source library to load PDFs | Package |\n",
    "| [Amazon Textract](https://python.langchain.com/docs/integrations/document_loaders/amazon_textract) | Uses AWS API to load PDFs | API |\n",
    "| [MathPix](https://python.langchain.com/docs/integrations/document_loaders/mathpix) | Uses MathPix to load PDFs | Package |\n",
    "| [PDFPlumber](https://python.langchain.com/docs/integrations/document_loaders/pdfplumber) | Load PDF files using PDFPlumber | Package |\n",
    "| [PyPDFDirectry](https://python.langchain.com/docs/integrations/document_loaders/pypdfdirectory) | Load a directory with PDF files | Package |\n",
    "| [PyPDFium2](https://python.langchain.com/docs/integrations/document_loaders/pypdfium2) | Load PDF files using PyPDFium2 | Package |\n",
    "| [PyMuPDF](https://python.langchain.com/docs/integrations/document_loaders/pymupdf) | Load PDF files using PyMuPDF | Package |\n",
    "| [PyMuPDF4LLM](https://python.langchain.com/docs/integrations/document_loaders/pymupdf4llm) | Load PDF content to Markdown using PyMuPDF4LLM | Package |\n",
    "| [PDFMiner](https://python.langchain.com/docs/integrations/document_loaders/pdfminer) | Load PDF files using PDFMiner | Package |\n",
    "| [Upstage Document Parse Loader](https://python.langchain.com/docs/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader | Package |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Load PDF files using Docling | Package |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45978598",
   "metadata": {},
   "source": [
    "There's also a way you can build multimodal rag but basically what that means is we convert image to text and treat that document as normal vectorized doc\n",
    "\n",
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/rag-preprocessing-for-images.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cec56",
   "metadata": {},
   "source": [
    "For now let's just get a document loading function ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296718b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal Document Loading System for teaching\n",
    "class DocumentLoadingSystem:\n",
    "    \"\"\"Simple, synchronous document loader used for examples.\n",
    "    Keeps a tiny in-memory collection of documents (content + metadata).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "\n",
    "    def load_text(self, text_path: str) -> list:\n",
    "        \"\"\"Load a plain text file and return a simple document dict.\n",
    "        In this tutorial we keep it synchronous and minimal.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            doc = {\"content\": content, \"metadata\": {\"file_path\": text_path}}\n",
    "            self.documents.append(doc)\n",
    "            print(f\"✅ Loaded: {text_path}\")\n",
    "            return [doc]\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not load {text_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_sample_documents(self) -> list:\n",
    "        \"\"\"Return a short list of small sample documents for demos.\"\"\"\n",
    "        samples = [\n",
    "            {\"content\": \"AI and ML: overview of key concepts and applications.\", \"metadata\": {\"title\": \"AI Guide\"}},\n",
    "            {\"content\": \"RAG: combine retrieval and generation to ground LLM outputs.\", \"metadata\": {\"title\": \"RAG Notes\"}},\n",
    "        ]\n",
    "        self.documents.extend(samples)\n",
    "        print(f\"✅ Created {len(samples)} sample documents\")\n",
    "        return samples\n",
    "\n",
    "# initialize and register\n",
    "doc_loader = DocumentLoadingSystem()\n",
    "tutorial_state['doc_loader'] = doc_loader\n",
    "print('📚 Minimal DocumentLoadingSystem ready (use doc_loader.create_sample_documents()).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2383753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Document Loading Capabilities\n",
    "\n",
    "print(\"🧪 TESTING DOCUMENT LOADING CAPABILITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Load sample documents (for demonstration)\n",
    "print(\"\\n📝 Test 1: Loading Sample Documents\")\n",
    "sample_docs = doc_loader.create_sample_documents()\n",
    "\n",
    "# Display sample document info\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    print(f\"\\n   Document {i+1}:\")\n",
    "    print(f\"   Title: {doc['metadata'].get('title', 'No title')}\")\n",
    "    print(f\"   Type: {doc['source_type']}\")\n",
    "    print(f\"   Content length: {len(doc['content'])} characters\")\n",
    "    print(f\"   Preview: {doc['content'][:100]}...\")\n",
    "\n",
    "# Test 2: Web content loading (simulated with sample URLs)\n",
    "print(f\"\\n🌐 Test 2: Web Content Loading Demo\")\n",
    "print(\"   Note: Using sample content to demonstrate web loading capabilities\")\n",
    "\n",
    "# Simulate web loading\n",
    "web_urls = [\n",
    "    \"https://docs.python.org/3/tutorial/\",\n",
    "    \"https://langchain.readthedocs.io/\"\n",
    "]\n",
    "\n",
    "print(f\"   Simulated loading from: {web_urls[0]}\")\n",
    "print(\"   In production, this would fetch live content from these URLs\")\n",
    "\n",
    "# Test 3: Document metadata analysis\n",
    "print(f\"\\n📊 Test 3: Document Metadata Analysis\")\n",
    "\n",
    "metadata_summary = {}\n",
    "for doc in sample_docs:\n",
    "    doc_type = doc['source_type']\n",
    "    if doc_type not in metadata_summary:\n",
    "        metadata_summary[doc_type] = {\n",
    "            'count': 0,\n",
    "            'total_chars': 0,\n",
    "            'loaders_used': set()\n",
    "        }\n",
    "    \n",
    "    metadata_summary[doc_type]['count'] += 1\n",
    "    metadata_summary[doc_type]['total_chars'] += len(doc['content'])\n",
    "    metadata_summary[doc_type]['loaders_used'].add(doc['loader_used'])\n",
    "\n",
    "print(\"   Document Type Summary:\")\n",
    "for doc_type, stats in metadata_summary.items():\n",
    "    print(f\"     {doc_type}:\")\n",
    "    print(f\"       Count: {stats['count']}\")\n",
    "    print(f\"       Total characters: {stats['total_chars']}\")\n",
    "    print(f\"       Loaders used: {', '.join(stats['loaders_used'])}\")\n",
    "\n",
    "# Store results\n",
    "tutorial_state['loaded_documents']['samples'] = sample_docs\n",
    "tutorial_state['loading_metadata'] = metadata_summary\n",
    "\n",
    "print(f\"\\n✅ DOCUMENT LOADING TESTS COMPLETE\")\n",
    "print(f\"   Loaded {len(sample_docs)} documents successfully\")\n",
    "print(\"   Ready to proceed to document splitting and chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbf6bd",
   "metadata": {},
   "source": [
    "#### Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d596d",
   "metadata": {},
   "source": [
    "Document splitting represents a critical juncture in the RAG pipeline where raw content transforms into intelligently segmented knowledge units. Unlike the straightforward task of simply loading documents, splitting requires sophisticated understanding of document structure, semantic boundaries, and downstream processing requirements. The challenge lies in preserving meaningful context while creating chunks that are optimally sized for both embedding generation and retrieval accuracy.\n",
    "\n",
    "Consider the complexity: a technical whitepaper might contain dense theoretical sections that benefit from larger chunks to maintain conceptual coherence, while a customer service manual with step-by-step procedures might work better with smaller, action-oriented segments. Meanwhile, a legal document requires preservation of precise clause boundaries, and a research paper needs to maintain the relationship between hypotheses and supporting evidence. Each document type demands a nuanced approach to splitting that balances computational constraints with semantic integrity.\n",
    "\n",
    "**The Multi-Dimensional Challenge:**\n",
    "\n",
    "Document splitting isn't just about managing size—it's about optimizing the fundamental trade-offs that determine RAG system performance. **Chunk size** directly impacts both embedding quality and retrieval precision: smaller chunks provide granular relevance but may lack sufficient context, while larger chunks offer comprehensive context but may dilute the signal for specific queries. **Semantic boundaries** determine whether related concepts stay together or get artificially separated, affecting the model's ability to understand relationships and dependencies. **Overlap strategies** influence how much redundancy exists between chunks, which can improve retrieval recall but increase storage costs and processing time.\n",
    "\n",
    "**The Computational Reality:**\n",
    "\n",
    "Modern embedding models typically handle 512-8192 tokens efficiently, but optimal chunk size varies significantly based on your specific use case. Dense retrieval systems often perform better with 200-500 token chunks for precision, while semantic search applications may benefit from 500-1000 token chunks for context. The mathematical relationship isn't linear—doubling chunk size doesn't simply halve precision or double context value. Instead, there's often a sweet spot that maximizes the information density while maintaining retrievability.\n",
    "\n",
    "**Strategic Splitting Approaches:**\n",
    "\n",
    "The most effective splitting strategies employ hierarchical approaches that operate at multiple levels simultaneously. **Structural splitting** respects document organization by breaking at natural boundaries like chapters, sections, and paragraphs, preserving the author's intended information architecture. **Semantic splitting** uses NLP techniques to identify topic boundaries and conceptual transitions, ensuring that related ideas remain grouped together. **Sliding window approaches** create overlapping chunks that capture relationships spanning traditional boundaries, particularly valuable for documents where context frequently spans multiple sections.\n",
    "\n",
    "The choice of splitting strategy often determines whether your RAG system delivers frustratingly fragmented responses or remarkably coherent, context-aware answers that feel like they were written by a domain expert who has read and understood your entire knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89400",
   "metadata": {},
   "source": [
    "##### Text Splitting Strategies in LangChain\n",
    "\n",
    "LangChain provides a rich ecosystem of text splitters, each designed for specific document types and use cases. Let's explore the most popular and effective ones:\n",
    "\n",
    "**Core Text Splitters:**\n",
    "\n",
    "| **Splitter** | **Best For** | **How It Works** | **Key Parameters** | **Use Cases** |\n",
    "|--------------|--------------|------------------|-------------------|---------------|\n",
    "| **CharacterTextSplitter** | General text, simple splitting | Splits on a single character (default `\\n\\n`) | `chunk_size`, `chunk_overlap`, `separator` | Blog posts, articles, simple documents |\n",
    "| **RecursiveCharacterTextSplitter** | Most documents (default choice) | Tries separators in order: `\\n\\n`, `\\n`, ` `, `\"\"` | `chunk_size`, `chunk_overlap`, `separators` | General purpose, mixed content |\n",
    "| **TokenTextSplitter** | Token-aware splitting | Splits based on token count (uses tiktoken) | `chunk_size`, `chunk_overlap`, `encoding_name` | When exact token limits matter (GPT models) |\n",
    "| **SentenceTransformersTokenTextSplitter** | Sentence transformer models | Optimized for sentence-transformers library | `chunk_size`, `chunk_overlap`, `model_name` | When using sentence-transformers embeddings |\n",
    "| **MarkdownHeaderTextSplitter** | Markdown documents | Splits by headers, preserves hierarchy | `headers_to_split_on` | Documentation, README files, technical docs |\n",
    "| **HTMLHeaderTextSplitter** | HTML content | Splits by HTML headers (`<h1>`, `<h2>`, etc.) | `headers_to_split_on` | Web pages, HTML documentation |\n",
    "| **CodeTextSplitter** | Source code | Language-aware splitting (Python, JS, etc.) | `language`, `chunk_size`, `chunk_overlap` | Code repositories, programming tutorials |\n",
    "| **LatexTextSplitter** | LaTeX documents | Preserves LaTeX structure and math | `chunk_size`, `chunk_overlap` | Academic papers, mathematical content |\n",
    "| **NLTKTextSplitter** | Sentence-based splitting | Uses NLTK for proper sentence boundary detection | `chunk_size`, `chunk_overlap` | Natural language text requiring sentence integrity |\n",
    "| **SpacyTextSplitter** | Advanced NLP splitting | Uses spaCy for linguistic-aware splitting | `chunk_size`, `chunk_overlap`, `pipeline` | Complex text requiring NLP processing |\n",
    "\n",
    "**Mathematical Foundations:**\n",
    "\n",
    "The core splitting logic can be expressed as:\n",
    "\n",
    "**1. Basic Chunking:**\n",
    "$$C = \\\\{c_1, c_2, ..., c_n\\\\} \\\\text{ where } |c_i| \\\\leq chunk\\\\_size$$\n",
    "\n",
    "**2. Overlap Strategy:**\n",
    "$$c_i \\\\cap c_{i+1} = overlap \\\\text{ tokens}$$\n",
    "\n",
    "**3. Information Retention:**\n",
    "$$R(chunks) = \\\\frac{\\\\text{Preserved Semantic Units}}{\\\\text{Total Semantic Units}}$$\n",
    "\n",
    "**4. Optimal Chunk Size** (empirical finding):\n",
    "$$size_{optimal} = \\\\arg\\\\max_{s} \\\\left(\\\\frac{precision(s) + recall(s)}{2} - \\\\lambda \\\\cdot cost(s)\\\\right)$$\n",
    "\n",
    "Where:\n",
    "- **precision(s)**: How relevant retrieved chunks are at size s\n",
    "- **recall(s)**: How many relevant chunks are found at size s  \n",
    "- **cost(s)**: Computational cost (embedding + storage)\n",
    "- **λ**: Cost weight factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal text splitter imports and small corpus for demonstrations\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Compact sample texts used by splitter demos\n",
    "sample_texts = {\n",
    "    'article': (\n",
    "        \"Artificial Intelligence (AI) enables machines to learn from data. \"\n",
    "        \"RAG systems combine retrieval with generation to produce grounded answers. \"\n",
    "        \"This short article is for splitter demonstrations.\"\n",
    "    ),\n",
    "    'code': (\"def add(a, b):\\n    return a + b\\n\\nprint(add(1,2))\"),\n",
    "    'markdown': (\"# Title\\n\\n## Section\\n\\nContent under section.\"),\n",
    "    'html': (\"<h1>Title</h1><p>Paragraph about RAG.</p>\"),\n",
    "}\n",
    "\n",
    "print('Sample texts and splitter imports ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e786c",
   "metadata": {},
   "source": [
    "##### 1. CharacterTextSplitter - Basic Splitting\n",
    "\n",
    "The simplest splitter that splits on a single character separator. Best for straightforward text where you know the natural boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14843768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact demonstration of CharacterTextSplitter\n",
    "\n",
    "def demonstrate_character_splitter():\n",
    "    splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=200, chunk_overlap=20)\n",
    "    chunks = splitter.split_text(sample_texts['article'])\n",
    "    print(f\"Character splitter produced {len(chunks)} chunk(s).\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"--- Chunk {i} ---\\n{chunk[:200]}\\n\")\n",
    "    return chunks\n",
    "\n",
    "character_chunks = demonstrate_character_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4bc33",
   "metadata": {},
   "source": [
    "##### 2. RecursiveCharacterTextSplitter - Adaptive Splitting (Most Popular!)\n",
    "\n",
    "The **default choice** for most applications. It recursively tries different separators in order until chunks fit the desired size. This is the most versatile and commonly used splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact demonstration of RecursiveCharacterTextSplitter\n",
    "\n",
    "def demonstrate_recursive_splitter():\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "    chunks = splitter.split_text(sample_texts['article'])\n",
    "    print(f\"Recursive splitter produced {len(chunks)} chunk(s). Avg size: {sum(len(c) for c in chunks)/len(chunks):.1f} chars\")\n",
    "    return chunks\n",
    "\n",
    "recursive_chunks = demonstrate_recursive_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4aa70",
   "metadata": {},
   "source": [
    "##### 3. TokenTextSplitter - Token-Aware Splitting\n",
    "\n",
    "Splits based on **token count** rather than characters. Essential when working with models that have strict token limits (like GPT-3.5/4 with 8K/16K/32K context windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d48c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_token_splitter():\n",
    "    \"\"\"\n",
    "    TokenTextSplitter: Splits based on token count, not characters\n",
    "    \n",
    "    Why tokens matter:\n",
    "    - Different from characters (1 token ≈ 4 characters in English)\n",
    "    - Models have token limits (GPT-3.5: 4K, GPT-4: 8K/32K)\n",
    "    - Token counting varies by model/encoding\n",
    "    \n",
    "    Use Cases:\n",
    "    - When working with LLMs with strict token limits\n",
    "    - Need precise control over context window usage\n",
    "    - Billing based on tokens\n",
    "    - Ensuring chunks fit model context\n",
    "    \n",
    "    Token vs Character:\n",
    "    - \"Hello\" = 1 token, 5 characters\n",
    "    - \"Hello world!\" = 3 tokens, 12 characters\n",
    "    - Token count < character count (usually 1 token ≈ 4 chars)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3. TOKEN TEXT SPLITTER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Using GPT-3.5-turbo encoding\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=100,          # Maximum tokens per chunk\n",
    "        chunk_overlap=10,        # Overlap in tokens\n",
    "        encoding_name=\"cl100k_base\"  # GPT-3.5/4 encoding\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(sample_texts['article'])\n",
    "    \n",
    "    # Calculate actual token counts\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_counts = [len(encoding.encode(chunk)) for chunk in chunks]\n",
    "    \n",
    "    print(f\"\\n📊 Token-based splitting results:\")\n",
    "    print(f\"   Original text: {len(sample_texts['article'])} characters\")\n",
    "    print(f\"   Original tokens: {len(encoding.encode(sample_texts['article']))}\")\n",
    "    print(f\"   Number of chunks: {len(chunks)}\")\n",
    "    print(f\"   Token counts per chunk: {token_counts}\")\n",
    "    print(f\"   Average tokens per chunk: {sum(token_counts) / len(token_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\n📝 First chunk:\")\n",
    "    print(chunks[0])\n",
    "    print(f\"   → Tokens: {token_counts[0]}, Characters: {len(chunks[0])}\")\n",
    "    \n",
    "    # Compare different encodings\n",
    "    print(f\"\\n\\n🔬 Different encodings (same chunk size = 100 tokens):\")\n",
    "    encodings_to_test = [\n",
    "        (\"cl100k_base\", \"GPT-3.5/4\"),\n",
    "        (\"p50k_base\", \"GPT-3/Codex\"),\n",
    "        (\"r50k_base\", \"GPT-2\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Encoding':>15} | {'Model':>15} | {'# Chunks':>10} | {'Total Tokens':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for enc_name, model_name in encodings_to_test:\n",
    "        test_splitter = TokenTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=10,\n",
    "            encoding_name=enc_name\n",
    "        )\n",
    "        test_chunks = test_splitter.split_text(sample_texts['article'])\n",
    "        enc = tiktoken.get_encoding(enc_name)\n",
    "        total_tokens = sum(len(enc.encode(chunk)) for chunk in test_chunks)\n",
    "        print(f\"{enc_name:>15} | {model_name:>15} | {len(test_chunks):>10} | {total_tokens:>12}\")\n",
    "    \n",
    "    # Demonstrate chunk size impact\n",
    "    print(f\"\\n\\n📏 Impact of chunk size on token splitting:\")\n",
    "    print(f\"{'Chunk Size':>12} | {'Overlap':>8} | {'# Chunks':>10} | {'Max Tokens':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for chunk_size in [50, 100, 200, 500]:\n",
    "        test_splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=10,\n",
    "            encoding_name=\"cl100k_base\"\n",
    "        )\n",
    "        test_chunks = test_splitter.split_text(sample_texts['article'])\n",
    "        max_tokens = max(len(encoding.encode(chunk)) for chunk in test_chunks)\n",
    "        print(f\"{chunk_size:12d} | {10:8d} | {len(test_chunks):>10} | {max_tokens:>12}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "token_chunks = demonstrate_token_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837817a",
   "metadata": {},
   "source": [
    "##### 4. MarkdownHeaderTextSplitter - Structure-Aware Markdown Splitting\n",
    "\n",
    "Splits Markdown documents based on headers, preserving the document hierarchy and structure. Perfect for documentation, README files, and technical content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact demonstration of MarkdownHeaderTextSplitter\n",
    "\n",
    "def demonstrate_markdown_splitter():\n",
    "    splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=200, chunk_overlap=20)\n",
    "    chunks = splitter.split_text(sample_texts['markdown'])\n",
    "    print(f\"Markdown splitter (simplified) produced {len(chunks)} chunk(s).\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        print(f\"--- Section {i} ---\\n{c}\\n\")\n",
    "    return chunks\n",
    "\n",
    "markdown_chunks = demonstrate_markdown_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882b133",
   "metadata": {},
   "source": [
    "##### 5. HTMLHeaderTextSplitter - HTML Structure Parsing\n",
    "\n",
    "Similar to Markdown splitter but for HTML documents. Preserves HTML header hierarchy and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8df071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_html_splitter():\n",
    "    \"\"\"\n",
    "    HTMLHeaderTextSplitter: Preserves HTML document structure\n",
    "    \n",
    "    How it works:\n",
    "    - Splits on HTML header tags (h1, h2, h3, etc.)\n",
    "    - Extracts text while preserving hierarchy\n",
    "    - Metadata includes header hierarchy\n",
    "    \n",
    "    Use Cases:\n",
    "    - Web scraped content\n",
    "    - HTML documentation\n",
    "    - Blog posts\n",
    "    - Web articles\n",
    "    \n",
    "    Advantages:\n",
    "    - Understands HTML structure\n",
    "    - Removes HTML tags but preserves organization\n",
    "    - Maintains semantic sections\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"5. HTML HEADER TEXT SPLITTER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define HTML headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    splitter = HTMLHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    \n",
    "    # Split HTML document\n",
    "    chunks = splitter.split_text(sample_texts['html'])\n",
    "    \n",
    "    print(f\"\\n📊 HTML splitting results:\")\n",
    "    print(f\"   Original HTML length: {len(sample_texts['html'])} characters\")\n",
    "    print(f\"   Number of sections: {len(chunks)}\")\n",
    "    \n",
    "    print(f\"\\n📝 Extracted sections with hierarchy:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Section {i} ---\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content: {chunk.page_content}\")\n",
    "        print(f\"Length: {len(chunk.page_content)} chars\")\n",
    "    \n",
    "    # Show hierarchy structure\n",
    "    print(f\"\\n\\n🌳 Document structure:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * (len(chunk.metadata) - 1)\n",
    "        headers = \" > \".join([f\"{k}: {v}\" for k, v in chunk.metadata.items()])\n",
    "        print(f\"{indent}{i}. {headers}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "html_chunks = demonstrate_html_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b44f7",
   "metadata": {},
   "source": [
    "##### 6. Language-Specific Code Splitter - Syntax-Aware Code Splitting\n",
    "\n",
    "Splits source code based on programming language syntax. Understands language-specific constructs like functions, classes, and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact LangChain-like chunking utilities (pure-Python, didactic)\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "def fixed_size_chunk(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Split text into fixed-size chunks with simple overlap (pure Python).\"\"\"\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError(\"chunk_size must be greater than overlap\")\n",
    "    step = chunk_size - overlap\n",
    "    chunks = []\n",
    "    for i in range(0, max(0, len(text)), step):\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append({\"id\": f\"chunk_{i//step}\", \"content\": chunk, \"start\": i, \"end\": i + len(chunk)})\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def context_enrich_chunking(text: str, chunk_size: int = 400, overlap: int = 40, doc_title: str = \"Document\") -> List[Dict]:\n",
    "    \"\"\"Create small context-enriched chunks with a simple prefix for teaching purposes.\"\"\"\n",
    "    base = fixed_size_chunk(text, chunk_size=chunk_size, overlap=overlap)\n",
    "    enriched = []\n",
    "    for c in base:\n",
    "        prefix = f\"[Document: {doc_title}] [Pos: {c['start']}..{c['end']}]\\n\\n\"\n",
    "        enriched.append({\"id\": c[\"id\"], \"content\": c[\"content\"], \"enriched\": prefix + c[\"content\"]})\n",
    "    return enriched\n",
    "\n",
    "# Register into tutorial_state for reuse in later examples\n",
    "tutorial_state.setdefault(\"chunking\", {})\n",
    "tutorial_state[\"chunking\"][\"fixed_size_chunk\"] = fixed_size_chunk\n",
    "tutorial_state[\"chunking\"][\"context_enrich_chunking\"] = context_enrich_chunking\n",
    "\n",
    "print('Compact chunking utilities ready (fixed_size_chunk, context_enrich_chunking).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7fee1",
   "metadata": {},
   "source": [
    "##### 7. LatexTextSplitter - LaTeX Document Splitting\n",
    "\n",
    "Specialized splitter for LaTeX documents, preserving mathematical equations and LaTeX-specific structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_latex_splitter():\n",
    "    \"\"\"\n",
    "    LatexTextSplitter: Specialized for LaTeX documents\n",
    "    \n",
    "    How it works:\n",
    "    - Recognizes LaTeX structure (sections, subsections)\n",
    "    - Preserves equation environments\n",
    "    - Understands LaTeX-specific syntax\n",
    "    \n",
    "    Use Cases:\n",
    "    - Academic papers\n",
    "    - Mathematical content\n",
    "    - Scientific publications\n",
    "    - Technical reports with equations\n",
    "    \n",
    "    Advantages:\n",
    "    - Preserves math equations\n",
    "    - Respects LaTeX structure\n",
    "    - Maintains semantic units\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"7. LATEX TEXT SPLITTER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    splitter = LatexTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(sample_texts['latex'])\n",
    "    \n",
    "    print(f\"\\n📊 LaTeX splitting results:\")\n",
    "    print(f\"   Original LaTeX length: {len(sample_texts['latex'])} characters\")\n",
    "    print(f\"   Number of chunks: {len(chunks)}\")\n",
    "    \n",
    "    print(f\"\\n📝 LaTeX chunks (preserving equations):\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "        print(chunk)\n",
    "    \n",
    "    print(f\"\\n\\n🔧 LaTeX-specific separators used:\")\n",
    "    latex_separators = [\n",
    "        '\\\\n\\\\\\\\chapter{',\n",
    "        '\\\\n\\\\\\\\section{',\n",
    "        '\\\\n\\\\\\\\subsection{',\n",
    "        '\\\\n\\\\\\\\subsubsection{',\n",
    "        '\\\\n\\\\\\\\begin{enumerate}',\n",
    "        '\\\\n\\\\\\\\begin{itemize}',\n",
    "        '\\\\n\\\\\\\\begin{description}',\n",
    "        '\\\\n\\\\\\\\begin{list}',\n",
    "        '\\\\n\\\\\\\\begin{quote}',\n",
    "        '\\\\n\\\\\\\\begin{quotation}',\n",
    "        '\\\\n\\\\\\\\begin{verse}',\n",
    "        '\\\\n\\\\\\\\begin{verbatim}',\n",
    "        '\\\\n\\\\\\\\begin{align}',\n",
    "        '\\\\n$$',\n",
    "        '\\\\n\\\\n',\n",
    "        '\\\\n',\n",
    "        ' ',\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    print(\"   Priority order:\")\n",
    "    for i, sep in enumerate(latex_separators[:10], 1):\n",
    "        print(f\"   {i}. {repr(sep):30s}\")\n",
    "    print(f\"   ... and {len(latex_separators) - 10} more\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "latex_chunks = demonstrate_latex_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ff280",
   "metadata": {},
   "source": [
    "##### 8. NLTKTextSplitter - Sentence-Based Splitting\n",
    "\n",
    "Uses NLTK (Natural Language Toolkit) for proper sentence boundary detection. More accurate than simple character-based splitting for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_nltk_splitter():\n",
    "    \"\"\"\n",
    "    NLTKTextSplitter: Sentence-aware splitting using NLTK\n",
    "    \n",
    "    How it works:\n",
    "    - Uses NLTK's sentence tokenizer\n",
    "    - Handles abbreviations (Dr., Mr., etc.)\n",
    "    - Understands sentence boundaries better than regex\n",
    "    \n",
    "    Use Cases:\n",
    "    - Natural language text\n",
    "    - When sentence integrity is crucial\n",
    "    - Content where periods appear in abbreviations\n",
    "    - Proper grammatical splitting\n",
    "    \n",
    "    Advantages:\n",
    "    - More accurate sentence detection\n",
    "    - Handles edge cases (abbreviations, decimals)\n",
    "    - Linguistic awareness\n",
    "    \n",
    "    Note: Requires NLTK punkt tokenizer\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"8. NLTK TEXT SPLITTER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Download NLTK data if not already present\n",
    "    try:\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            print(\"📥 Downloading NLTK punkt tokenizer...\")\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  NLTK not available: {e}\")\n",
    "        print(\"   Install with: pip install nltk\")\n",
    "        return []\n",
    "    \n",
    "    splitter = NLTKTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    \n",
    "    # Create text with tricky sentence boundaries\n",
    "    tricky_text = \"\"\"\n",
    "Dr. Smith works at A.I. Labs Inc. in the U.S.A. He published a paper in 2024.\n",
    "The research showed 95.5% accuracy. Mr. Johnson said, \"This is great!\"\n",
    "However, Ms. Brown noted several issues. The model costs $1,000.50 to run.\n",
    "Prof. Anderson's team achieved better results. They used GPT-4 for testing.\n",
    "\"\"\"\n",
    "    \n",
    "    chunks = splitter.split_text(tricky_text)\n",
    "    \n",
    "    print(f\"\\n📊 NLTK splitting results:\")\n",
    "    print(f\"   Original text length: {len(tricky_text)} characters\")\n",
    "    print(f\"   Number of chunks: {len(chunks)}\")\n",
    "    \n",
    "    print(f\"\\n📝 How NLTK handles sentence boundaries:\")\n",
    "    print(\"   Original text with abbreviations:\")\n",
    "    print(tricky_text)\n",
    "    \n",
    "    print(f\"\\n   Split into {len(chunks)} chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n   Chunk {i}:\")\n",
    "        print(f\"   {chunk}\")\n",
    "    \n",
    "    # Compare with simple period-based splitting\n",
    "    print(f\"\\n\\n🔬 Comparison: NLTK vs Simple Period Split\")\n",
    "    simple_chunks = tricky_text.split('. ')\n",
    "    \n",
    "    print(f\"   NLTK splitting: {len(chunks)} chunks (sentence-aware)\")\n",
    "    print(f\"   Period splitting: {len(simple_chunks)} chunks (breaks on abbreviations)\")\n",
    "    \n",
    "    print(f\"\\n   Simple period split (incorrect):\")\n",
    "    for i, chunk in enumerate(simple_chunks[:5], 1):\n",
    "        print(f\"   {i}. {chunk[:50]}...\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "try:\n",
    "    nltk_chunks = demonstrate_nltk_splitter()\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  NLTK splitter demo skipped: {e}\")\n",
    "    nltk_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee6c5a",
   "metadata": {},
   "source": [
    "##### 9. SpacyTextSplitter - Advanced NLP-Based Splitting\n",
    "\n",
    "Uses spaCy for linguistic-aware splitting with advanced NLP features like named entity recognition and dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed341155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_spacy_splitter():\n",
    "    \"\"\"\n",
    "    SpacyTextSplitter: Advanced NLP-based splitting\n",
    "    \n",
    "    How it works:\n",
    "    - Uses spaCy's linguistic models\n",
    "    - Understands sentence structure\n",
    "    - Can leverage NER, POS tagging, dependencies\n",
    "    \n",
    "    Use Cases:\n",
    "    - Complex natural language text\n",
    "    - When linguistic features matter\n",
    "    - Multi-language support\n",
    "    - Advanced text analysis\n",
    "    \n",
    "    Advantages:\n",
    "    - Most sophisticated sentence detection\n",
    "    - Multi-language support\n",
    "    - Access to linguistic features\n",
    "    - Production-grade NLP\n",
    "    \n",
    "    Note: Requires spaCy and language model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"9. SPACY TEXT SPLITTER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        import spacy\n",
    "        # Try to load a spacy model\n",
    "        try:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"📥 spaCy model not found. Install with:\")\n",
    "            print(\"   python -m spacy download en_core_web_sm\")\n",
    "            print(\"\\n⚠️  Skipping spaCy demo\")\n",
    "            return []\n",
    "        \n",
    "        splitter = SpacyTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=20,\n",
    "            pipeline=\"en_core_web_sm\"\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_text(sample_texts['article'])\n",
    "        \n",
    "        print(f\"\\n📊 spaCy splitting results:\")\n",
    "        print(f\"   Original text length: {len(sample_texts['article'])} characters\")\n",
    "        print(f\"   Number of chunks: {len(chunks)}\")\n",
    "        print(f\"   Chunk sizes: {[len(chunk) for chunk in chunks]}\")\n",
    "        \n",
    "        print(f\"\\n📝 Sample chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:2], 1):\n",
    "            print(f\"\\n--- Chunk {i} ---\")\n",
    "            print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
    "        \n",
    "        # Demonstrate linguistic awareness\n",
    "        print(f\"\\n\\n🧠 spaCy's linguistic awareness:\")\n",
    "        doc = nlp(sample_texts['article'][:300])\n",
    "        \n",
    "        print(f\"\\n   Detected {len(list(doc.sents))} sentences\")\n",
    "        print(f\"   Named entities found:\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"      - {ent.text:20s} ({ent.label_})\")\n",
    "        \n",
    "        print(f\"\\n✅ spaCy provides the most sophisticated NLP-based splitting\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  spaCy not installed. Install with:\")\n",
    "        print(\"   pip install spacy\")\n",
    "        print(\"   python -m spacy download en_core_web_sm\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error in spaCy demo: {e}\")\n",
    "        return []\n",
    "\n",
    "spacy_chunks = demonstrate_spacy_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a563d",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb282f3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dailydoseofds.com/content/images/2024/11/chunking-rag-1.gif\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640c017",
   "metadata": {},
   "source": [
    "While **splitting** divides documents at natural boundaries (paragraphs, headers, sentences), **chunking** is the strategic process of creating optimally-sized, semantically coherent units for embedding and retrieval. Chunking goes beyond simple text division—it's about engineering the perfect information containers for your RAG system.\n",
    "\n",
    "**The Critical Distinction:**\n",
    "\n",
    "| Aspect | **Splitting** | **Chunking** |\n",
    "|--------|---------------|--------------|\n",
    "| **Purpose** | Break documents at natural boundaries | Create optimal retrieval units |\n",
    "| **Focus** | Structure and syntax | Semantics and meaning |\n",
    "| **Output** | Text pieces of varying utility | Engineered information containers |\n",
    "| **Optimization** | Readability, structure preservation | Retrieval accuracy, embedding quality |\n",
    "\n",
    "**Why Chunking Matters:**\n",
    "\n",
    "The quality of your chunks directly determines RAG system performance:\n",
    "\n",
    "$$RAG\\\\_Quality = f(chunk\\\\_semantics, chunk\\\\_size, chunk\\\\_overlap, context\\\\_preservation)$$\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Context Window vs. Precision**: Larger chunks provide more context but reduce retrieval precision\n",
    "2. **Semantic Coherence**: Chunks must be self-contained and meaningful\n",
    "3. **Embedding Quality**: Chunks must fit embedding model constraints while maintaining semantic integrity\n",
    "4. **Retrieval Granularity**: Finding the sweet spot between too specific and too generic\n",
    "\n",
    "**The Chunking Spectrum:**\n",
    "\n",
    "```\n",
    "Fixed-Size → Sentence-Based → Semantic → Hierarchical → Context-Enriched\n",
    "  (Simple)         (Better)      (Smart)    (Advanced)     (Production)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09846e",
   "metadata": {},
   "source": [
    "##### Chunking Strategies Overview\n",
    "\n",
    "We'll explore chunking from simple to sophisticated, using both **LangChain** (primary, most common) and **LlamaIndex** (advanced, for complex hierarchies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the compact chunking utilities on a short sample\n",
    "sample_document = \"\"\"\n",
    "Neural networks learn patterns from data. They use layers, backpropagation and optimization.\n",
    "This short sample demonstrates chunking behavior in a compact, deterministic way.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n🎯 Compact chunking demonstration\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fixed = tutorial_state[\"chunking\"][\"fixed_size_chunk\"](sample_document, chunk_size=80, overlap=10)\n",
    "print(f\"Fixed-size chunks produced: {len(fixed)}\")\n",
    "print(f\" First chunk preview:\\n{fixed[0]['content'][:120]}\\n\")\n",
    "\n",
    "enriched = tutorial_state[\"chunking\"][\"context_enrich_chunking\"](sample_document, chunk_size=80, overlap=10, doc_title=\"Neural Networks Overview\")\n",
    "print(f\"Context-enriched chunks produced: {len(enriched)}\")\n",
    "print(f\" Sample enriched preview:\\n{enriched[0]['enriched'][:200]}\\n\")\n",
    "\n",
    "# Store simple results for later demonstration\n",
    "tutorial_state.setdefault('chunking_results', {})\n",
    "tutorial_state['chunking_results']['fixed'] = fixed\n",
    "tutorial_state['chunking_results']['enriched'] = enriched\n",
    "\n",
    "print('Chunking demo complete — results saved to tutorial_state[\"chunking_results\"].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68eeee",
   "metadata": {},
   "source": [
    "##### 1. LangChain Chunking Strategies (Primary Approach)\n",
    "\n",
    "LangChain provides flexible chunking through its text splitters, which we covered in the Splitting section. Here we'll focus on **how to optimize them specifically for chunking** in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b002ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLangChainChunkingStrategies\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    LangChain-based chunking strategies optimized for RAG systems\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    This class demonstrates various chunking approaches using LangChain's\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    text splitters, with focus on optimizing for retrieval and embedding quality.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mLangChainChunkingStrategies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategies \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔧 LangChain Chunking Strategies initialized\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfixed_size_chunking\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, chunk_overlap: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m[Dict]:\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Strategy 1: Fixed-Size Chunking\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    - Artificial boundaries\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# Minimal orchestrator example — demonstrates a simple RAG + skill + tool flow\n",
    "\n",
    "def orchestrator(query: str) -> dict:\n",
    "    \"\"\"A tiny orchestration pattern for teaching.\n",
    "    Steps shown:\n",
    "    1. Retrieve (from the simple in-memory doc_loader)\n",
    "    2. Run a skill (from the skill registry)\n",
    "    3. Call an MCP-like tool (simulated)\n",
    "    4. Aggregate results and return a clear, labelled dict\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # 1) Retrieval (very small, synchronous)\n",
    "    docs = tutorial_state.get('doc_loader').documents if tutorial_state.get('doc_loader') else []\n",
    "    retrieved = [d for d in docs if query.lower() in d.get('content','').lower()]\n",
    "    results['retrieved_count'] = len(retrieved)\n",
    "    results['retrieved_preview'] = retrieved[0]['content'][:200] if retrieved else None\n",
    "\n",
    "    # 2) Run a skill if available\n",
    "    skill_output = None\n",
    "    if 'skills' in tutorial_state and 'financial_analysis' in tutorial_state['skills']:\n",
    "        skill_res = run_skill('financial_analysis', query)\n",
    "        skill_output = {'success': skill_res.success, 'output': skill_res.output, 'confidence': skill_res.confidence}\n",
    "    results['skill'] = skill_output\n",
    "\n",
    "    # 3) Call an MCP-like tool (simulated)\n",
    "    try:\n",
    "        mcp_data = mcp_read_resource_sync('analytics') if 'mcp_read_resource_sync' in globals() else '{}'\n",
    "    except Exception as e:\n",
    "        mcp_data = json.dumps({'error': str(e)})\n",
    "    results['mcp_analytics'] = json.loads(mcp_data) if isinstance(mcp_data, str) else mcp_data\n",
    "\n",
    "    # 4) Simple generation/decision step (simulated LLM output)\n",
    "    # For teaching, avoid real LLM calls; produce an instructive summary instead\n",
    "    results['summary'] = (\n",
    "        f\"Orchestrator ran retrieval ({results['retrieved_count']} docs), \"\n",
    "        f\"ran skill: {bool(skill_output)}, and fetched analytics.\" \n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage (teaching demo)\n",
    "demo_q = \"retrieval\"  # change this string to test different flows\n",
    "print('\\n--- ORCHESTRATOR DEMO ---')\n",
    "print('Query:', demo_q)\n",
    "print('Result:', orchestrator(demo_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ccd76",
   "metadata": {},
   "source": [
    "LlamaIndex takes a more sophisticated approach by creating **Nodes** instead of simple text chunks. Nodes are first-class objects with rich metadata, relationships, and hierarchy.\n",
    "\n",
    "**Key Differences from LangChain:**\n",
    "\n",
    "| Aspect | LangChain | LlamaIndex |\n",
    "|--------|-----------|------------|\n",
    "| **Output** | Text strings | Node objects with metadata |\n",
    "| **Relationships** | Manual | Automatic parent-child links |\n",
    "| **Metadata** | Basic | Rich, structured metadata |\n",
    "| **Use Case** | Flexible text processing | Index-centric workflows |\n",
    "| **Integration** | Works with any system | Optimized for LlamaIndex indexes |\n",
    "\n",
    "**When to Use LlamaIndex:**\n",
    "- Building complex document hierarchies\n",
    "- Need automatic parent-child relationships\n",
    "- Using LlamaIndex for indexing/retrieval\n",
    "- Require sophisticated metadata extraction\n",
    "- Want semantic-based splitting (embedding-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f52ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex conceptual demo (kept minimal for tutorial)\n",
    "try:\n",
    "    import llama_index  # type: ignore\n",
    "    LLAMAINDEX_AVAILABLE = True\n",
    "    print(\"✅ LlamaIndex import found — advanced demos can be enabled if you install dependencies.\")\n",
    "except Exception:\n",
    "    LLAMAINDEX_AVAILABLE = False\n",
    "    print(\"⚠️ LlamaIndex not installed. To run advanced node parsing demos, install: pip install llama-index\")\n",
    "\n",
    "\n",
    "def llamaindex_simple_node_concept(text: str):\n",
    "    \"\"\"Return a short conceptual description of what node parsing would produce.\"\"\"\n",
    "    if not LLAMAINDEX_AVAILABLE:\n",
    "        return {\"conceptual\": True, \"nodes\": int(max(1, len(text) // 400)), \"note\": \"Install llama-index to run real node parsing.\"}\n",
    "    # If available, a small real demo could be added here (kept out of scope)\n",
    "    return {\"conceptual\": False, \"nodes\": 0}\n",
    "\n",
    "\n",
    "def llamaindex_semantic_splitter_concept(text: str):\n",
    "    \"\"\"Conceptual semantic splitter explanation/result for teaching.\"\"\"\n",
    "    return {\"conceptual\": True, \"explanation\": \"Semantic splitting groups sentences by embedding similarity; requires an embedding model.\"}\n",
    "\n",
    "# Register simple conceptual helpers\n",
    "tutorial_state.setdefault('llamaindex', {})\n",
    "tutorial_state['llamaindex']['simple_node_concept'] = llamaindex_simple_node_concept\n",
    "tutorial_state['llamaindex']['semantic_splitter_concept'] = llamaindex_semantic_splitter_concept\n",
    "\n",
    "print('LlamaIndex conceptual helpers registered in tutorial_state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d351c",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311af1f",
   "metadata": {},
   "source": [
    "<img src=\"https://framerusercontent.com/images/v8f1U7fmqjvMy7Rcq8qGO2WJpTI.png?width=960&height=540\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67b029",
   "metadata": {},
   "source": [
    "What Are Document Embeddings?\n",
    "\n",
    "Document embeddings are **dense numerical vector representations** of text that capture semantic meaning in a high-dimensional space. Unlike simple keyword matching, embeddings encode the *meaning* and *context* of text, allowing machines to understand that \"car\" and \"automobile\" are similar, or that \"king\" relates to \"queen\" in a way similar to how \"man\" relates to \"woman\".\n",
    "\n",
    "Key Characteristics\n",
    "\n",
    "- **Dense Vectors**: Typically 384 to 1536+ dimensions (depending on the model)\n",
    "- **Semantic Representation**: Similar meanings → similar vectors\n",
    "- **Fixed Length**: Any text length → fixed-size vector\n",
    "- **Learned Representations**: Trained on massive text corpora to capture language patterns\n",
    "\n",
    " Example Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf2c63",
   "metadata": {},
   "source": [
    "Text: \"The cat sat on the mat\"\n",
    "Embedding: [0.23, -0.45, 0.67, ..., 0.12]  # 768 dimensions\n",
    "\n",
    "Text: \"A feline rested on the rug\"  \n",
    "Embedding: [0.21, -0.43, 0.69, ..., 0.15]  # Very similar values!\n",
    "\n",
    "Text: \"Python programming language\"\n",
    "Embedding: [-0.82, 0.31, -0.15, ..., 0.91]  # Very different!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462679e7",
   "metadata": {},
   "source": [
    "In a RAG system, embeddings serve as the **bridge between natural language queries and relevant documents**. Here's how they fit into the pipeline:\n",
    "\n",
    "1. **Indexing Phase** (Offline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a663c",
   "metadata": {},
   "source": [
    "Documents → Split into Chunks → Generate Embeddings → Store in Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65d44a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Break documents into semantic chunks (paragraphs, passages)\n",
    "- Convert each chunk to an embedding vector\n",
    "- Store vectors with metadata in a vector database\n",
    "\n",
    " 2. **Retrieval Phase** (Online)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11784df0",
   "metadata": {},
   "source": [
    "User Query → Generate Query Embedding → Search Similar Vectors → Retrieve Top-K Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6ccc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Convert user's question to an embedding (same model)\n",
    "- Find vectors closest to the query vector (cosine similarity)\n",
    "- Return the most semantically relevant document chunks\n",
    "\n",
    "3. **Generation Phase**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d980a1",
   "metadata": {},
   "source": [
    "Retrieved Chunks + Query → LLM → Grounded Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dabf6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Feed relevant chunks as context to the LLM\n",
    "- LLM generates answer based on retrieved information\n",
    "\n",
    " Why Embeddings Are Critical for RAG\n",
    "\n",
    " Traditional Keyword Search Limitations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48ecc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query: \"How do I fix a leaky faucet?\"\n",
    "Document: \"Repairing a dripping tap requires...\"\n",
    "\n",
    "# ❌ Keyword match: Poor (no shared words)\n",
    "# ✅ Semantic match: Excellent (same meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1f64a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Embedding-Based Search Advantages\n",
    "\n",
    "1. **Semantic Understanding**\n",
    "   - Matches meaning, not just words\n",
    "   - Handles synonyms, paraphrasing, and context\n",
    "\n",
    "2. **Multi-lingual Capability**\n",
    "   - Cross-language retrieval possible\n",
    "   - \"hello\" can match \"bonjour\" in embedding space\n",
    "\n",
    "3. **Contextual Nuance**\n",
    "   - \"bank\" (financial) vs \"bank\" (river) distinguished by context\n",
    "   - Homonyms handled correctly\n",
    "\n",
    "4. **Ranked Relevance**\n",
    "   - Similarity scores for ranking results\n",
    "   - Top-K retrieval returns best matches\n",
    "\n",
    " Embedding Space Intuition\n",
    "\n",
    "Think of embedding space as a **map of meaning**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203a704",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "        Pets\n",
    "      /      \\\n",
    "   Dogs      Cats\n",
    "    |         |\n",
    "  Puppy    Kitten\n",
    "    \n",
    "    (far away)\n",
    "    \n",
    "   Programming\n",
    "      /    \\\n",
    "  Python  JavaScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d62a60",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Related concepts cluster together\n",
    "- Distance = semantic similarity\n",
    "- Queries find nearest neighbors in this space\n",
    "\n",
    " What Makes Good Embeddings for RAG?\n",
    "\n",
    "1. **Domain Alignment**: Trained on relevant data (general vs specialized)\n",
    "2. **Dimensionality**: Balance between expressiveness and compute (384-1536)\n",
    "3. **Consistency**: Same model for indexing and querying\n",
    "4. **Retrieval Optimization**: Some models trained specifically for search tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee09f8a",
   "metadata": {},
   "source": [
    " Types of Embeddings and Their Mathematics\n",
    "\n",
    " Word Embeddings vs Document Embeddings\n",
    "\n",
    "**Word Embeddings** represent individual words as vectors, while **Document Embeddings** represent entire passages, sentences, or documents. For RAG, we primarily use document embeddings since we need to encode chunks of text.\n",
    "\n",
    "**Word-Level Examples:**\n",
    "- Word2Vec (2013)\n",
    "- GloVe (2014)\n",
    "- FastText (2016)\n",
    "\n",
    "**Document-Level Examples:**\n",
    "- Sentence-BERT (2019)\n",
    "- Universal Sentence Encoder\n",
    "- OpenAI Ada-002\n",
    "- BGE, E5, Instructor models (2023+)\n",
    "\n",
    " The Mathematics Behind Embeddings\n",
    "\n",
    "##### Cosine Similarity: The Core Metric\n",
    "\n",
    "The most common way to measure similarity between embeddings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a26a3f",
   "metadata": {},
   "source": [
    "cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)\n",
    "\n",
    "Where:\n",
    "- A · B = dot product (sum of element-wise multiplication)\n",
    "- ||A|| = magnitude/length of vector A\n",
    "- Result ranges from -1 (opposite) to 1 (identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb3c94",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Example calculation:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8910b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Two embedding vectors\n",
    "embedding_a = np.array([0.5, 0.8, -0.3, 0.6])\n",
    "embedding_b = np.array([0.6, 0.7, -0.2, 0.5])\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(embedding_a, embedding_b)\n",
    "# 0.5*0.6 + 0.8*0.7 + (-0.3)*(-0.2) + 0.6*0.5 = 1.22\n",
    "\n",
    "# Magnitudes\n",
    "magnitude_a = np.linalg.norm(embedding_a)  # 1.145\n",
    "magnitude_b = np.linalg.norm(embedding_b)  # 1.0\n",
    "\n",
    "# Cosine similarity\n",
    "similarity = dot_product / (magnitude_a * magnitude_b)\n",
    "# = 1.22 / 1.145 = 0.9345 (very similar!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ba2ee",
   "metadata": {},
   "source": [
    "distance = sqrt(sum((A_i - B_i)²))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08d96b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Dot Product** (without normalization):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e34a2",
   "metadata": {},
   "source": [
    "similarity = sum(A_i × B_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9d7b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Manhattan Distance** (L1 distance):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353e0f6",
   "metadata": {},
   "source": [
    "distance = sum(|A_i - B_i|)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01604e9c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**When to use which:**\n",
    "- **Cosine similarity**: Most common, ignores magnitude (only direction matters)\n",
    "- **Euclidean distance**: When magnitude matters (rare in semantic search)\n",
    "- **Dot product**: Faster, used when vectors are already normalized\n",
    "\n",
    " Types of Embedding Models\n",
    "\n",
    "##### 1. Dense Embeddings (Standard Approach)\n",
    "\n",
    "**What they are:** Every dimension has a non-zero value, creating a compact representation.\n",
    "\n",
    "**Characteristics:**\n",
    "- Fixed-size vectors (typically 384, 768, or 1536 dimensions)\n",
    "- All values contribute to meaning\n",
    "- Efficient for similarity search\n",
    "\n",
    "**Popular Models:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers (most popular for RAG)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Small, fast model (384 dimensions)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Larger, more accurate (768 dimensions)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Optimized for retrieval (1024 dimensions)\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "\n",
    "# Generate embeddings\n",
    "texts = [\"This is a document\", \"Another document\"]\n",
    "embeddings = model.encode(texts)\n",
    "print(embeddings.shape)  # (2, 384) or (2, 768) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f66cb0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**When to use:**\n",
    "- General-purpose RAG applications\n",
    "- Need good out-of-the-box performance\n",
    "- Have moderate compute resources\n",
    "\n",
    "##### 2. Sparse Embeddings (BM25-style)\n",
    "\n",
    "**What they are:** Most dimensions are zero, only a few non-zero values representing keywords.\n",
    "\n",
    "**Mathematics (BM25 algorithm):**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac78fbc",
   "metadata": {},
   "source": [
    "BM25(query, doc) = sum over terms( IDF(term) × (freq × (k1 + 1)) / (freq + k1 × (1 - b + b × |doc|/avgdl)) )\n",
    "\n",
    "Where:\n",
    "- IDF = log((N - n + 0.5) / (n + 0.5))\n",
    "- N = total documents\n",
    "- n = documents containing term\n",
    "- freq = term frequency in document\n",
    "- k1, b = tuning parameters (typically 1.5, 0.75)\n",
    "- |doc| = document length\n",
    "- avgdl = average document length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053061b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Implementation:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Tokenize documents\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog played in the park\",\n",
    "    \"Cats and dogs are pets\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "\n",
    "# Create BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Query\n",
    "query = \"cat pet\"\n",
    "tokenized_query = query.split()\n",
    "\n",
    "# Get scores for all documents\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print(scores)  # [higher score for doc 1 and 3]\n",
    "\n",
    "# Get top documents\n",
    "top_docs = bm25.get_top_n(tokenized_query, corpus, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e48895",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**When to use:**\n",
    "- Exact keyword matching is important\n",
    "- Domain-specific terminology\n",
    "- Hybrid with dense embeddings for best results\n",
    "\n",
    "##### 3. Hybrid Embeddings (Dense + Sparse)\n",
    "\n",
    "**What they are:** Combine semantic understanding (dense) with keyword precision (sparse).\n",
    "\n",
    "**Implementation:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ea9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, documents, alpha=0.5):\n",
    "        \"\"\"\n",
    "        alpha: weight for dense (1-alpha for sparse)\n",
    "        alpha=0.5 means equal weight\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Dense embeddings\n",
    "        self.dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.doc_embeddings = self.dense_model.encode(documents)\n",
    "        \n",
    "        # Sparse embeddings (BM25)\n",
    "        tokenized = [doc.split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "    \n",
    "    def retrieve(self, query, top_k=5):\n",
    "        # Dense scores (cosine similarity)\n",
    "        query_embedding = self.dense_model.encode([query])[0]\n",
    "        dense_scores = np.dot(self.doc_embeddings, query_embedding)\n",
    "        dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min())\n",
    "        \n",
    "        # Sparse scores (BM25)\n",
    "        sparse_scores = self.bm25.get_scores(query.split())\n",
    "        sparse_scores = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-10)\n",
    "        \n",
    "        # Combine scores\n",
    "        hybrid_scores = self.alpha * dense_scores + (1 - self.alpha) * sparse_scores\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(hybrid_scores)[-top_k:][::-1]\n",
    "        return [(self.documents[i], hybrid_scores[i]) for i in top_indices]\n",
    "\n",
    "# Usage\n",
    "docs = [\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Deep learning uses neural networks with many layers\",\n",
    "    \"Python is popular for data science and ML\"\n",
    "]\n",
    "\n",
    "retriever = HybridRetriever(docs, alpha=0.7)  # 70% dense, 30% sparse\n",
    "results = retriever.retrieve(\"neural network training\", top_k=2)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.3f} | Doc: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1c4bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**When to use:**\n",
    "- Best of both worlds for production systems\n",
    "- Need both semantic and keyword matching\n",
    "- Can tune alpha based on your domain\n",
    "\n",
    "##### 4. Cross-Encoders (Re-ranking)\n",
    "\n",
    "**What they are:** Instead of separate embeddings, they encode query + document together for scoring.\n",
    "\n",
    "**Key difference:**\n",
    "- **Bi-encoders** (standard): Encode query and doc separately, compare embeddings (fast)\n",
    "- **Cross-encoders**: Encode query+doc together (slow but accurate)\n",
    "\n",
    "**Mathematics:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369712e",
   "metadata": {},
   "source": [
    "bi-encoder: similarity(embed(query), embed(doc))\n",
    "cross-encoder: score(concat(query, doc))  # joint encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd016e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Implementation (for re-ranking):**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# First retrieve with bi-encoder (fast)\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "query = \"How to train a neural network?\"\n",
    "docs = [...]  # your document corpus\n",
    "\n",
    "query_emb = bi_encoder.encode(query)\n",
    "doc_embs = bi_encoder.encode(docs)\n",
    "\n",
    "# Get top 100 candidates (fast)\n",
    "similarities = np.dot(doc_embs, query_emb)\n",
    "top_100_indices = np.argsort(similarities)[-100:][::-1]\n",
    "top_100_docs = [docs[i] for i in top_100_indices]\n",
    "\n",
    "# Re-rank with cross-encoder (accurate)\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "query_doc_pairs = [[query, doc] for doc in top_100_docs]\n",
    "rerank_scores = cross_encoder.predict(query_doc_pairs)\n",
    "\n",
    "# Get final top-k\n",
    "final_top_k = np.argsort(rerank_scores)[-5:][::-1]\n",
    "final_results = [top_100_docs[i] for i in final_top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bc237",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**When to use:**\n",
    "- Production systems where accuracy is critical\n",
    "- Two-stage retrieval: fast bi-encoder → accurate cross-encoder\n",
    "- Can afford extra compute for re-ranking\n",
    "\n",
    " Specialized Embedding Models\n",
    "\n",
    "##### Domain-Specific Models\n",
    "\n",
    "**Medical/Scientific:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ca296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioBERT for medical text\n",
    "model = SentenceTransformer('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')\n",
    "\n",
    "# SciBERT for scientific papers\n",
    "model = SentenceTransformer('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622fe0b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Code Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a7b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeBERT for code snippets\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadff72",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Multi-lingual:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works across 100+ languages\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68dc47",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Embedding dimensions vs performance:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade-off example\n",
    "model_384 = SentenceTransformer('all-MiniLM-L6-v2')   # 384d\n",
    "model_768 = SentenceTransformer('all-mpnet-base-v2')  # 768d\n",
    "model_1024 = SentenceTransformer('bge-large-en-v1.5') # 1024d\n",
    "\n",
    "# Speed: 384d > 768d > 1024d\n",
    "# Accuracy: 1024d > 768d > 384d\n",
    "# Storage: 384d < 768d < 1024d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30835d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Normalization importance:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always normalize for cosine similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "embeddings_normalized = normalize(embeddings)\n",
    "\n",
    "# Now dot product = cosine similarity (faster computation)\n",
    "similarity = np.dot(embeddings_normalized[0], embeddings_normalized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d238e7",
   "metadata": {},
   "source": [
    "### Storing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96a82f",
   "metadata": {},
   "source": [
    "<img src=\"https://d11qzsb0ksp6iz.cloudfront.net/assets/dff374c348_indexing-in-vector-database.webp\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb48998",
   "metadata": {},
   "source": [
    "\n",
    "Once we've converted documents into embeddings, we need an efficient way to store and retrieve them. This is where **vector databases** come in—specialized data stores optimized for similarity search over high-dimensional vectors.\n",
    "\n",
    " Why Traditional Databases Don't Work\n",
    "\n",
    "**Traditional SQL databases** are designed for exact matches:\n",
    "```sql\n",
    "SELECT * FROM documents WHERE title = 'Machine Learning';  -- Fast with index\n",
    "SELECT * FROM documents WHERE vector_similarity(embedding, query) > 0.8;  -- Slow!\n",
    "```\n",
    "\n",
    "**The problem:** Computing cosine similarity against millions of vectors requires comparing every single vector—an O(n) operation that doesn't scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive approach (don't do this at scale!)\n",
    "import numpy as np\n",
    "\n",
    "def find_similar(query_vector, all_vectors, top_k=5):\n",
    "    similarities = []\n",
    "    for i, doc_vector in enumerate(all_vectors):  # O(n) - checks EVERY vector\n",
    "        similarity = np.dot(query_vector, doc_vector)\n",
    "        similarities.append((i, similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# For 1 million documents with 768-dim embeddings:\n",
    "# = 1M × 768 dot products = 768 million operations PER QUERY!\n",
    "print(\"For 1M docs: ~768M operations per query (too slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477119b6",
   "metadata": {},
   "source": [
    "#### Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9f893",
   "metadata": {},
   "source": [
    "What Makes Vector Databases Special\n",
    "\n",
    "Vector databases use specialized data structures and algorithms to enable **approximate nearest neighbor (ANN)** search, reducing complexity from O(n) to O(log n) or even O(1) in some cases.\n",
    "\n",
    "**Key capabilities:**\n",
    "1. **Efficient similarity search** using specialized indexing\n",
    "2. **Horizontal scaling** for billions of vectors\n",
    "3. **Filtering** with metadata (dates, categories, etc.)\n",
    "4. **Real-time updates** without full reindexing\n",
    "5. **Multiple distance metrics** (cosine, euclidean, dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbf37d",
   "metadata": {},
   "source": [
    "The Mathematics of Approximate Nearest Neighbor (ANN) Search\n",
    "\n",
    "##### 1. Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "**Core idea:** Hash similar vectors to the same bucket, so you only search within relevant buckets.\n",
    "\n",
    "**Mathematics:**\n",
    "```\n",
    "Hash function h(v) maps vectors to buckets such that:\n",
    "P(h(v₁) = h(v₂)) is high when similarity(v₁, v₂) is high\n",
    "\n",
    "Example hash function (random projection):\n",
    "h(v) = sign(w · v)\n",
    "where w is a random unit vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSH:\n",
    "    def __init__(self, num_tables=10, num_hash_functions=8, dim=768):\n",
    "        \"\"\"\n",
    "        num_tables: More tables = better recall but slower\n",
    "        num_hash_functions: More functions = fewer false positives\n",
    "        \"\"\"\n",
    "        self.num_tables = num_tables\n",
    "        self.num_hash_functions = num_hash_functions\n",
    "        \n",
    "        # Random projection vectors for each hash function in each table\n",
    "        self.random_vectors = [\n",
    "            np.random.randn(num_hash_functions, dim)\n",
    "            for _ in range(num_tables)\n",
    "        ]\n",
    "        \n",
    "        # Hash tables (dict of dict)\n",
    "        self.tables = [{} for _ in range(num_tables)]\n",
    "    \n",
    "    def _hash(self, vector, table_idx):\n",
    "        \"\"\"Create hash key using random projections\"\"\"\n",
    "        # Dot product with random vectors\n",
    "        projections = np.dot(self.random_vectors[table_idx], vector)\n",
    "        # Convert to binary hash\n",
    "        binary_hash = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "        return binary_hash\n",
    "    \n",
    "    def insert(self, vector, doc_id):\n",
    "        \"\"\"Insert vector into all hash tables\"\"\"\n",
    "        for table_idx in range(self.num_tables):\n",
    "            hash_key = self._hash(vector, table_idx)\n",
    "            \n",
    "            if hash_key not in self.tables[table_idx]:\n",
    "                self.tables[table_idx][hash_key] = []\n",
    "            \n",
    "            self.tables[table_idx][hash_key].append((doc_id, vector))\n",
    "    \n",
    "    def query(self, query_vector, top_k=5):\n",
    "        \"\"\"Find similar vectors using LSH\"\"\"\n",
    "        candidates = set()\n",
    "        \n",
    "        # Get candidates from all tables\n",
    "        for table_idx in range(self.num_tables):\n",
    "            hash_key = self._hash(query_vector, table_idx)\n",
    "            \n",
    "            if hash_key in self.tables[table_idx]:\n",
    "                for doc_id, vector in self.tables[table_idx][hash_key]:\n",
    "                    candidates.add((doc_id, tuple(vector)))\n",
    "        \n",
    "        # Compute exact similarities for candidates only\n",
    "        results = []\n",
    "        for doc_id, vector in candidates:\n",
    "            vector = np.array(vector)\n",
    "            similarity = np.dot(query_vector, vector)\n",
    "            results.append((doc_id, similarity))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "# Demo\n",
    "lsh = LSH(num_tables=5, num_hash_functions=4, dim=384)\n",
    "print(\"LSH Index created with 5 tables and 4 hash functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594e1fa",
   "metadata": {},
   "source": [
    "**Trade-offs:**\n",
    "- **More tables** → better recall (find more relevant docs) but slower & more memory\n",
    "- **More hash functions** → fewer false positives but smaller buckets\n",
    "- **Approximate results** → might miss some relevant docs (95%+ recall typical)\n",
    "\n",
    "##### 2. Hierarchical Navigable Small World (HNSW)\n",
    "\n",
    "**Core idea:** Build a multi-layer graph where each node is a vector. Navigate through layers to quickly zoom in on nearest neighbors.\n",
    "\n",
    "**Mathematics:**\n",
    "```\n",
    "Graph construction:\n",
    "1. Insert vectors as nodes\n",
    "2. Connect to M nearest neighbors per layer\n",
    "3. Higher layers = sparser connections (long jumps)\n",
    "4. Lower layers = denser connections (fine-grained)\n",
    "\n",
    "Search complexity: O(log n) with proper parameters\n",
    "```\n",
    "\n",
    "**Visualization:**\n",
    "```\n",
    "Layer 2: A ---------> Z         (sparse, long-range connections)\n",
    "         |            |\n",
    "Layer 1: A --> B --> Y --> Z    (medium density)\n",
    "         |     |     |     |\n",
    "Layer 0: A->B->C->..X->Y->Z    (dense, all vectors)\n",
    "```\n",
    "\n",
    "**How search works:**\n",
    "1. Start at top layer (sparse)\n",
    "2. Navigate to closest neighbor at each step\n",
    "3. Move down layers when no closer neighbor found\n",
    "4. At bottom layer, find exact nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install hnswlib if needed: pip install hnswlib\n",
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, dim=384, max_elements=10000):\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements\n",
    "        \n",
    "        # Initialize index\n",
    "        self.index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        self.index.init_index(\n",
    "            max_elements=max_elements,\n",
    "            ef_construction=200,  # Higher = better quality but slower build\n",
    "            M=16                   # Number of connections per layer\n",
    "        )\n",
    "        \n",
    "        self.index.set_ef(50)  # Higher = better search recall but slower\n",
    "        \n",
    "        self.doc_ids = []\n",
    "    \n",
    "    def add_documents(self, embeddings, doc_ids):\n",
    "        \"\"\"Add vectors to index\"\"\"\n",
    "        self.index.add_items(embeddings, ids=range(len(embeddings)))\n",
    "        self.doc_ids.extend(doc_ids)\n",
    "    \n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search for similar vectors\"\"\"\n",
    "        # Returns (ids, distances)\n",
    "        labels, distances = self.index.knn_query(query_embedding, k=top_k)\n",
    "        \n",
    "        results = []\n",
    "        for label, distance in zip(labels[0], distances[0]):\n",
    "            doc_id = self.doc_ids[label]\n",
    "            similarity = 1 - distance  # Convert distance to similarity\n",
    "            results.append((doc_id, similarity))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demo (will work when hnswlib is installed)\n",
    "print(\"HNSW Index class defined\")\n",
    "print(\"Parameters: M=16 (connections), ef_construction=200, ef=50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98dc40b",
   "metadata": {},
   "source": [
    "**Parameters explained:**\n",
    "- **M** (connections per node): Higher = better accuracy but more memory (12-48 typical)\n",
    "- **ef_construction**: Higher = better index quality but slower build (100-200 typical)\n",
    "- **ef** (search): Higher = better recall but slower query (50-200 typical)\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Very fast queries** (microseconds even for millions of vectors)\n",
    "- **High recall** (>95% with proper parameters)\n",
    "- **Memory intensive** (stores full graph structure)\n",
    "- **No easy deletes** (rebuilding required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46736572",
   "metadata": {},
   "source": [
    "\n",
    " Comparison Matrix\n",
    "\n",
    "| Database | Type | Best For | Scale | Speed | Setup Complexity |\n",
    "|----------|------|----------|-------|-------|------------------|\n",
    "| **FAISS** | Library | On-premise, high performance | Billions | Very Fast | Medium |\n",
    "| **Chroma** | Embedded | Prototyping, local dev | Millions | Fast | Low |\n",
    "| **Pinecone** | Cloud (Managed) | Production, scaling | Billions | Very Fast | Low |\n",
    "| **Weaviate** | Self-hosted/Cloud | Hybrid search, GraphQL | 10Ms+ | Fast | Medium |\n",
    "| **Qdrant** | Self-hosted/Cloud | Production, filtering | Billions | Very Fast | Medium |\n",
    "| **Milvus** | Distributed | Enterprise, huge scale | Billions | Fast | High |\n",
    "| **PostgreSQL+pgvector** | SQL Extension | Existing Postgres apps | Millions | Medium | Low |\n",
    "| **Redis** | In-memory | Ultra-low latency | Millions | Very Fast | Low |\n",
    "\n",
    "##### 1. **FAISS** (Facebook AI Similarity Search)\n",
    "\n",
    "**When to use:**\n",
    "- Need maximum performance on-premise\n",
    "- Handling billions of vectors\n",
    "- Have engineering resources\n",
    "- Want full control\n",
    "\n",
    "**Code example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import faiss\n",
    "    \n",
    "    # Create simple flat index (exact search)\n",
    "    dim = 384\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    \n",
    "    # Add vectors\n",
    "    vectors = np.random.randn(1000, dim).astype('float32')\n",
    "    index.add(vectors)\n",
    "    \n",
    "    # Search\n",
    "    query = np.random.randn(1, dim).astype('float32')\n",
    "    distances, indices = index.search(query, k=5)\n",
    "    \n",
    "    print(\"✅ FAISS example:\")\n",
    "    print(f\"   Indexed: {index.ntotal} vectors\")\n",
    "    print(f\"   Top-5 indices: {indices[0]}\")\n",
    "    print(f\"   Distances: {distances[0]}\")\n",
    "    \n",
    "    # Save/load\n",
    "    faiss.write_index(index, \"faiss_index.bin\")\n",
    "    loaded_index = faiss.read_index(\"faiss_index.bin\")\n",
    "    print(f\"   Saved and loaded index successfully\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  FAISS not installed. Run: pip install faiss-cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe1616",
   "metadata": {},
   "source": [
    "##### 2. **Chroma** (Simple & Embedded)\n",
    "\n",
    "**When to use:**\n",
    "- Rapid prototyping\n",
    "- Local development\n",
    "- Small to medium datasets\n",
    "- Want simplicity\n",
    "\n",
    "**Code example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79767d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.utils import embedding_functions\n",
    "    \n",
    "    # Initialize client\n",
    "    client = chromadb.Client()\n",
    "    \n",
    "    # Create collection with embedding function\n",
    "    collection = client.create_collection(\n",
    "        name=\"demo_collection\",\n",
    "        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add documents (auto-embeds!)\n",
    "    collection.add(\n",
    "        documents=[\n",
    "            \"This is about machine learning\",\n",
    "            \"This is about deep learning\"\n",
    "        ],\n",
    "        metadatas=[\n",
    "            {\"category\": \"AI\"},\n",
    "            {\"category\": \"AI\"}\n",
    "        ],\n",
    "        ids=[\"doc1\", \"doc2\"]\n",
    "    )\n",
    "    \n",
    "    # Query with filters\n",
    "    results = collection.query(\n",
    "        query_texts=[\"neural networks\"],\n",
    "        n_results=2,\n",
    "        where={\"category\": \"AI\"}\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Chroma example:\")\n",
    "    print(f\"   Documents: {results['documents']}\")\n",
    "    print(f\"   Distances: {results['distances']}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  Chroma not installed. Run: pip install chromadb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608dc711",
   "metadata": {},
   "source": [
    "#### Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e19279",
   "metadata": {},
   "source": [
    "explain different types of vector database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8555b",
   "metadata": {},
   "source": [
    "### Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dcc54",
   "metadata": {},
   "source": [
    "introduce to retreiver mechanisms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c40d32",
   "metadata": {},
   "source": [
    "explain different types of retreival mechanisms , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbce248",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b8387",
   "metadata": {},
   "source": [
    "introduce to evaluation mechniasms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd43da7",
   "metadata": {},
   "source": [
    "explain different types of evaluation methods , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186c77d",
   "metadata": {},
   "source": [
    "### Additionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240e557",
   "metadata": {},
   "source": [
    "##### Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f82ea2",
   "metadata": {},
   "source": [
    "## A Complete Agentic System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11448f82",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adba2b0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813de77",
   "metadata": {},
   "source": [
    "### Key Takeaways: Vector Databases for RAG\n",
    "\n",
    "**🎯 Algorithm Choice:**\n",
    "\n",
    "| Scale | Accuracy Need | Memory | Choose |\n",
    "|-------|---------------|--------|--------|\n",
    "| < 100K | High | Unlimited | **Flat Index** (exact search) |\n",
    "| 100K-1M | High | Limited | **HNSW** (fast + accurate) |\n",
    "| 1M-10M | Medium | Very Limited | **IVF** (clustering) |\n",
    "| 10M+ | Medium | Extremely Limited | **IVF + PQ** (compressed) |\n",
    "\n",
    "**⚡ Performance Tips:**\n",
    "\n",
    "1. **Start simple**: Use flat index for prototyping\n",
    "2. **Profile first**: Measure before optimizing\n",
    "3. **Normalize vectors**: For consistent cosine similarity\n",
    "4. **Batch operations**: Add/query in batches\n",
    "5. **Monitor recall**: Track accuracy vs speed trade-offs\n",
    "\n",
    "**🏗️ Production Checklist:**\n",
    "\n",
    "- ✅ **Persistence**: Save/load indexes\n",
    "- ✅ **Metadata filtering**: Support rich queries\n",
    "- ✅ **Monitoring**: Track query latency, recall\n",
    "- ✅ **Scaling**: Plan for growth (10x, 100x)\n",
    "- ✅ **Backup**: Regular index backups\n",
    "- ✅ **Version control**: Track index versions\n",
    "\n",
    "**🚀 Next Steps:**\n",
    "\n",
    "With vector storage mastered, you can now:\n",
    "1. **Build complete RAG pipelines** (retrieval + generation)\n",
    "2. **Implement hybrid search** (vector + keyword)\n",
    "3. **Add reranking** for improved accuracy\n",
    "4. **Optimize for your use case** (latency vs accuracy)#### Workflow Pattern Selection Guide & Best Practices\n",
    "\n",
    "Choosing the right workflow pattern is crucial for building effective agentic systems. Here's a comprehensive guide based on production experience and Anthropic's research:\n",
    "\n",
    "**🔗 Prompt Chaining** - Use when:\n",
    "- Tasks can be cleanly decomposed into sequential steps\n",
    "- Each step benefits from focused attention\n",
    "- Quality is more important than latency\n",
    "- You need programmatic validation gates\n",
    "- Examples: Content generation → review → translation → cultural adaptation\n",
    "\n",
    "**📍 Routing** - Use when:\n",
    "- Input types have distinct handling requirements  \n",
    "- Specialized expertise improves outcomes significantly\n",
    "- Classification can be performed reliably\n",
    "- Different cost/performance tradeoffs exist per route\n",
    "- Examples: Customer service triage, query complexity routing\n",
    "\n",
    "**⚡ Parallelization** - Use when:\n",
    "- **Sectioning**: Independent subtasks can run simultaneously\n",
    "- **Voting**: Multiple perspectives improve decision confidence\n",
    "- Latency reduction is critical\n",
    "- Ensemble methods provide measurable accuracy gains\n",
    "- Examples: Multi-aspect analysis, content moderation, code review\n",
    "\n",
    "**🎯 Orchestrator-Workers** - Use when:\n",
    "- Task requirements can't be predicted in advance\n",
    "- Dynamic subtask generation is needed\n",
    "- Different specialists handle different aspects\n",
    "- Complex coordination is required\n",
    "- Examples: Software development, research synthesis, creative projects\n",
    "\n",
    "**🔄 Evaluator-Optimizer** - Use when:\n",
    "- Iterative refinement demonstrably improves quality\n",
    "- Clear evaluation criteria exist\n",
    "- The LLM can provide meaningful self-criticism\n",
    "- Quality improvement justifies additional latency\n",
    "- Examples: Creative writing, complex analysis, strategic planning\n",
    "\n",
    "**🤖 Autonomous Agents** - Use when:\n",
    "- Open-ended problems with unpredictable steps\n",
    "- Long-running tasks requiring persistence\n",
    "- Environment interaction and feedback loops exist\n",
    "- Human oversight can be incorporated at checkpoints\n",
    "- Trust level supports autonomous operation\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "1. **Start Simple**: Begin with the simplest pattern that meets requirements\n",
    "2. **Measure Performance**: Always evaluate accuracy, latency, and cost tradeoffs\n",
    "3. **Error Handling**: Implement robust error recovery and fallback strategies\n",
    "4. **Human Oversight**: Include checkpoints for critical decisions\n",
    "5. **Composability**: Patterns can be combined for sophisticated workflows\n",
    "6. **Tool Design**: Invest heavily in clear, well-documented tool interfaces\n",
    "7. **Testing**: Extensive testing in sandboxed environments before production\n",
    "\n",
    "\n",
    "### Memory Systems Quick Reference\n",
    "\n",
    "Now that we've seen memory systems in action, here's a practical guide for choosing the right approach:\n",
    "\n",
    "| Memory Type | Best Use Case | Pros | Cons | Complexity |\n",
    "|-------------|---------------|------|------|------------|\n",
    "| **ConversationBufferMemory** | Short, detail-critical conversations | Perfect recall, simple setup | Linear cost growth, token limits | O(n) |\n",
    "| **ConversationSummaryMemory** | Long-term relationships, key themes | Scales indefinitely, preserves important info | Loses detail, summarization overhead | O(log n) |\n",
    "| **ConversationBufferWindowMemory** | Task-oriented, recent context matters | Predictable performance, constant cost | Forgets older context completely | O(k) |\n",
    "| **ConversationTokenBufferMemory** | Production apps, cost control | Optimal context usage, never exceeds limits | Complex token counting logic | O(tokens) |\n",
    "| **ConversationEntityMemory** | Relationship tracking, complex scenarios | Maintains entity relationships, intelligent context | Requires entity extraction, higher complexity | O(entities) |\n",
    "| **CombinedMemory** | Sophisticated applications | Leverages multiple approaches, flexible | Complex setup, coordination overhead | O(combined) |\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "- 📝 **Need perfect recall?** → Buffer Memory\n",
    "- 🔄 **Long conversations?** → Summary Memory  \n",
    "- ⚡ **Recent context only?** → Window Memory\n",
    "- 💰 **Cost control critical?** → Token Memory\n",
    "- 👥 **Tracking relationships?** → Entity Memory\n",
    "- 🧠 **Multiple requirements?** → Combined Memory\n",
    "\n",
    "**Memory Performance Characteristics:**\n",
    "- **Buffer**: Grows with conversation length - great for short, detailed discussions\n",
    "- **Summary**: Logarithmic growth - ideal for ongoing relationships  \n",
    "- **Window**: Constant size - perfect for task-focused interactions\n",
    "- **Token**: Bounded growth - excellent for production cost control\n",
    "- **Entity**: Scales with entities - powerful for complex relationship tracking\n",
    "- **Combined**: Flexible scaling - adaptable to diverse requirements\n",
    "\n",
    "\n",
    "**🎯 Key Takeaways:**\n",
    "\n",
    "**1. Chunking vs Splitting:**\n",
    "- **Splitting** = Breaking documents at boundaries\n",
    "- **Chunking** = Engineering optimal retrieval units\n",
    "- Chunking is about **semantic coherence and retrieval quality**\n",
    "\n",
    "**2. Framework Choice:**\n",
    "\n",
    "| Scenario | Framework | Why |\n",
    "|----------|-----------|-----|\n",
    "| Simple RAG, getting started | **LangChain** | Easier, faster, flexible |\n",
    "| Production with basic needs | **LangChain** | Battle-tested, well-documented |\n",
    "| Complex hierarchies needed | **LlamaIndex** | Built-in parent-child relationships |\n",
    "| Semantic boundaries critical | **LlamaIndex** | SemanticSplitterNodeParser |\n",
    "| Mixed document types | **Both** | Use appropriate tool per document |\n",
    "\n",
    "**3. Optimal Chunk Sizes:**\n",
    "\n",
    "$$optimal\\\\_size = f(embedding\\\\_model, document\\\\_type, retrieval\\\\_precision)$$\n",
    "\n",
    "General guidelines:\n",
    "- **100-200 tokens**: High precision, may lack context\n",
    "- **200-500 tokens**: ⭐ **SWEET SPOT** for most cases\n",
    "- **500-1000 tokens**: More context, lower precision\n",
    "- **Overlap**: 10-20% of chunk size\n",
    "\n",
    "**4. Chunking Strategies Ranking:**\n",
    "\n",
    "| Strategy | Complexity | Quality | Use When |\n",
    "|----------|------------|---------|----------|\n",
    "| Fixed-size | ⭐ | ⭐⭐⭐ | Baseline, simple docs |\n",
    "| Sentence-based | ⭐⭐ | ⭐⭐⭐⭐ | Natural language, Q&A |\n",
    "| Semantic (structure) | ⭐⭐⭐ | ⭐⭐⭐⭐ | Structured documents |\n",
    "| Context-enriched | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | Production systems |\n",
    "| Semantic (embedding) | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | Maximum quality, have resources |\n",
    "| Hierarchical | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | Complex documents |\n",
    "\n",
    "**5. Advanced Techniques:**\n",
    "\n",
    "```\n",
    "Basic → Sentence → Semantic → Hierarchical → Semantic+Hierarchical\n",
    "                                              (Production-Ready)\n",
    "```\n",
    "\n",
    "**Mathematical Foundations:**\n",
    "\n",
    "**Chunk Quality Score:**\n",
    "$$Q(chunk) = \\\\alpha \\\\cdot coherence(chunk) + \\\\beta \\\\cdot size\\\\_optimality(chunk) + \\\\gamma \\\\cdot context\\\\_preservation(chunk)$$\n",
    "\n",
    "**Semantic Coherence:**\n",
    "$$coherence(chunk) = \\\\frac{\\\\sum_{i,j} sim(sent_i, sent_j)}{n(n-1)/2}$$\n",
    "\n",
    "**Retrieval Effectiveness:**\n",
    "$$effectiveness = \\\\frac{precision \\\\times recall}{storage\\\\_cost \\\\times compute\\\\_cost}$$\n",
    "\n",
    "**6. Production Checklist:**\n",
    "\n",
    "✅ **Tested chunk sizes** on your specific data  \n",
    "✅ **Measured retrieval quality** (precision/recall)  \n",
    "✅ **Added metadata** (source, section, timestamps)  \n",
    "✅ **Implemented overlap** for context continuity  \n",
    "✅ **Considered hierarchies** for complex documents  \n",
    "✅ **Monitored chunk size distribution**  \n",
    "✅ **Documented configuration** for reproducibility  \n",
    "✅ **Set up A/B testing** for optimization  \n",
    "\n",
    "**7. Common Pitfalls to Avoid:**\n",
    "\n",
    "❌ Using same chunk size for all document types  \n",
    "❌ No overlap (loses boundary context)  \n",
    "❌ Ignoring document structure  \n",
    "❌ Not testing retrieval quality  \n",
    "❌ Chunks too large (poor precision)  \n",
    "❌ Chunks too small (insufficient context)  \n",
    "❌ Missing metadata enrichment  \n",
    "❌ No monitoring/metrics  \n",
    "\n",
    "**8. Next Steps:**\n",
    "\n",
    "Now that you have optimal chunks, the next step is **embedding** them into vector representations for semantic search. We'll cover:\n",
    "\n",
    "- Embedding models (OpenAI, HuggingFace, etc.)\n",
    "- Embedding dimensions and trade-offs\n",
    "- Batch embedding strategies\n",
    "- Embedding caching and optimization\n",
    "- Multi-representation embeddings\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Key Takeaways from Text Splitting:**\n",
    "\n",
    "**🎯 Default Choice:**\n",
    "- **RecursiveCharacterTextSplitter** works for 80% of cases\n",
    "- Adaptive, efficient, minimal configuration needed\n",
    "- Respects natural text boundaries\n",
    "\n",
    "**📏 Optimal Chunk Sizes:**\n",
    "- **Small (50-200 tokens)**: Precise but may lack context\n",
    "- **Medium (200-500 tokens)**: ⭐ **RECOMMENDED** - best balance\n",
    "- **Large (500-1000 tokens)**: More context, less precise\n",
    "- **Overlap**: 10-20% of chunk size for context continuity\n",
    "\n",
    "**🔧 Specialized Splitters:**\n",
    "| Document Type | Splitter | Why |\n",
    "|---------------|----------|-----|\n",
    "| Code | `RecursiveCharacterTextSplitter.from_language()` | Syntax-aware, preserves functions/classes |\n",
    "| Markdown | `MarkdownHeaderTextSplitter` | Structure preservation, rich metadata |\n",
    "| HTML | `HTMLHeaderTextSplitter` | Clean extraction, hierarchy preservation |\n",
    "| LaTeX/Academic | `LatexTextSplitter` | Equation preservation |\n",
    "| Token-constrained | `TokenTextSplitter` | Exact token control for LLMs |\n",
    "\n",
    "**💡 Production Tips:**\n",
    "1. **Test multiple strategies** with your specific data\n",
    "2. **Measure retrieval quality** - adjust based on results\n",
    "3. **Use hierarchical splitting** for structured documents\n",
    "4. **Add 10-20% overlap** to preserve context\n",
    "5. **Store metadata** (source, section, hierarchy)\n",
    "6. **Consider multi-representation** for flexibility\n",
    "7. **Monitor chunk size distribution** - aim for consistency\n",
    "8. **Document your configuration** for reproducibility\n",
    "\n",
    "**⚠️ Common Pitfalls:**\n",
    "- ❌ Using same chunk size for all document types\n",
    "- ❌ No overlap (loses boundary context)\n",
    "- ❌ Too large chunks (poor precision)\n",
    "- ❌ Too small chunks (insufficient context)\n",
    "- ❌ Ignoring document structure\n",
    "- ❌ Not testing retrieval quality\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Now that we have optimally split documents, we'll explore **chunking strategies** and **embedding** techniques to transform these text chunks into vector representations for semantic search.\n",
    "\n",
    "**🔥 Pro Tip**: Start simple (LangChain + fixed-size), measure quality, then optimize. Don't over-engineer early!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**When to use:**\n",
    "- Significant domain-specific vocabulary\n",
    "- Generic models perform poorly on your data\n",
    "- Have domain-specific training data\n",
    "\n",
    "### Choosing the Right Embedding Model\n",
    "\n",
    "**Decision matrix:**\n",
    "\n",
    "| Use Case | Model Type | Example | Dimensions |\n",
    "|----------|------------|---------|------------|\n",
    "| General RAG, good balance | Dense (medium) | all-mpnet-base-v2 | 768 |\n",
    "| Fast retrieval, limited resources | Dense (small) | all-MiniLM-L6-v2 | 384 |\n",
    "| Best accuracy, have compute | Dense (large) | bge-large-en-v1.5 | 1024 |\n",
    "| Keyword-heavy domain | Sparse (BM25) | rank_bm25 | Variable |\n",
    "| Production system | Hybrid | Dense + BM25 | Both |\n",
    "| Need reranking | Cross-encoder | ms-marco-MiniLM | N/A |\n",
    "| Medical/Legal/Code | Domain-specific | BioBERT/CodeBERT | Varies |\n",
    "| Multi-language | Multilingual | multilingual-e5 | 768 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6403629",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de317b91",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d2156",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
