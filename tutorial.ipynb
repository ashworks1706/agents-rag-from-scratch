{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36be9c2",
   "metadata": {},
   "source": [
    "# Agents and RAG, A Technical Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389560b9",
   "metadata": {},
   "source": [
    "In this notebook, i'll be using the Lang and Llama family for building and exploring RAG from scratch and the techniques we can do with Agents\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/awan_getting_langchain_ecosystem_1-1024x574.png\" width=700>\n",
    "\n",
    "<img src=\"https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2023/10/12015949/LlamaIndex.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb5de",
   "metadata": {},
   "source": [
    "### Brief History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4eff09",
   "metadata": {},
   "source": [
    "The concept of intelligent agents has evolved dramatically over the past seven decades, transforming from simple rule-based systems to today's sophisticated AI companions that can reason, plan, and act autonomously. Understanding this progression is essential because it helps us appreciate why modern agentic systems represent such a significant breakthrough and why they're becoming central to how we build AI applications. The journey began in the 1950s when researchers like Allen Newell and Herbert Simon created the Logic Theorist, a program that could prove mathematical theorems by exploring different logical paths. These early agents were like skilled craftsmen‚Äîthey could perform specific tasks very well, but only within narrow, pre-defined domains. The 1970s and 1980s brought expert systems like MYCIN for medical diagnosis and DENDRAL for chemical analysis. While impressive, these systems required months of manual knowledge engineering, where human experts had to explicitly encode their domain knowledge into rigid rule sets.\n",
    "\n",
    "The 1990s marked a shift toward more flexible software agents that could operate in networked environments and coordinate with other agents. This period introduced the concept of multi-agent systems, where multiple specialized agents could collaborate to solve complex problems. However, these systems still required extensive manual programming and could only handle situations their creators had anticipated. The real transformation began in the 2000s with machine learning advances. Agents could now learn from data rather than relying solely on hand-coded rules. Virtual assistants like Siri and Alexa brought agent technology to mainstream consumers, though they remained relatively narrow in scope‚Äîessentially sophisticated voice interfaces for search and simple task execution.\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*Ygen57Qiyrc8DXAFsjZLNA.gif\" width=700>\n",
    "\n",
    "The breakthrough moment arrived with large language models starting around 2020. Systems like GPT-3 and GPT-4 combined vast knowledge with sophisticated reasoning abilities, creating agents that could understand natural language, maintain context across conversations, and tackle a wide variety of tasks without task-specific programming. Unlike their predecessors, these modern agents can break down complex problems into steps, use external tools when needed, and adapt to new situations they've never encountered before. This evolution represents a fundamental shift from automation to augmentation. Where early agents automated specific, predefined tasks, today's agents can understand our goals and work as collaborative partners in problem-solving. They can handle ambiguous instructions, incomplete information, and constantly changing contexts‚Äîcapabilities that make them invaluable for building sophisticated applications like retrieval-augmented generation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0100815",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b4ae",
   "metadata": {},
   "source": [
    "When we talk about agents in 2025, we're entering a landscape where the term has become both ubiquitous and somewhat ambiguous. Different organizations and researchers use \"agent\" to describe everything from simple chatbots to fully autonomous systems that can operate independently for weeks. This diversity in definition isn't just academic‚Äîit reflects fundamentally different architectural approaches that will determine how we build the next generation of AI applications.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!A_Oy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3177e12-432e-4e41-814f-6febf7a35f68_1360x972.png\" width=700>\n",
    "\n",
    "At its core, an agent is a system that can perceive its environment, make decisions, and take actions to achieve specific goals. However, the way these capabilities are implemented varies dramatically. Some define agents as fully autonomous systems that operate independently over extended periods, using various tools and adapting their strategies based on feedback. Think of these like a personal assistant who can manage your entire schedule, book flights, handle emails, and make decisions on your behalf without constant supervision.\n",
    "\n",
    "Others use the term more broadly to describe any system that follows predefined workflows to accomplish tasks. These implementations are more like following a detailed recipe‚Äîeach step is predetermined, and while the system can handle some variations, it operates within clearly defined boundaries. The distinction between these approaches is crucial because it affects everything from system reliability to development complexity.\n",
    "\n",
    "The most useful way to think about this spectrum is through the lens of control and decision-making. Workflows are systems where large language models and tools are orchestrated through predefined code paths. Every decision point is anticipated by the developer, and the system follows predetermined logic to handle different scenarios. Agents, in contrast, are systems where the LLM dynamically directs its own processes and tool usage, maintaining control over how it accomplishes tasks. The model itself decides what to do next, which tools to use, and how to adapt when things don't go as planned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069523fb",
   "metadata": {},
   "source": [
    "#### Simplicity defines perfectionism not complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db57831",
   "metadata": {},
   "source": [
    "\n",
    "When building applications with LLMs, the fundamental principle should be finding the simplest solution that meets your requirements. This might mean not building agentic systems at all. Agentic systems inherently trade latency and cost for better task performance, and you need to carefully consider when this tradeoff makes sense for your specific use case.\n",
    "\n",
    "When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks where you can anticipate most scenarios and edge cases. They're excellent for standardized processes like data processing pipelines, content moderation, or structured analysis tasks. Agents become the better choice when you need flexibility and model-driven decision-making at scale‚Äîsituations where the variety of inputs and required responses is too broad to predefine, or where the system needs to adapt to entirely new scenarios.\n",
    "\n",
    "The reality is that for many applications, the most effective approach involves optimizing single LLM calls with retrieval and in-context examples rather than building complex agentic systems. However, as we'll explore throughout this tutorial, there are compelling scenarios where the additional complexity of agents becomes not just beneficial, but necessary for achieving your goals. Understanding when and how to make this transition is what separates effective AI system builders from those who over-engineer solutions to problems that could be solved more simply.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bd7ab",
   "metadata": {},
   "source": [
    "#### Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb1b8e",
   "metadata": {},
   "source": [
    "Prompts are the fundamental interface between human intent and AI capabilities, serving as the bridge that translates our natural language requests into structured instructions that language models can understand and act upon. In the context of agentic systems, prompts become even more critical because they not only convey what we want the agent to accomplish, but also how the agent should approach problem-solving, what tools it can use, and how it should reason through complex tasks.\n",
    "\n",
    "Think of prompts as the instruction manual for your AI agent‚Äîjust as a well-written manual can make the difference between a novice successfully assembling furniture or ending up with a pile of confused parts, a well-crafted prompt determines whether your agent performs brilliantly or struggles to understand your intent. The quality and structure of your prompts directly influence the agent's reasoning capabilities, tool usage patterns, and overall effectiveness in completing tasks.\n",
    "\n",
    "<img src=\"https://www.datablist.com/_next/image?url=%2Fhowto_images%2Fhow-to-write-prompt-ai-agents%2Fstructured-ai-agent-prompt.png&w=3840&q=75\" width=700>\n",
    "\n",
    "There are several types of prompts that serve different purposes in agentic systems. System prompts establish the agent's role, personality, and fundamental operating principles‚Äîthese are like giving someone their job description and company handbook before they start work. User prompts contain the specific tasks or questions you want the agent to handle, while few-shot prompts provide examples of desired input-output patterns to guide the agent's responses. Chain-of-thought prompts encourage step-by-step reasoning, helping agents break down complex problems into manageable pieces.\n",
    "\n",
    "In multi-step agentic workflows, prompt engineering becomes particularly sophisticated because you need to design prompts that not only solve individual tasks but also coordinate between different stages of processing. The agent needs to understand when to use specific tools, how to interpret tool outputs, and how to maintain context across multiple interaction cycles. This requires careful consideration of prompt structure, token efficiency, and the logical flow of information through your system.\n",
    "\n",
    "Let's explore how to implement basic prompt templates using LangChain with Google's Gemini model to see these concepts in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# COMPREHENSIVE SETUP AND IMPORTS\n",
    "# ================================\n",
    "# This cell contains all imports and basic setup for the entire tutorial\n",
    "\n",
    "# Core LangChain and LLM imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Memory system imports\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory, \n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationEntityMemory,\n",
    "    CombinedMemory,\n",
    "    ReadOnlySharedMemory,\n",
    "    SimpleMemory\n",
    ")\n",
    "from langchain.memory.entity import InMemoryEntityStore\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Mathematical libraries for calculations\n",
    "import numpy as np\n",
    "\n",
    "# ================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Initialize primary LLM with balanced settings\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\", \n",
    "    temperature=0.3,  # Balanced creativity and consistency\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Global variables for the tutorial workflow\n",
    "tutorial_state = {\n",
    "    \"current_section\": \"setup\",\n",
    "    \"demo_data\": {},\n",
    "    \"conversation_history\": [],\n",
    "    \"skills_registry\": {},\n",
    "    \"memory_systems\": {}\n",
    "}\n",
    "\n",
    "print(\"üöÄ Agents and RAG Tutorial - Setup Complete\")\n",
    "print(\"üì¶ All imports loaded successfully\")\n",
    "print(\"üîß Global configuration initialized\")\n",
    "print(\"üìã Tutorial state tracking ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# PROMPTING FUNDAMENTALS\n",
    "# ================================\n",
    "# Demonstrate different prompt types and their effectiveness\n",
    "\n",
    "def create_prompt_examples():\n",
    "    \"\"\"Create various prompt templates for demonstration\"\"\"\n",
    "    \n",
    "    # Basic instructional prompt\n",
    "    basic_template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"audience\"],\n",
    "        template=\"\"\"You are an expert educator who excels at explaining complex topics clearly.\n",
    "        \n",
    "        Topic: {topic}\n",
    "        Audience: {audience}\n",
    "        \n",
    "        Please provide a clear, engaging explanation that includes:\n",
    "        1. Core concept definition\n",
    "        2. Relevant examples or analogies  \n",
    "        3. Key takeaways for the audience level\n",
    "        \n",
    "        Keep your explanation appropriate for the specified audience.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Conversational prompt with memory\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant with expertise in technology and science. \n",
    "        You provide accurate, clear explanations and engage in detailed discussions.\n",
    "        Always think step-by-step when solving problems and explain your reasoning.\"\"\"),\n",
    "        (\"human\", \"I need help understanding {concept}. Can you break it down for me?\"),\n",
    "        (\"ai\", \"I'd be happy to help explain {concept}! Let me break this down step by step.\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    \n",
    "    return basic_template, chat_template\n",
    "\n",
    "# Create prompt templates\n",
    "basic_template, chat_template = create_prompt_examples()\n",
    "\n",
    "# Create reusable chains using LangChain Expression Language (LCEL)\n",
    "basic_chain = basic_template | llm | StrOutputParser()\n",
    "chat_chain = chat_template | llm | StrOutputParser()\n",
    "\n",
    "# Store in tutorial state for later use\n",
    "tutorial_state[\"prompt_templates\"] = {\n",
    "    \"basic\": basic_template,\n",
    "    \"chat\": chat_template\n",
    "}\n",
    "\n",
    "tutorial_state[\"chains\"] = {\n",
    "    \"basic\": basic_chain,\n",
    "    \"chat\": chat_chain\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Prompt Engineering Components Ready\")\n",
    "print(\"üìù Basic and conversational templates created\") \n",
    "print(\"üîó LCEL chains initialized and stored in tutorial state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacbea9",
   "metadata": {},
   "source": [
    "Great! now our LLM can respond to our questions, but how can we tweak it more to determine how much it weighs the prompt guideline while responding with it's own knowledge and reasoning? let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c858ca",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d7ff9",
   "metadata": {},
   "source": [
    "Hyperparameters are the control knobs that determine how a language model generates responses, acting like the settings on a sophisticated instrument that can dramatically change the output quality and behavior. Understanding these parameters is crucial for building effective agents because they directly influence how the model balances following prompt instructions versus drawing on its pre-trained knowledge, how creative or conservative its responses are, and how consistently it behaves across multiple interactions.\n",
    "\n",
    "### Mathematical Foundation of Hyperparameters\n",
    "\n",
    "**Temperature (œÑ)** controls the randomness in the model's token selection process through the softmax function. Given logits $z_i$ for each possible token $i$, the probability distribution is calculated as:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{z_i/œÑ}}{\\sum_{j=1}^{V} e^{z_j/œÑ}}$$\n",
    "\n",
    "Where:\n",
    "- $œÑ$ (tau) is the temperature parameter\n",
    "- $V$ is the vocabulary size  \n",
    "- Lower $œÑ$ ‚Üí sharper distribution (more deterministic)\n",
    "- Higher $œÑ$ ‚Üí flatter distribution (more random)\n",
    "\n",
    "At $œÑ = 1$, we get the standard softmax. As $œÑ ‚Üí 0$, the distribution approaches a one-hot encoding of the highest logit. As $œÑ ‚Üí ‚àû$, the distribution becomes uniform.\n",
    "\n",
    "**Top-p (Nucleus Sampling)** works by selecting the smallest set of tokens whose cumulative probability exceeds threshold $p$:\n",
    "\n",
    "$$\\text{Nucleus} = \\{i : \\sum_{j \\in \\text{top-k tokens}} P(token_j) \\leq p\\}$$\n",
    "\n",
    "**Top-k** simply restricts consideration to the $k$ highest-probability tokens, where $k$ is a fixed integer.\n",
    "\n",
    "**Max tokens** provides an upper bound $N_{max}$ on sequence length, while **stop sequences** define termination conditions based on specific token patterns.\n",
    "\n",
    "Let's explore how these parameters affect model behavior in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# HYPERPARAMETER EXPERIMENTATION\n",
    "# ================================\n",
    "# Demonstrate how different hyperparameters affect model behavior\n",
    "\n",
    "def create_hyperparameter_variants():\n",
    "    \"\"\"Create LLM instances with different hyperparameter settings\"\"\"\n",
    "    \n",
    "    # Conservative configuration (low temperature)\n",
    "    conservative_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.1,  # œÑ = 0.1 for high determinism\n",
    "        max_tokens=150,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Balanced configuration  \n",
    "    balanced_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\", \n",
    "        temperature=0.7,  # œÑ = 0.7 for creativity-consistency balance\n",
    "        max_tokens=150,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Creative configuration (high temperature)\n",
    "    creative_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=1.2,  # œÑ = 1.2 for high creativity\n",
    "        max_tokens=150, \n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"conservative\": conservative_llm,\n",
    "        \"balanced\": balanced_llm, \n",
    "        \"creative\": creative_llm\n",
    "    }\n",
    "\n",
    "def test_hyperparameter_effects(topic=\"quantum computing\"):\n",
    "    \"\"\"Test how different hyperparameters affect responses\"\"\"\n",
    "    \n",
    "    llm_variants = create_hyperparameter_variants()\n",
    "    \n",
    "    # Shared prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=\"Explain {topic} in exactly three sentences. Be accurate but engaging.\"\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, llm_variant in llm_variants.items():\n",
    "        chain = prompt | llm_variant | StrOutputParser()\n",
    "        response = chain.invoke({\"topic\": topic})\n",
    "        results[config_name] = response\n",
    "        print(f\"\\n{config_name.upper()} (œÑ={llm_variant.temperature}):\")\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_instruction_adherence():\n",
    "    \"\"\"Test how temperature affects prompt instruction following\"\"\"\n",
    "    \n",
    "    instruction_prompt = PromptTemplate(\n",
    "        input_variables=[\"format\", \"content\"],\n",
    "        template=\"\"\"You must follow this format EXACTLY: {format}\n",
    "        \n",
    "        Content to format: {content}\n",
    "        \n",
    "        CRITICAL: Strict adherence to the format is required.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # High vs low temperature comparison\n",
    "    strict_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.0,  # Maximum determinism\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    flexible_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.9,  # More creativity\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\") \n",
    "    )\n",
    "    \n",
    "    strict_chain = instruction_prompt | strict_llm | StrOutputParser()\n",
    "    flexible_chain = instruction_prompt | flexible_llm | StrOutputParser()\n",
    "    \n",
    "    test_format = \"1. [Topic] 2. [Definition] 3. [Example]\"\n",
    "    test_content = \"Machine learning algorithms that improve through experience\"\n",
    "    \n",
    "    strict_result = strict_chain.invoke({\n",
    "        \"format\": test_format,\n",
    "        \"content\": test_content\n",
    "    })\n",
    "    \n",
    "    flexible_result = flexible_chain.invoke({\n",
    "        \"format\": test_format, \n",
    "        \"content\": test_content\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"strict_adherence\": strict_result,\n",
    "        \"flexible_interpretation\": flexible_result\n",
    "    }\n",
    "\n",
    "# Run hyperparameter demonstrations\n",
    "print(\"üß™ Testing Hyperparameter Effects\")\n",
    "hyperparameter_results = test_hyperparameter_effects()\n",
    "\n",
    "print(\"\\nüéØ Testing Instruction Adherence\")  \n",
    "adherence_results = test_instruction_adherence()\n",
    "\n",
    "# Store results in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"hyperparameters\"] = hyperparameter_results\n",
    "tutorial_state[\"demo_data\"][\"instruction_adherence\"] = adherence_results\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter experimentation complete\")\n",
    "print(\"üìä Results stored in tutorial_state for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347026a1",
   "metadata": {},
   "source": [
    "The examples above demonstrate how hyperparameters create a fundamental tradeoff between instruction following and creative knowledge application. Low temperature models excel at following precise formatting requirements and maintaining consistency across multiple calls, making them ideal for structured data extraction, API responses, and workflows where predictability is paramount. Higher temperature models bring more of the model's training knowledge into play, generating more diverse responses and creative solutions, but at the cost of strict instruction adherence.\n",
    "\n",
    "This balance becomes critical in agentic systems where you need to decide whether your agent should be a precise executor of specific instructions or a creative problem-solver that can adapt its approach based on context. The choice often depends on your use case: customer service bots might need low-temperature consistency, while creative writing assistants might benefit from higher-temperature diversity.\n",
    "\n",
    "Now that we understand how to control our model's behavior through prompts and hyperparameters, we need to give our agents the ability to extend beyond their base knowledge and interact with the world. This is where tools come into play - they're what transform a language model from a sophisticated text generator into an active agent that can perform real actions and access current information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df627673",
   "metadata": {},
   "source": [
    "### Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf486d",
   "metadata": {},
   "source": [
    "\n",
    "Tools are what transform language models from sophisticated text generators into active agents capable of performing real-world actions and accessing live information. Think of tools as the hands and senses of your AI agent - without them, even the most advanced language model is limited to working with only the knowledge it was trained on, which becomes stale the moment training ends. Tools bridge this gap by allowing agents to interact with databases, APIs, web services, file systems, and any other external systems your application needs to work with.\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGyFCaSY8w4Ag/article-cover_image-shrink_720_1280/B4DZYg8dDRHAAI-/0/1744309441965?e=1762992000&v=beta&t=NS3gCnYSTWkxVwnRpHX6tCG7wcXcGgEknNpowIVAo2k\" width=700>\n",
    "\n",
    "The fundamental concept behind tools in agentic systems is function calling (also known as tool calling). Modern language models like GPT-4, Claude, and Gemini have been specifically trained to understand when they need external information or capabilities, and can generate structured function calls with appropriate parameters. When an agent encounters a question about current weather, stock prices, or needs to perform calculations, it doesn't hallucinate an answer - instead, it recognizes the limitation and calls the appropriate tool.\n",
    "\n",
    "The tool execution process follows a predictable pattern: the agent receives a user request, analyzes what information or actions are needed, determines which tools to use, formats the tool calls with proper parameters, executes the tools, receives the results, and then synthesizes a response using both its knowledge and the tool outputs. This creates a powerful feedback loop where agents can chain multiple tool calls together, use the output of one tool as input to another, and dynamically adapt their approach based on intermediate results.\n",
    "\n",
    "There are three main categories of tools we'll explore: **built-in tools** that come pre-integrated with language model providers, **explicit tools** that you define and implement yourself, and **Model Context Protocol (MCP) tools** that provide standardized interfaces for complex integrations. Each category serves different purposes and offers varying levels of customization and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233785",
   "metadata": {},
   "source": [
    "#### Built-in Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b29da",
   "metadata": {},
   "source": [
    "\n",
    "Built-in tools are native capabilities provided directly by language model providers, eliminating the need for external integrations or custom implementations. Google's Gemini models, for example, come with several powerful built-in tools including Google Search integration, code execution capabilities, and mathematical computation tools. These tools are particularly valuable because they're optimized for the specific model, have minimal latency overhead, and don't require additional API keys or setup beyond your primary model access.\n",
    "\n",
    "The advantage of built-in tools is their seamless integration - the model provider handles all the complexity of tool execution, result formatting, and error handling. When you enable Google Search for Gemini, the model can perform web searches and incorporate real-time information directly into its responses without any additional code on your part. Similarly, the code execution tool allows Gemini to write and run Python code in a sandboxed environment, making it excellent for data analysis, mathematical calculations, and generating visualizations.\n",
    "\n",
    "Let's explore how to use Gemini's built-in tools with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# BUILT-IN TOOLS DEMONSTRATION  \n",
    "# ================================\n",
    "# Showcase Google Gemini's native tool capabilities\n",
    "\n",
    "def create_builtin_tool_agents():\n",
    "    \"\"\"Create agents with different built-in tool configurations\"\"\"\n",
    "    \n",
    "    # Agent with Google Search integration\n",
    "    search_agent = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.3,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        tools=[\"google_search_retrieval\"]\n",
    "    )\n",
    "    \n",
    "    # Agent with code execution capability\n",
    "    code_agent = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\", \n",
    "        temperature=0.1,  # Lower temperature for code reliability\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        tools=[\"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    # Agent with multiple built-in tools\n",
    "    multi_tool_agent = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.4,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        tools=[\"google_search_retrieval\", \"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"search_agent\": search_agent,\n",
    "        \"code_agent\": code_agent, \n",
    "        \"multi_tool_agent\": multi_tool_agent\n",
    "    }\n",
    "\n",
    "def test_builtin_tools():\n",
    "    \"\"\"Test various built-in tool capabilities\"\"\"\n",
    "    \n",
    "    agents = create_builtin_tool_agents()\n",
    "    \n",
    "    # Test prompts for different tool types\n",
    "    search_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can search for current information when needed.\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    code_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can execute Python code for calculations and analysis.\"),\n",
    "        (\"human\", \"{analysis_request}\")\n",
    "    ])\n",
    "    \n",
    "    multi_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Use search for current info and code execution for calculations.\"),\n",
    "        (\"human\", \"{complex_query}\")\n",
    "    ])\n",
    "    \n",
    "    # Create chains\n",
    "    search_chain = search_prompt | agents[\"search_agent\"] | StrOutputParser()\n",
    "    code_chain = code_prompt | agents[\"code_agent\"] | StrOutputParser()\n",
    "    multi_chain = multi_prompt | agents[\"multi_tool_agent\"] | StrOutputParser()\n",
    "    \n",
    "    # Test queries\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Search capability test\n",
    "        search_result = search_chain.invoke({\n",
    "            \"query\": \"Latest developments in AI safety research 2024\"\n",
    "        })\n",
    "        results[\"search_test\"] = search_result[:300] + \"...\"\n",
    "        \n",
    "        # Code execution test  \n",
    "        code_result = code_chain.invoke({\n",
    "            \"analysis_request\": \"\"\"\n",
    "            Analyze this sales data: [120, 150, 180, 95, 200, 175, 160, 140, 190, 210]\n",
    "            Calculate mean, median, standard deviation, and identify outliers.\n",
    "            \"\"\"\n",
    "        })\n",
    "        results[\"code_test\"] = code_result[:300] + \"...\"\n",
    "        \n",
    "        # Multi-tool test\n",
    "        multi_result = multi_chain.invoke({\n",
    "            \"complex_query\": \"\"\"\n",
    "            Research current AI market cap data for top 3 companies in 2024,\n",
    "            then calculate what percentage each represents of total market cap.\n",
    "            \"\"\"\n",
    "        })\n",
    "        results[\"multi_tool_test\"] = multi_result[:300] + \"...\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = f\"Tool execution error: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute built-in tools demonstration\n",
    "print(\"üîß Testing Built-in Tool Capabilities\")\n",
    "builtin_results = test_builtin_tools()\n",
    "\n",
    "for test_name, result in builtin_results.items():\n",
    "    print(f\"\\n{test_name.upper()}:\")\n",
    "    print(result)\n",
    "\n",
    "# Store in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"builtin_tools\"] = builtin_results\n",
    "tutorial_state[\"current_section\"] = \"builtin_tools\"\n",
    "\n",
    "print(\"\\n‚úÖ Built-in tools demonstration complete\")\n",
    "print(\"üè™ Tool results stored in tutorial state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75c830",
   "metadata": {},
   "source": [
    "#### Explicit Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a351b4",
   "metadata": {},
   "source": [
    "While built-in tools provide excellent out-of-the-box functionality, the real power of agentic systems comes from creating custom tools tailored to your specific use case. Explicit tools are functions you define and implement yourself, giving agents the ability to interact with your databases, APIs, business logic, or any other systems your application requires. This is where agents transform from general-purpose assistants into specialized experts for your domain.\n",
    "\n",
    "The process of creating explicit tools involves defining the tool's interface (what parameters it accepts and what it returns), implementing the actual functionality, and then registering the tool with your agent framework. LangChain makes this process straightforward through its `@tool` decorator and `Tool` class, which handle the integration details while letting you focus on the business logic.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/1*fu9Lu8D8DLnVFPAWg7N0jQ.png\" width=700>\n",
    "\n",
    "When designing explicit tools, it's important to think about granularity and composability. Rather than creating one massive tool that does everything, it's better to create focused tools that do one thing well and can be combined. For example, instead of a single \"manage_database\" tool, you might create separate \"query_user\", \"update_inventory\", and \"calculate_metrics\" tools that can work together.\n",
    "\n",
    "Let's explore how to create and use explicit tools with LangChain and Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# EXPLICIT TOOLS IMPLEMENTATION\n",
    "# ================================\n",
    "# Create custom tools for specific business logic and integrations\n",
    "\n",
    "def create_custom_tools():\n",
    "    \"\"\"Define custom tools using @tool decorator for explicit functionality\"\"\"\n",
    "    \n",
    "    @tool\n",
    "    def get_weather(city: str, country: str = \"US\") -> str:\n",
    "        \"\"\"\n",
    "        Get current weather information for a specified city.\n",
    "        \n",
    "        Args:\n",
    "            city: The name of the city to get weather for\n",
    "            country: The country code (default: US)\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with weather information\n",
    "        \"\"\"\n",
    "        # Simulate weather API call - replace with real API in production\n",
    "        weather_conditions = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\", \"partly cloudy\"]\n",
    "        temperature = random.randint(-10, 35)\n",
    "        condition = random.choice(weather_conditions)\n",
    "        \n",
    "        weather_data = {\n",
    "            \"city\": city,\n",
    "            \"country\": country,\n",
    "            \"temperature\": temperature,\n",
    "            \"condition\": condition,\n",
    "            \"humidity\": random.randint(30, 90),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return json.dumps(weather_data, indent=2)\n",
    "\n",
    "    @tool\n",
    "    def calculate_compound_interest(principal: float, rate: float, time: int, compounds_per_year: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Calculate compound interest using the formula: A = P(1 + r/n)^(nt)\n",
    "        \n",
    "        Mathematical Foundation:\n",
    "        A = P(1 + r/n)^(nt)\n",
    "        Where:\n",
    "        - A = final amount\n",
    "        - P = principal (initial investment) \n",
    "        - r = annual interest rate (as decimal)\n",
    "        - n = number of times interest compounds per year\n",
    "        - t = time in years\n",
    "        \n",
    "        Args:\n",
    "            principal: Initial investment amount\n",
    "            rate: Annual interest rate (as decimal, e.g., 0.05 for 5%)\n",
    "            time: Number of years\n",
    "            compounds_per_year: Compounding frequency (default: 1)\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string with calculation details\n",
    "        \"\"\"\n",
    "        # Apply compound interest formula\n",
    "        amount = principal * (1 + rate/compounds_per_year) ** (compounds_per_year * time)\n",
    "        interest_earned = amount - principal\n",
    "        \n",
    "        result = {\n",
    "            \"principal\": principal,\n",
    "            \"annual_rate\": f\"{rate*100}%\", \n",
    "            \"time_years\": time,\n",
    "            \"compounds_per_year\": compounds_per_year,\n",
    "            \"final_amount\": round(amount, 2),\n",
    "            \"interest_earned\": round(interest_earned, 2),\n",
    "            \"total_return_percentage\": round((interest_earned/principal)*100, 2)\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "\n",
    "    @tool  \n",
    "    def search_user_database(query: str, user_type: str = \"all\") -> str:\n",
    "        \"\"\"\n",
    "        Search a simulated user database for customer information.\n",
    "        \n",
    "        Args:\n",
    "            query: Search term (name, email, or ID)\n",
    "            user_type: Filter by user type - \"premium\", \"basic\", or \"all\"\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with user information\n",
    "        \"\"\"\n",
    "        # Mock database - replace with actual database queries in production\n",
    "        mock_users = [\n",
    "            {\"id\": \"001\", \"name\": \"Alice Johnson\", \"email\": \"alice@email.com\", \"type\": \"premium\", \"status\": \"active\"},\n",
    "            {\"id\": \"002\", \"name\": \"Bob Smith\", \"email\": \"bob@email.com\", \"type\": \"basic\", \"status\": \"active\"}, \n",
    "            {\"id\": \"003\", \"name\": \"Carol Davis\", \"email\": \"carol@email.com\", \"type\": \"premium\", \"status\": \"inactive\"},\n",
    "            {\"id\": \"004\", \"name\": \"David Wilson\", \"email\": \"david@email.com\", \"type\": \"basic\", \"status\": \"active\"}\n",
    "        ]\n",
    "        \n",
    "        # Apply user type filter\n",
    "        if user_type != \"all\":\n",
    "            mock_users = [user for user in mock_users if user[\"type\"] == user_type]\n",
    "        \n",
    "        # Search logic with fuzzy matching\n",
    "        results = []\n",
    "        query_lower = query.lower()\n",
    "        for user in mock_users:\n",
    "            if (query_lower in user[\"name\"].lower() or \n",
    "                query_lower in user[\"email\"].lower() or \n",
    "                query_lower == user[\"id\"]):\n",
    "                results.append(user)\n",
    "        \n",
    "        return json.dumps({\"query\": query, \"results\": results}, indent=2)\n",
    "    \n",
    "    return [get_weather, calculate_compound_interest, search_user_database]\n",
    "\n",
    "def create_tool_agent(tools_list):\n",
    "    \"\"\"Create an agent executor with custom tools\"\"\"\n",
    "    \n",
    "    tool_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant with access to several specialized tools:\n",
    "        \n",
    "        üå§Ô∏è  get_weather: Get current weather for any city\n",
    "        üí∞ calculate_compound_interest: Calculate investment returns with compound interest\n",
    "        üë• search_user_database: Look up customer information in database\n",
    "        \n",
    "        Use these tools when needed to provide accurate, helpful responses.\n",
    "        Always explain which tool you're using and why.\n",
    "        Format JSON data nicely for users.\"\"\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    \n",
    "    agent = create_tool_calling_agent(llm, tools_list, tool_prompt)\n",
    "    \n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=tools_list, \n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "def test_explicit_tools():\n",
    "    \"\"\"Test the custom tools with various scenarios\"\"\"\n",
    "    \n",
    "    custom_tools = create_custom_tools()\n",
    "    tool_agent = create_tool_agent(custom_tools)\n",
    "    \n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Weather Query\",\n",
    "            \"input\": \"What's the weather like in Tokyo, Japan right now?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Financial Calculation\", \n",
    "            \"input\": \"If I invest $10,000 at 6% annual interest compounded monthly for 10 years, what will I have?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Database Search\",\n",
    "            \"input\": \"Can you find information about user Alice in our database?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-Tool Chain\",\n",
    "            \"input\": \"\"\"I need help with:\n",
    "            1. Weather in San Francisco\n",
    "            2. Find premium users named David \n",
    "            3. Calculate $5000 invested at 4.5% annually for 5 years\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\nüß™ Testing: {scenario['name']}\")\n",
    "        try:\n",
    "            response = tool_agent.invoke({\"input\": scenario[\"input\"]})\n",
    "            results[scenario[\"name\"]] = response[\"output\"]\n",
    "            print(f\"‚úÖ Success: {response['output'][:150]}...\")\n",
    "        except Exception as e:\n",
    "            results[scenario[\"name\"]] = f\"Error: {str(e)}\"\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    return results, custom_tools\n",
    "\n",
    "# Execute explicit tools demonstration\n",
    "print(\"üõ†Ô∏è  Creating Custom Tools\")\n",
    "explicit_results, custom_tools = test_explicit_tools()\n",
    "\n",
    "# Store in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"explicit_tools\"] = explicit_results\n",
    "tutorial_state[\"tools\"] = {\"custom_tools\": custom_tools}\n",
    "tutorial_state[\"current_section\"] = \"explicit_tools\"\n",
    "\n",
    "print(\"\\n‚úÖ Explicit tools implementation complete\")\n",
    "print(\"üéØ Custom tools integrated and tested successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3328",
   "metadata": {},
   "source": [
    "#### Model Context Protocol (MCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56fff",
   "metadata": {},
   "source": [
    "Model Context Protocol (MCP) represents the next evolution in AI tool integration, providing a standardized way for AI applications to securely connect to data sources and tools. Think of MCP as a universal translator that allows any AI system to communicate with any external service through a common protocol, eliminating the need for custom integrations for each tool or data source.\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/tweet_video_thumb/Gl7C44tXYAAdDSJ.jpg\" width=700>\n",
    "\n",
    "MCP was developed by Anthropic to solve the fragmentation problem in AI tool ecosystems. Before MCP, every AI application had to implement its own custom integrations for databases, APIs, file systems, and other external resources. This led to duplicated effort, security inconsistencies, and tools that only worked with specific AI platforms. MCP standardizes these interactions through a client-server architecture where MCP servers expose resources (like databases or file systems) and tools (like calculators or API clients) through a uniform interface.\n",
    "\n",
    "The protocol operates on JSON-RPC 2.0, enabling real-time, bidirectional communication between AI applications (MCP clients) and external resources (MCP servers). This means your agent can not only call tools but also receive real-time updates, notifications, and streaming data from external systems. The security model is built around explicit capability declarations and sandboxed execution, ensuring that agents can only access resources they've been explicitly granted permission to use.\n",
    "\n",
    "What makes MCP particularly powerful for RAG and agentic systems is its ability to provide **contextual data access**. Instead of just calling functions, MCP servers can expose rich contextual information about resources - like database schemas, file structures, or API capabilities - allowing agents to make more informed decisions about how to interact with external systems.\n",
    "\n",
    "Let's explore how to integrate MCP servers with LangChain and Gemini. For this example, we'll use the MCP SDK to create a simple server and then connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example demonstrates MCP concepts. In practice, you would install:\n",
    "# pip install mcp langchain-mcp\n",
    "\n",
    "# For now, we'll simulate MCP functionality to understand the concepts\n",
    "from typing import Any, Dict, List\n",
    "import json\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Simulate an MCP server interface\n",
    "@dataclass\n",
    "class MCPResource:\n",
    "    \"\"\"Represents a resource exposed by an MCP server\"\"\"\n",
    "    uri: str\n",
    "    name: str\n",
    "    description: str\n",
    "    mime_type: str\n",
    "\n",
    "@dataclass \n",
    "class MCPTool:\n",
    "    \"\"\"Represents a tool exposed by an MCP server\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: Dict[str, Any]\n",
    "\n",
    "class MockMCPServer:\n",
    "    \"\"\"Simulated MCP server for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.resources: List[MCPResource] = []\n",
    "        self.tools: List[MCPTool] = []\n",
    "        \n",
    "    def add_resource(self, resource: MCPResource):\n",
    "        self.resources.append(resource)\n",
    "        \n",
    "    def add_tool(self, tool: MCPTool):\n",
    "        self.tools.append(tool)\n",
    "        \n",
    "    def list_resources(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all available resources\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"uri\": r.uri,\n",
    "                \"name\": r.name, \n",
    "                \"description\": r.description,\n",
    "                \"mimeType\": r.mime_type\n",
    "            } for r in self.resources\n",
    "        ]\n",
    "        \n",
    "    def list_tools(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all available tools\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"name\": t.name,\n",
    "                \"description\": t.description,\n",
    "                \"inputSchema\": t.input_schema\n",
    "            } for t in self.tools\n",
    "        ]\n",
    "        \n",
    "    def read_resource(self, uri: str) -> str:\n",
    "        \"\"\"Read content from a resource\"\"\"\n",
    "        # Simulate resource reading\n",
    "        if \"customer_db\" in uri:\n",
    "            return json.dumps({\n",
    "                \"customers\": [\n",
    "                    {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\", \"tier\": \"gold\"},\n",
    "                    {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"tier\": \"silver\"}\n",
    "                ],\n",
    "                \"schema\": {\n",
    "                    \"id\": \"integer\",\n",
    "                    \"name\": \"string\", \n",
    "                    \"email\": \"string\",\n",
    "                    \"tier\": \"string\"\n",
    "                }\n",
    "            })\n",
    "        elif \"inventory\" in uri:\n",
    "            return json.dumps({\n",
    "                \"items\": [\n",
    "                    {\"sku\": \"A001\", \"name\": \"Laptop\", \"quantity\": 50, \"price\": 999.99},\n",
    "                    {\"sku\": \"A002\", \"name\": \"Mouse\", \"quantity\": 200, \"price\": 29.99}\n",
    "                ]\n",
    "            })\n",
    "        return \"Resource not found\"\n",
    "        \n",
    "    def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute a tool with given arguments\"\"\"\n",
    "        if tool_name == \"query_analytics\":\n",
    "            metric = arguments.get(\"metric\", \"sales\")\n",
    "            period = arguments.get(\"period\", \"month\")\n",
    "            return json.dumps({\n",
    "                \"metric\": metric,\n",
    "                \"period\": period,\n",
    "                \"value\": 150000 if metric == \"sales\" else 1200,\n",
    "                \"trend\": \"increasing\",\n",
    "                \"timestamp\": \"2024-10-22T10:00:00Z\"\n",
    "            })\n",
    "        elif tool_name == \"send_notification\":\n",
    "            return json.dumps({\n",
    "                \"status\": \"sent\",\n",
    "                \"recipient\": arguments.get(\"recipient\"),\n",
    "                \"message\": arguments.get(\"message\"),\n",
    "                \"delivery_id\": \"notify_12345\"\n",
    "            })\n",
    "        return json.dumps({\"error\": \"Tool not found\"})\n",
    "\n",
    "# Create a mock MCP server with business resources and tools\n",
    "business_mcp = MockMCPServer(\"business_system\")\n",
    "\n",
    "# Add resources (data sources the agent can read)\n",
    "business_mcp.add_resource(MCPResource(\n",
    "    uri=\"mcp://business/customer_db\",\n",
    "    name=\"Customer Database\",\n",
    "    description=\"Customer information and account details\", \n",
    "    mime_type=\"application/json\"\n",
    "))\n",
    "\n",
    "business_mcp.add_resource(MCPResource(\n",
    "    uri=\"mcp://business/inventory\",\n",
    "    name=\"Inventory System\", \n",
    "    description=\"Product inventory and stock levels\",\n",
    "    mime_type=\"application/json\"\n",
    "))\n",
    "\n",
    "# Add tools (actions the agent can perform)\n",
    "business_mcp.add_tool(MCPTool(\n",
    "    name=\"query_analytics\",\n",
    "    description=\"Query business analytics and metrics\",\n",
    "    input_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"metric\": {\"type\": \"string\", \"enum\": [\"sales\", \"users\", \"revenue\"]},\n",
    "            \"period\": {\"type\": \"string\", \"enum\": [\"day\", \"week\", \"month\", \"year\"]}\n",
    "        },\n",
    "        \"required\": [\"metric\"]\n",
    "    }\n",
    "))\n",
    "\n",
    "business_mcp.add_tool(MCPTool(\n",
    "    name=\"send_notification\", \n",
    "    description=\"Send notifications to users or systems\",\n",
    "    input_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"recipient\": {\"type\": \"string\"},\n",
    "            \"message\": {\"type\": \"string\"},\n",
    "            \"priority\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n",
    "        },\n",
    "        \"required\": [\"recipient\", \"message\"]\n",
    "    }\n",
    "))\n",
    "\n",
    "print(\"=== MCP Server Created ===\")\n",
    "print(f\"Server: {business_mcp.name}\")\n",
    "print(f\"Resources: {len(business_mcp.resources)}\")\n",
    "print(f\"Tools: {len(business_mcp.tools)}\")\n",
    "\n",
    "# List available resources and tools\n",
    "print(\"\\n=== Available Resources ===\")\n",
    "for resource in business_mcp.list_resources():\n",
    "    print(f\"- {resource['name']}: {resource['description']}\")\n",
    "    \n",
    "print(\"\\n=== Available Tools ===\") \n",
    "for tool in business_mcp.list_tools():\n",
    "    print(f\"- {tool['name']}: {tool['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain tools that interface with our MCP server\n",
    "# This demonstrates how MCP servers can be integrated into LangChain workflows\n",
    "\n",
    "@tool\n",
    "def mcp_read_resource(resource_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Read data from MCP server resources like databases or file systems.\n",
    "    \n",
    "    Args:\n",
    "        resource_name: Name of the resource to read (customer_db, inventory)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with resource data\n",
    "    \"\"\"\n",
    "    uri_map = {\n",
    "        \"customer_db\": \"mcp://business/customer_db\",\n",
    "        \"customers\": \"mcp://business/customer_db\", \n",
    "        \"inventory\": \"mcp://business/inventory\",\n",
    "        \"products\": \"mcp://business/inventory\"\n",
    "    }\n",
    "    \n",
    "    uri = uri_map.get(resource_name.lower())\n",
    "    if not uri:\n",
    "        return json.dumps({\"error\": f\"Resource '{resource_name}' not found\"})\n",
    "        \n",
    "    return business_mcp.read_resource(uri)\n",
    "\n",
    "@tool\n",
    "def mcp_query_analytics(metric: str, period: str = \"month\") -> str:\n",
    "    \"\"\"\n",
    "    Query business analytics through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        metric: The metric to query (sales, users, revenue)\n",
    "        period: Time period for the metric (day, week, month, year)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with analytics data\n",
    "    \"\"\"\n",
    "    return business_mcp.call_tool(\"query_analytics\", {\n",
    "        \"metric\": metric,\n",
    "        \"period\": period\n",
    "    })\n",
    "\n",
    "@tool  \n",
    "def mcp_send_notification(recipient: str, message: str, priority: str = \"medium\") -> str:\n",
    "    \"\"\"\n",
    "    Send notifications through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        recipient: Who to send the notification to\n",
    "        message: The notification message\n",
    "        priority: Priority level (low, medium, high)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with delivery confirmation\n",
    "    \"\"\"\n",
    "    return business_mcp.call_tool(\"send_notification\", {\n",
    "        \"recipient\": recipient,\n",
    "        \"message\": message,\n",
    "        \"priority\": priority\n",
    "    })\n",
    "\n",
    "# Create MCP-enabled tools list\n",
    "mcp_tools = [mcp_read_resource, mcp_query_analytics, mcp_send_notification]\n",
    "\n",
    "# Create an agent that can use MCP tools\n",
    "mcp_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "mcp_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a business intelligence assistant with access to company systems through MCP.\n",
    "    \n",
    "    Available MCP resources:\n",
    "    - customer_db: Customer information and account details\n",
    "    - inventory: Product inventory and stock levels\n",
    "    \n",
    "    Available MCP tools:\n",
    "    - mcp_query_analytics: Get business metrics and analytics\n",
    "    - mcp_send_notification: Send notifications to users or systems\n",
    "    - mcp_read_resource: Read data from company databases and systems\n",
    "    \n",
    "    Use these tools to provide comprehensive business insights and take actions when requested.\n",
    "    Always format data nicely and explain what you're doing.\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "mcp_agent = create_tool_calling_agent(mcp_llm, mcp_tools, mcp_prompt)\n",
    "mcp_executor = AgentExecutor(\n",
    "    agent=mcp_agent,\n",
    "    tools=mcp_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"=== MCP-Enabled Agent Created ===\")\n",
    "print(\"Agent ready with MCP server integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the MCP-enabled agent with business scenarios\n",
    "\n",
    "print(\"=== Test 1: Customer Data Analysis ===\")\n",
    "customer_analysis = mcp_executor.invoke({\n",
    "    \"input\": \"Can you analyze our customer data? I want to see the customer information and understand our customer tiers.\"\n",
    "})\n",
    "print(\"Response:\", customer_analysis['output'])\n",
    "\n",
    "print(\"\\n=== Test 2: Business Analytics ===\")\n",
    "analytics_query = mcp_executor.invoke({\n",
    "    \"input\": \"What were our sales metrics for this month? Also check user metrics.\"\n",
    "})\n",
    "print(\"Response:\", analytics_query['output'])\n",
    "\n",
    "print(\"\\n=== Test 3: Inventory Management ===\") \n",
    "inventory_check = mcp_executor.invoke({\n",
    "    \"input\": \"Check our current inventory levels and identify any products that might need restocking.\"\n",
    "})\n",
    "print(\"Response:\", inventory_check['output'])\n",
    "\n",
    "print(\"\\n=== Test 4: Complex Business Workflow ===\")\n",
    "complex_workflow = mcp_executor.invoke({\n",
    "    \"input\": \"\"\"I need a comprehensive business report:\n",
    "    1. Check our customer database for gold tier customers\n",
    "    2. Get our current sales metrics\n",
    "    3. Review inventory levels\n",
    "    4. If sales are good and we have low inventory, send a notification to 'inventory-team@company.com' about restocking\n",
    "    \n",
    "    Please provide a summary with actionable insights.\"\"\"\n",
    "})\n",
    "print(\"Response:\", complex_workflow['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56c6e2",
   "metadata": {},
   "source": [
    "The examples above demonstrate the power of tools in transforming language models into capable agents. We've seen how **built-in tools** provide immediate capabilities with minimal setup, **explicit tools** offer complete customization for your specific needs, and **MCP tools** enable standardized integration with complex systems while maintaining security and scalability.\n",
    "\n",
    "The key insight is that tools are what bridge the gap between language model intelligence and real-world utility. Without tools, even the most sophisticated language model is limited to generating text based on its training data. With tools, agents become active participants in your business processes, capable of querying databases, performing calculations, calling APIs, and taking actions in response to user needs.\n",
    "\n",
    "As we design agentic systems, the choice between different tool types depends on your specific requirements:\n",
    "- Use **built-in tools** when the model provider offers functionality that meets your needs\n",
    "- Create **explicit tools** when you need custom integration with your specific systems  \n",
    "- Implement **MCP tools** when you need standardized, scalable integrations across multiple AI applications\n",
    "\n",
    "Now that our agents can take actions in the world through tools, we need to ensure they can maintain context and remember information across interactions. This is where memory and context management become crucial for building agents that can handle complex, multi-step workflows and maintain coherent conversations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0edd85",
   "metadata": {},
   "source": [
    "### Context Management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7cd88",
   "metadata": {},
   "source": [
    "Context management is the cognitive backbone of sophisticated agents, determining how they maintain awareness of ongoing conversations, remember past interactions, and build upon previous knowledge to provide coherent, contextually relevant responses. Without proper context management, even the most capable agents become like individuals with severe short-term memory loss‚Äîthey might excel at individual tasks but fail to maintain meaningful, coherent interactions over time.\n",
    "\n",
    "Think of context management as the difference between having a conversation with a knowledgeable expert who remembers your entire discussion versus repeatedly starting fresh with someone who has no recollection of what you've already covered. The former builds understanding progressively, references earlier points, and adapts their communication based on your evolving needs. The latter, while potentially knowledgeable, forces you to repeat yourself and cannot build on the conversational foundation you've established.\n",
    "\n",
    "In agentic systems, context management becomes even more critical because agents need to coordinate information across multiple tool calls, maintain state during complex workflows, and remember important details that influence future decisions. An agent helping with financial planning needs to remember your risk tolerance, investment timeline, and previous decisions to provide consistent advice. A customer service agent should recall your account history, previous issues, and preferences to deliver personalized support.\n",
    "\n",
    "The challenge lies in balancing several competing factors: **memory capacity** (how much information can be retained), **relevance** (what information is most important to keep), **efficiency** (managing token limits and processing costs), and **persistence** (maintaining memory across sessions). Different memory strategies excel in different scenarios, and the best approach often involves combining multiple memory types to create a comprehensive context management system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b3a57",
   "metadata": {},
   "source": [
    "<img src=\"https://substackcdn.com/image/fetch/$s_!AyLS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0e3c002-0841-4d5f-9171-3eb63c321824_1600x1224.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592d71",
   "metadata": {},
   "source": [
    "Memory systems in agentic applications serve different purposes and have distinct strengths and limitations. Understanding these differences is crucial for selecting the right memory strategy for your specific use case. Let's explore the major categories of memory available in LangChain and how they can be effectively utilized.\n",
    "\n",
    "**Buffer-based memories** store raw conversation history up to certain limits, providing complete fidelity but consuming significant token space. **Summary-based memories** compress conversation history into concise summaries, trading some detail for efficiency. **Window-based memories** maintain only recent interactions, ensuring relevance while discarding older context. **Token-aware memories** dynamically manage content based on token consumption, balancing completeness with cost constraints.\n",
    "\n",
    "Each memory type excels in specific scenarios: use buffer memory for short conversations where every detail matters, summary memory for long-running sessions where themes and key decisions need tracking, window memory for task-oriented interactions where only recent context is relevant, and token buffer memory for cost-sensitive applications with unpredictable conversation lengths.\n",
    "\n",
    "Let's implement and compare these different memory systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# MEMORY SYSTEMS COMPREHENSIVE DEMO\n",
    "# ================================\n",
    "# Demonstrate different memory strategies and their mathematical foundations\n",
    "\n",
    "class MemorySystemsDemo:\n",
    "    \"\"\"Comprehensive demonstration of different memory strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.memory_systems = {}\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def create_all_memory_types(self):\n",
    "        \"\"\"Initialize all memory system types\"\"\"\n",
    "        \n",
    "        # 1. ConversationBufferMemory - Complete history storage\n",
    "        # Memory complexity: O(n) where n = total messages\n",
    "        buffer_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # 2. ConversationSummaryMemory - Compressed summaries\n",
    "        # Memory complexity: O(log n) with periodic compression\n",
    "        summary_memory = ConversationSummaryMemory(\n",
    "            llm=self.llm,\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # 3. ConversationBufferWindowMemory - Sliding window \n",
    "        # Memory complexity: O(k) where k = window size\n",
    "        window_memory = ConversationBufferWindowMemory(\n",
    "            k=3,  # Keep last 3 conversation pairs\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # 4. ConversationTokenBufferMemory - Token-aware pruning\n",
    "        # Memory complexity: O(m) where m = max_token_limit\n",
    "        token_memory = ConversationTokenBufferMemory(\n",
    "            llm=self.llm,\n",
    "            max_token_limit=500,\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # 5. ConversationEntityMemory - Entity relationship tracking\n",
    "        # Memory complexity: O(e) where e = number of entities\n",
    "        entity_store = InMemoryEntityStore()\n",
    "        entity_memory = ConversationEntityMemory(\n",
    "            llm=self.llm,\n",
    "            entity_store=entity_store,\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        self.memory_systems = {\n",
    "            \"buffer\": buffer_memory,\n",
    "            \"summary\": summary_memory,\n",
    "            \"window\": window_memory,\n",
    "            \"token\": token_memory,\n",
    "            \"entity\": entity_memory\n",
    "        }\n",
    "        \n",
    "        return self.memory_systems\n",
    "    \n",
    "    def create_memory_conversations(self):\n",
    "        \"\"\"Create conversation chains for each memory type\"\"\"\n",
    "        \n",
    "        conversations = {}\n",
    "        \n",
    "        for memory_name, memory_system in self.memory_systems.items():\n",
    "            conversation = ConversationChain(\n",
    "                llm=self.llm,\n",
    "                memory=memory_system,\n",
    "                verbose=False  # Reduced verbosity for cleaner output\n",
    "            )\n",
    "            conversations[memory_name] = conversation\n",
    "        \n",
    "        return conversations\n",
    "    \n",
    "    def test_memory_scenarios(self):\n",
    "        \"\"\"Test different memory systems with various conversation patterns\"\"\"\n",
    "        \n",
    "        conversations = self.create_memory_conversations()\n",
    "        \n",
    "        # Test scenarios for different memory characteristics\n",
    "        scenarios = {\n",
    "            \"short_detailed\": [\n",
    "                \"I'm planning a wedding for next summer.\",\n",
    "                \"The venue is in California, budget is $50,000.\",\n",
    "                \"We expect 150 guests, mostly family and close friends.\"\n",
    "            ],\n",
    "            \"long_business\": [\n",
    "                \"Let's discuss the Q4 marketing strategy for TechCorp.\",\n",
    "                \"Our target is to increase market share by 15% this quarter.\", \n",
    "                \"The main competitors are DataSys and CloudFlow Solutions.\",\n",
    "                \"We have a budget of $2M for digital advertising campaigns.\",\n",
    "                \"Sarah Johnson is the marketing director, she prefers social media focus.\",\n",
    "                \"The product launch is scheduled for December 15th.\",\n",
    "                \"We need to coordinate with the sales team led by Mike Chen.\",\n",
    "                \"Previous campaigns showed 12% conversion rates on LinkedIn ads.\"\n",
    "            ],\n",
    "            \"entity_heavy\": [\n",
    "                \"I work with Microsoft on cloud projects. The PM is Alice Wang.\",\n",
    "                \"Alice mentioned they're migrating to Azure Service Bus.\",\n",
    "                \"The project budget is $500K and timeline is 6 months.\",\n",
    "                \"We also collaborate with Amazon Web Services team.\",\n",
    "                \"John Smith from AWS handles the integration requirements.\",\n",
    "                \"Microsoft wants completion by March 2025 for their fiscal year.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for scenario_name, messages in scenarios.items():\n",
    "            print(f\"\\nüß™ Testing Scenario: {scenario_name}\")\n",
    "            scenario_results = {}\n",
    "            \n",
    "            for memory_type, conversation in conversations.items():\n",
    "                print(f\"  Testing {memory_type} memory...\")\n",
    "                \n",
    "                # Reset conversation for each test\n",
    "                conversation.memory.clear()\n",
    "                \n",
    "                responses = []\n",
    "                for message in messages:\n",
    "                    response = conversation.predict(input=message)\n",
    "                    responses.append(response[:100] + \"...\" if len(response) > 100 else response)\n",
    "                \n",
    "                # Analyze memory state\n",
    "                memory_state = self.analyze_memory_state(conversation.memory, memory_type)\n",
    "                \n",
    "                scenario_results[memory_type] = {\n",
    "                    \"responses\": responses,\n",
    "                    \"memory_analysis\": memory_state\n",
    "                }\n",
    "            \n",
    "            results[scenario_name] = scenario_results\n",
    "        \n",
    "        self.test_results = results\n",
    "        return results\n",
    "    \n",
    "    def analyze_memory_state(self, memory, memory_type):\n",
    "        \"\"\"Analyze the current state of a memory system\"\"\"\n",
    "        \n",
    "        analysis = {\"type\": memory_type}\n",
    "        \n",
    "        try:\n",
    "            if hasattr(memory, 'chat_memory'):\n",
    "                analysis[\"message_count\"] = len(memory.chat_memory.messages)\n",
    "                \n",
    "            if memory_type == \"buffer\":\n",
    "                # Buffer memory: complete message history\n",
    "                analysis[\"storage_type\"] = \"complete_history\"\n",
    "                analysis[\"growth_pattern\"] = \"linear\"\n",
    "                \n",
    "            elif memory_type == \"summary\":\n",
    "                # Summary memory: compressed representation\n",
    "                analysis[\"storage_type\"] = \"compressed_summary\"\n",
    "                analysis[\"growth_pattern\"] = \"logarithmic\"\n",
    "                if hasattr(memory, 'buffer'):\n",
    "                    analysis[\"current_summary\"] = memory.buffer[:100] + \"...\" if memory.buffer else \"No summary yet\"\n",
    "                    \n",
    "            elif memory_type == \"window\": \n",
    "                # Window memory: fixed-size sliding window\n",
    "                analysis[\"storage_type\"] = \"sliding_window\"\n",
    "                analysis[\"growth_pattern\"] = \"constant\"\n",
    "                analysis[\"window_size\"] = memory.k * 2  # k conversation pairs\n",
    "                \n",
    "            elif memory_type == \"token\":\n",
    "                # Token memory: token-aware pruning\n",
    "                analysis[\"storage_type\"] = \"token_limited\"\n",
    "                analysis[\"growth_pattern\"] = \"bounded\"\n",
    "                analysis[\"max_tokens\"] = memory.max_token_limit\n",
    "                \n",
    "            elif memory_type == \"entity\":\n",
    "                # Entity memory: relationship tracking\n",
    "                analysis[\"storage_type\"] = \"entity_graph\"\n",
    "                analysis[\"growth_pattern\"] = \"entity_proportional\"\n",
    "                if hasattr(memory, 'entity_store'):\n",
    "                    entities = list(memory.entity_store.store.keys())\n",
    "                    analysis[\"tracked_entities\"] = entities[:5]  # Show first 5\n",
    "                    analysis[\"entity_count\"] = len(entities)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            analysis[\"error\"] = str(e)\n",
    "            \n",
    "        return analysis\n",
    "\n",
    "# Initialize and run memory systems demonstration\n",
    "print(\"üß† Initializing Memory Systems Demo\")\n",
    "memory_demo = MemorySystemsDemo(llm)\n",
    "memory_systems = memory_demo.create_all_memory_types()\n",
    "\n",
    "print(\"üîÑ Testing Memory Systems with Different Scenarios\")\n",
    "memory_test_results = memory_demo.test_memory_scenarios()\n",
    "\n",
    "# Store comprehensive results\n",
    "tutorial_state[\"memory_systems\"] = memory_systems\n",
    "tutorial_state[\"demo_data\"][\"memory_tests\"] = memory_test_results\n",
    "tutorial_state[\"current_section\"] = \"memory_systems\"\n",
    "\n",
    "print(\"\\n‚úÖ Memory Systems Comprehensive Demo Complete\")\n",
    "print(\"üéØ All memory types tested and analyzed\")\n",
    "print(\"üìä Results include complexity analysis and performance characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c45e1",
   "metadata": {},
   "source": [
    "##### ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10d05e",
   "metadata": {},
   "source": [
    "ConversationBufferMemory is the most straightforward memory implementation, storing the complete conversation history without any compression or filtering. This memory type maintains perfect fidelity to the original conversation, preserving every nuance, detail, and context from the interaction history. It's like having a perfect recording of every word spoken in a meeting‚Äînothing is lost, but the storage requirements can become substantial.\n",
    "\n",
    "The primary advantage of buffer memory is its completeness and simplicity. Every message, response, and interaction detail remains available for the agent to reference, making it ideal for scenarios where precise recall is critical‚Äîthink legal consultations, medical histories, or technical support where missing details could have significant consequences. The agent can refer back to exact phrasings, specific numbers, or detailed explanations provided earlier in the conversation.\n",
    "\n",
    "However, buffer memory's strength becomes its weakness in extended conversations. As the conversation grows, token consumption increases linearly, potentially exceeding model context limits and significantly increasing costs. For models with 4K token limits, a detailed conversation might fill the entire context window, leaving little space for actual reasoning and response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51148081",
   "metadata": {},
   "source": [
    "##### ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14a425",
   "metadata": {},
   "source": [
    "\n",
    "ConversationSummaryMemory addresses the scalability limitations of buffer memory by maintaining a running summary of the conversation rather than storing every individual message. Think of it as having an intelligent note-taker who captures the key themes, decisions, and important details while filtering out redundant or less relevant information. This approach allows for indefinitely long conversations while maintaining reasonable token consumption.\n",
    "\n",
    "The summary mechanism works by periodically condensing older conversation history into concise summaries using the language model itself. When the conversation reaches a certain length, the memory system takes the oldest messages, generates a summary of their content, and replaces the original messages with this compressed representation. New messages continue to be stored in full until the next summarization cycle.\n",
    "\n",
    "This approach excels in scenarios where conversation themes and key decisions matter more than exact wording‚Äîthink project planning sessions, brainstorming meetings, or ongoing consulting relationships where the agent needs to remember overall context and previous decisions but doesn't need verbatim recall of every exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bb9a3",
   "metadata": {},
   "source": [
    "##### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c87af",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory takes a different approach to managing conversation length by maintaining only the most recent N interactions in full detail while discarding older messages entirely. This sliding window approach ensures consistent performance and predictable token usage, making it ideal for applications where recent context is most relevant and older interactions can be safely forgotten.\n",
    "\n",
    "The window size (k parameter) determines how many of the most recent message pairs to retain. For example, with k=3, the memory stores the last 3 human messages and their corresponding AI responses, totaling 6 messages. When a new interaction occurs, the oldest message pair is dropped to make room for the new one, maintaining a constant memory footprint.\n",
    "\n",
    "This memory type excels in task-oriented conversations where the immediate context matters most‚Äîthink customer service interactions, troubleshooting sessions, or iterative design processes where each step builds on the previous few but doesn't require the entire conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory - maintains only recent interactions\n",
    "window_memory = ConversationBufferWindowMemory(\n",
    "    k=3,  # Keep last 3 conversation pairs (6 messages total)\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "window_conversation = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== ConversationBufferWindowMemory Demo ===\")\n",
    "\n",
    "# Simulate a troubleshooting conversation\n",
    "troubleshooting_steps = [\n",
    "    \"My laptop won't start. The screen stays black when I press the power button.\",\n",
    "    \"I can see a small LED light on the laptop, but nothing else happens.\",\n",
    "    \"I tried holding the power button for 30 seconds, but no change.\",\n",
    "    \"Let me try removing the battery and plugging in just the power adapter.\",\n",
    "    \"That worked! The laptop started. So it seems like a battery issue?\",\n",
    "    \"Should I get a replacement battery or could this be something else?\",\n",
    "    \"How can I test if the battery is completely dead or just needs charging?\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(troubleshooting_steps, 1):\n",
    "    response = window_conversation.predict(input=user_input)\n",
    "    print(f\"Step {i}: Completed\")\n",
    "    \n",
    "    # Show memory contents after each step\n",
    "    memory_contents = window_memory.chat_memory.messages\n",
    "    print(f\"Memory contains {len(memory_contents)} messages (window size = {window_memory.k * 2})\")\n",
    "\n",
    "print(f\"\\n=== Final Memory State ===\")\n",
    "print(\"Window memory final contents:\")\n",
    "for i, message in enumerate(window_memory.chat_memory.messages):\n",
    "    print(f\"Message {i+1}: {type(message).__name__} - {message.content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b2ea7",
   "metadata": {},
   "source": [
    "##### ConversationTokenBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff95afa",
   "metadata": {},
   "source": [
    "ConversationTokenBufferMemory provides the most sophisticated approach to memory management by dynamically adjusting conversation history based on token consumption rather than fixed message counts or arbitrary summarization triggers. This memory type continuously monitors token usage and intelligently removes older messages when approaching the specified token limit, ensuring optimal utilization of the model's context window while maintaining as much relevant history as possible.\n",
    "\n",
    "The key innovation here is token-aware pruning‚Äîthe memory system counts tokens in the conversation history and removes the oldest messages when the total approaches the configured limit. This ensures you never exceed context limits while maximizing the amount of conversation history available to the model. It's like having an intelligent editor who knows exactly how much content fits in the available space and makes informed decisions about what to keep.\n",
    "\n",
    "This approach is particularly valuable in production applications where token costs matter and conversation lengths vary unpredictably. It provides the reliability of never exceeding context limits with the efficiency of using available context space optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88821756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationTokenBufferMemory - manages memory based on token count\n",
    "token_memory = ConversationTokenBufferMemory(\n",
    "    llm=memory_llm,\n",
    "    max_token_limit=500,  # Keep conversation under 500 tokens\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "token_conversation = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=token_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== ConversationTokenBufferMemory Demo ===\")\n",
    "\n",
    "# Test with progressively longer inputs to see token management\n",
    "test_inputs = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Can you explain machine learning in detail, including supervised and unsupervised learning approaches?\",\n",
    "    \"I'm interested in deep learning. Can you walk me through neural networks, backpropagation, and how gradient descent works?\",\n",
    "    \"What about transformers and attention mechanisms? How do they work in modern language models?\",\n",
    "    \"Can you compare different AI architectures like CNNs, RNNs, LSTMs, and transformers in terms of their strengths and use cases?\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n--- Interaction {i} ---\")\n",
    "    print(f\"Input length: ~{len(user_input.split()) * 1.3:.0f} tokens (estimated)\")\n",
    "    \n",
    "    response = token_conversation.predict(input=user_input)\n",
    "    \n",
    "    # Check token usage\n",
    "    current_messages = token_memory.chat_memory.messages\n",
    "    estimated_tokens = sum(len(msg.content.split()) * 1.3 for msg in current_messages)\n",
    "    \n",
    "    print(f\"Memory contains {len(current_messages)} messages\")\n",
    "    print(f\"Estimated tokens in memory: {estimated_tokens:.0f} / {token_memory.max_token_limit}\")\n",
    "    print(f\"Response preview: {response[:100]}...\")\n",
    "\n",
    "print(\"\\n=== Token Management Analysis ===\")\n",
    "print(\"Token buffer memory automatically pruned older messages to stay within limits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116c0c9",
   "metadata": {},
   "source": [
    "##### ConversationEntityMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3b2a7",
   "metadata": {},
   "source": [
    "ConversationEntityMemory represents a more sophisticated approach to context management by focusing on entities‚Äîspecific people, places, organizations, concepts, or objects‚Äîmentioned throughout the conversation. Rather than treating all information equally, this memory system identifies and tracks important entities, maintaining detailed information about each one and their relationships to the ongoing conversation.\n",
    "\n",
    "Think of entity memory as having a smart assistant who keeps detailed notes about every person, company, project, or concept you discuss, building a rich knowledge graph of your conversation topics. When you mention \"the Johnson project\" or \"Sarah from marketing,\" the system retrieves all relevant context about these entities from previous discussions, even if they were mentioned weeks ago.\n",
    "\n",
    "This approach excels in complex, ongoing relationships where tracking multiple entities and their evolving attributes is crucial‚Äîthink sales conversations tracking multiple clients and deals, project management discussions involving various stakeholders and deliverables, or research conversations where you're building understanding about multiple related concepts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb88ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationEntityMemory - tracks entities and their relationships\n",
    "entity_store = InMemoryEntityStore()\n",
    "entity_memory = ConversationEntityMemory(\n",
    "    llm=memory_llm,\n",
    "    entity_store=entity_store,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "entity_conversation = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=entity_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== ConversationEntityMemory Demo ===\")\n",
    "\n",
    "# Simulate a business conversation with multiple entities\n",
    "business_conversations = [\n",
    "    \"I'm working with TechCorp on a new software project. The CEO is Maria Rodriguez.\",\n",
    "    \"Maria mentioned they need the mobile app completed by December 15th for their Q4 launch.\",\n",
    "    \"The project budget is $250,000 and we have a team of 5 developers assigned.\",\n",
    "    \"We're also collaborating with DataSys Inc for the backend infrastructure. Their CTO is James Chen.\",\n",
    "    \"James told me they use AWS for hosting and PostgreSQL for the database.\",\n",
    "    \"TechCorp wants to integrate with their existing CRM system that was built by Solutions Ltd.\",\n",
    "    \"Maria is concerned about security compliance since they handle financial data.\",\n",
    "    \"The mobile app will have both iOS and Android versions, targeting business users.\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(business_conversations, 1):\n",
    "    response = entity_conversation.predict(input=user_input)\n",
    "    print(f\"Business discussion {i} completed\")\n",
    "\n",
    "# Examine the entities that were tracked\n",
    "print(\"\\n=== Entity Memory Analysis ===\")\n",
    "print(\"Entities identified and tracked:\")\n",
    "entity_data = entity_memory.entity_store.store\n",
    "for entity_name, entity_info in entity_data.items():\n",
    "    print(f\"- {entity_name}: {entity_info}\")\n",
    "\n",
    "# Test entity recall with a follow-up question\n",
    "print(\"\\n=== Entity Recall Test ===\")\n",
    "followup_response = entity_conversation.predict(\n",
    "    input=\"What was Maria's deadline for the project and what's their budget?\"\n",
    ")\n",
    "print(\"Follow-up response:\", followup_response[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ead55b",
   "metadata": {},
   "source": [
    "##### CombinedMemory and Advanced Patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee12e43",
   "metadata": {},
   "source": [
    "Real-world applications often benefit from combining multiple memory strategies to create sophisticated context management systems that leverage the strengths of different approaches while mitigating their individual limitations. CombinedMemory allows you to orchestrate multiple memory systems simultaneously, creating layered context awareness that can handle both immediate needs and long-term relationship building.\n",
    "\n",
    "For example, you might combine ConversationBufferWindowMemory for immediate context with ConversationEntityMemory for long-term entity tracking, plus a custom memory component for domain-specific information. This creates a multi-layered memory architecture where recent interactions provide immediate context, entity memory maintains relationship continuity, and specialized memory components handle domain-specific requirements like user preferences or system configurations.\n",
    "\n",
    "Let's implement a combined memory system that demonstrates this architectural approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CombinedMemory - orchestrate multiple memory systems\n",
    "from langchain.memory import SimpleMemory\n",
    "\n",
    "# Create individual memory components\n",
    "recent_memory = ConversationBufferWindowMemory(\n",
    "    k=2, memory_key=\"recent_history\", return_messages=True\n",
    ")\n",
    "\n",
    "entity_tracker = ConversationEntityMemory(\n",
    "    llm=memory_llm,\n",
    "    entity_store=InMemoryEntityStore(),\n",
    "    memory_key=\"entities\",\n",
    "    return_messages=False  # Just track entities, don't return full chat\n",
    ")\n",
    "\n",
    "# Create a simple memory for user preferences\n",
    "preferences_memory = SimpleMemory(\n",
    "    memories={\"user_preferences\": \"No specific preferences set yet\"}\n",
    ")\n",
    "\n",
    "# Combine all memory systems\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Custom prompt template that utilizes all memory types\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create chain with combined memory\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== CombinedMemory Demo ===\")\n",
    "\n",
    "# Test the combined memory system\n",
    "test_conversation = [\n",
    "    \"Hi, I'm Sarah and I prefer concise responses. I'm working on a Python project.\",\n",
    "    \"I need help with data analysis using pandas. Can you recommend some techniques?\",\n",
    "    \"Actually, I'm working with customer data for my company TechFlow Solutions.\",\n",
    "    \"Our CEO Mike Johnson wants insights on customer retention patterns.\",\n",
    "    \"Can you suggest a visualization approach for this data?\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(test_conversation, 1):\n",
    "    response = combined_chain.predict(input=user_input)\n",
    "    print(f\"Combined memory interaction {i} completed\")\n",
    "\n",
    "print(\"\\n=== Memory System Analysis ===\")\n",
    "print(\"Combined memory successfully integrated:\")\n",
    "print(\"- Recent conversation context\")\n",
    "print(\"- Entity tracking across sessions\") \n",
    "print(\"- User preference management\")\n",
    "print(\"- Seamless coordination between memory types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6f83a",
   "metadata": {},
   "source": [
    "The examples above demonstrate the spectrum of memory management strategies available for agentic systems. Each approach serves different purposes and excels in specific scenarios:\n",
    "\n",
    "**ConversationBufferMemory** provides perfect recall for short conversations where every detail matters, but becomes expensive in extended interactions. **ConversationSummaryMemory** enables indefinitely long conversations by maintaining key themes while sacrificing some detail. **ConversationBufferWindowMemory** offers predictable performance by keeping only recent context, ideal for task-oriented interactions. **ConversationTokenBufferMemory** provides optimal context utilization with cost control, perfect for production applications.\n",
    "\n",
    "**ConversationEntityMemory** excels at tracking relationships and building long-term understanding, while **CombinedMemory** allows sophisticated orchestration of multiple memory strategies. The choice depends on your specific requirements: conversation length, cost constraints, detail requirements, and the importance of long-term relationship building.\n",
    "\n",
    "In practice, most production agentic systems benefit from combining multiple memory approaches, using recent memory for immediate context, entity memory for relationship continuity, and token-aware management for cost control. This creates robust context management that adapts to different conversation patterns while maintaining performance and reliability.\n",
    "\n",
    "Now that our agents have sophisticated memory capabilities, let's explore how they can develop and refine specialized skills that make them even more effective at specific tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729477c1",
   "metadata": {},
   "source": [
    "#### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e400",
   "metadata": {},
   "source": [
    "Skills represent specialized capabilities that agents can develop and refine to excel in specific domains or tasks. Think of skills as the difference between a general practitioner and a specialist‚Äîwhile both are knowledgeable, the specialist has developed deep expertise, refined techniques, and domain-specific patterns that make them exceptionally effective in their area of focus.\n",
    "\n",
    "In agentic systems, skills are implemented as structured combinations of prompts, tools, memory patterns, and domain knowledge that work together to solve specific types of problems. A financial analysis skill might combine market data tools, statistical calculation capabilities, and specialized prompts for interpreting economic indicators. A creative writing skill could integrate research tools, style guidelines, and iterative refinement processes.\n",
    "\n",
    "Skills provide several key benefits: **Specialization** allows agents to develop deep expertise in specific areas rather than being mediocre generalists. **Consistency** ensures that similar problems are approached with proven, refined techniques. **Reusability** means successful skill patterns can be applied across different contexts and even shared between agents. **Composability** enables complex workflows where multiple skills collaborate to solve multifaceted problems.\n",
    "\n",
    "However, skills also introduce challenges: they can create **over-specialization** where agents become inflexible, **complexity** that makes systems harder to debug and maintain, and **coordination overhead** when multiple skills need to work together. The key is finding the right balance between specialization and flexibility for your specific use case.\n",
    "\n",
    "Let's implement a skill system that demonstrates these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skills Implementation - Specialized agent capabilities\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "@dataclass\n",
    "class SkillResult:\n",
    "    \"\"\"Result of executing a skill\"\"\"\n",
    "    success: bool\n",
    "    output: str\n",
    "    confidence: float\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class BaseSkill(ABC):\n",
    "    \"\"\"Base class for agent skills\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.execution_count = 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        \"\"\"Execute the skill with given input\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get skill metadata and performance stats\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description, \n",
    "            \"executions\": self.execution_count\n",
    "        }\n",
    "\n",
    "# Financial Analysis Skill\n",
    "class FinancialAnalysisSkill(BaseSkill):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            name=\"Financial Analysis\",\n",
    "            description=\"Analyze financial data and provide investment insights\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "        self.analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"analysis_type\"],\n",
    "            template=\"\"\"You are a senior financial analyst with expertise in investment analysis.\n",
    "            \n",
    "            Data to analyze: {data}\n",
    "            Analysis type: {analysis_type}\n",
    "            \n",
    "            Provide a comprehensive analysis including:\n",
    "            1. Key metrics interpretation\n",
    "            2. Risk assessment\n",
    "            3. Investment recommendation\n",
    "            4. Confidence level (1-10)\n",
    "            \n",
    "            Focus on actionable insights and clearly explain your reasoning.\"\"\"\n",
    "        )\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        analysis_type = context.get(\"analysis_type\", \"general\") if context else \"general\"\n",
    "        \n",
    "        try:\n",
    "            chain = self.analysis_prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\n",
    "                \"data\": input_data,\n",
    "                \"analysis_type\": analysis_type\n",
    "            })\n",
    "            \n",
    "            # Extract confidence from result (simplified)\n",
    "            confidence = 0.8  # Would normally parse this from LLM output\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=confidence,\n",
    "                metadata={\"analysis_type\": analysis_type}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Analysis failed: {str(e)}\",\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "# Code Review Skill  \n",
    "class CodeReviewSkill(BaseSkill):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            name=\"Code Review\", \n",
    "            description=\"Perform comprehensive code reviews with security and best practice focus\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "        self.review_prompt = PromptTemplate(\n",
    "            input_variables=[\"code\", \"language\", \"focus_areas\"],\n",
    "            template=\"\"\"You are a senior software engineer performing a detailed code review.\n",
    "            \n",
    "            Code to review:\n",
    "            ```{language}\n",
    "            {code}\n",
    "            ```\n",
    "            \n",
    "            Focus areas: {focus_areas}\n",
    "            \n",
    "            Provide a structured review covering:\n",
    "            1. Code quality and readability\n",
    "            2. Security vulnerabilities\n",
    "            3. Performance considerations\n",
    "            4. Best practice compliance\n",
    "            5. Specific improvement suggestions\n",
    "            \n",
    "            Rate each area from 1-10 and provide actionable feedback.\"\"\"\n",
    "        )\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        language = context.get(\"language\", \"python\") if context else \"python\"\n",
    "        focus_areas = context.get(\"focus_areas\", \"security, performance, readability\") if context else \"security, performance, readability\"\n",
    "        \n",
    "        try:\n",
    "            chain = self.review_prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\n",
    "                \"code\": input_data,\n",
    "                \"language\": language,\n",
    "                \"focus_areas\": focus_areas\n",
    "            })\n",
    "            \n",
    "            confidence = 0.85  # Would extract from actual analysis\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=confidence,\n",
    "                metadata={\"language\": language, \"focus_areas\": focus_areas}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Code review failed: {str(e)}\",\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "print(\"=== Skills System Implemented ===\")\n",
    "print(\"Created specialized skills for financial analysis and code review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c18d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skills Manager - Orchestrates and coordinates multiple skills\n",
    "class SkillsManager:\n",
    "    def __init__(self, llm):\n",
    "        self.skills: Dict[str, BaseSkill] = {}\n",
    "        self.llm = llm\n",
    "        self.skill_selection_prompt = PromptTemplate(\n",
    "            input_variables=[\"user_request\", \"available_skills\"],\n",
    "            template=\"\"\"You are a skill coordinator. Given a user request, determine which skill(s) would be most appropriate.\n",
    "            \n",
    "            User Request: {user_request}\n",
    "            \n",
    "            Available Skills: {available_skills}\n",
    "            \n",
    "            Respond with just the skill name that best matches the request, or \"none\" if no skill is suitable.\n",
    "            Consider the task type and choose the most specialized skill available.\"\"\"\n",
    "        )\n",
    "    \n",
    "    def register_skill(self, skill: BaseSkill):\n",
    "        \"\"\"Register a new skill with the manager\"\"\"\n",
    "        self.skills[skill.name] = skill\n",
    "        print(f\"Registered skill: {skill.name}\")\n",
    "    \n",
    "    def select_skill(self, user_request: str) -> str:\n",
    "        \"\"\"Intelligently select the best skill for a given request\"\"\"\n",
    "        available_skills = \"\\n\".join([\n",
    "            f\"- {name}: {skill.description}\" \n",
    "            for name, skill in self.skills.items()\n",
    "        ])\n",
    "        \n",
    "        chain = self.skill_selection_prompt | self.llm | StrOutputParser()\n",
    "        selected_skill = chain.invoke({\n",
    "            \"user_request\": user_request,\n",
    "            \"available_skills\": available_skills\n",
    "        }).strip()\n",
    "        \n",
    "        return selected_skill if selected_skill in self.skills else None\n",
    "    \n",
    "    def execute_skill(self, skill_name: str, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        \"\"\"Execute a specific skill\"\"\"\n",
    "        if skill_name not in self.skills:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Skill '{skill_name}' not found\",\n",
    "                confidence=0.0\n",
    "            )\n",
    "        \n",
    "        return self.skills[skill_name].execute(input_data, context)\n",
    "    \n",
    "    def auto_execute(self, user_request: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        \"\"\"Automatically select and execute the best skill for a request\"\"\"\n",
    "        selected_skill = self.select_skill(user_request)\n",
    "        \n",
    "        if not selected_skill:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=\"No suitable skill found for this request\",\n",
    "                confidence=0.0\n",
    "            )\n",
    "        \n",
    "        print(f\"Selected skill: {selected_skill}\")\n",
    "        return self.execute_skill(selected_skill, user_request, context)\n",
    "    \n",
    "    def get_skills_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of all registered skills\"\"\"\n",
    "        return {\n",
    "            name: skill.get_metadata() \n",
    "            for name, skill in self.skills.items()\n",
    "        }\n",
    "\n",
    "# Create skills manager and register our skills\n",
    "skills_manager = SkillsManager(memory_llm)\n",
    "skills_manager.register_skill(FinancialAnalysisSkill(memory_llm))\n",
    "skills_manager.register_skill(CodeReviewSkill(memory_llm))\n",
    "\n",
    "print(\"=== Skills Manager Created ===\")\n",
    "print(\"Skills system ready for intelligent task routing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the skills system with different types of requests\n",
    "\n",
    "print(\"=== Skills System Demonstration ===\")\n",
    "\n",
    "# Test 1: Financial Analysis Request\n",
    "financial_request = \"\"\"\n",
    "I have the following financial data for a tech company:\n",
    "- Revenue: $50M (up 25% YoY)\n",
    "- Operating margin: 15%\n",
    "- Cash flow: $8M positive\n",
    "- Debt-to-equity ratio: 0.3\n",
    "- P/E ratio: 22\n",
    "\n",
    "Should I invest in this company?\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Financial Analysis Test ---\")\n",
    "financial_result = skills_manager.auto_execute(\n",
    "    financial_request,\n",
    "    context={\"analysis_type\": \"investment_decision\"}\n",
    ")\n",
    "print(f\"Success: {financial_result.success}\")\n",
    "print(f\"Confidence: {financial_result.confidence}\")\n",
    "print(f\"Result preview: {financial_result.output[:200]}...\")\n",
    "\n",
    "# Test 2: Code Review Request  \n",
    "code_request = \"\"\"\n",
    "Please review this Python function:\n",
    "\n",
    "def process_user_data(user_input):\n",
    "    data = eval(user_input)\n",
    "    sql = f\"SELECT * FROM users WHERE id = {data['user_id']}\"\n",
    "    cursor.execute(sql)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "Is this code secure and well-written?\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Code Review Test ---\")\n",
    "code_result = skills_manager.auto_execute(\n",
    "    code_request,\n",
    "    context={\"language\": \"python\", \"focus_areas\": \"security, best practices\"}\n",
    ")\n",
    "print(f\"Success: {code_result.success}\")\n",
    "print(f\"Confidence: {code_result.confidence}\")\n",
    "print(f\"Result preview: {code_result.output[:200]}...\")\n",
    "\n",
    "# Test 3: General Request (no specific skill)\n",
    "general_request = \"What's the weather like today?\"\n",
    "\n",
    "print(\"\\n--- General Request Test ---\")\n",
    "general_result = skills_manager.auto_execute(general_request)\n",
    "print(f\"Success: {general_result.success}\")\n",
    "print(f\"Result: {general_result.output}\")\n",
    "\n",
    "# Show skills performance summary\n",
    "print(\"\\n=== Skills Performance Summary ===\")\n",
    "summary = skills_manager.get_skills_summary()\n",
    "for skill_name, metadata in summary.items():\n",
    "    print(f\"{skill_name}: {metadata['executions']} executions\")\n",
    "    print(f\"  Description: {metadata['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beac902",
   "metadata": {},
   "source": [
    "### Workflows and Chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e6ba0",
   "metadata": {},
   "source": [
    "##### LangGraph Parallel Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e371d",
   "metadata": {},
   "source": [
    "##### LangGraph Multi-Agent Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1de84",
   "metadata": {},
   "source": [
    "##### LangChain Agent Executors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5567e2",
   "metadata": {},
   "source": [
    "##### LlamaIndex AgentRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd518b",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e3e6c",
   "metadata": {},
   "source": [
    "### But Why RAG?\n",
    "\n",
    "Talk about LLM system in general, while introducing agents, where those workflows lack the limit of llms in context and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc169c2",
   "metadata": {},
   "source": [
    "### Finding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a060c",
   "metadata": {},
   "source": [
    "#### Webscraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3655c",
   "metadata": {},
   "source": [
    "##### LangChain WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d21253",
   "metadata": {},
   "source": [
    "##### LangChain AsyncHtmlLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76a4f4",
   "metadata": {},
   "source": [
    "##### LangChain SitemapLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016c6e8",
   "metadata": {},
   "source": [
    "##### LangChain PlaywrightURLLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19028a8c",
   "metadata": {},
   "source": [
    "##### LlamaIndex SimpleWebPageReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326287d8",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex BeautifulSoupWebReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d7a17",
   "metadata": {},
   "source": [
    "#### Document Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc20e9",
   "metadata": {},
   "source": [
    "##### LangChain PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb7df0",
   "metadata": {},
   "source": [
    "##### LangChain UnstructuredFileLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af53d8",
   "metadata": {},
   "source": [
    "##### LangChain CSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af499259",
   "metadata": {},
   "source": [
    "##### LangChain JSONLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70654f7e",
   "metadata": {},
   "source": [
    "##### LlamaIndex SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a64221",
   "metadata": {},
   "source": [
    "##### LlamaIndex PDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc25e03",
   "metadata": {},
   "source": [
    "### Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbf6bd",
   "metadata": {},
   "source": [
    "#### Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f88889",
   "metadata": {},
   "source": [
    "##### LangChain RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a690a5",
   "metadata": {},
   "source": [
    "##### LangChain TokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f31dec",
   "metadata": {},
   "source": [
    "##### LangChain MarkdownHeaderTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20168248",
   "metadata": {},
   "source": [
    "##### LangChain PythonCodeTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0bcfd",
   "metadata": {},
   "source": [
    "##### LlamaIndex SentenceSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4dd95",
   "metadata": {},
   "source": [
    "##### LlamaIndex SemanticSplitterNodeParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32531064",
   "metadata": {},
   "source": [
    "##### LlamaIndex HierarchicalNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfbca2",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8df09",
   "metadata": {},
   "source": [
    "##### LangChain SemanticChunker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d5275",
   "metadata": {},
   "source": [
    "##### LangChain ParentDocumentRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7efeb",
   "metadata": {},
   "source": [
    "##### LlamaIndex SimpleNodeParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b604a",
   "metadata": {},
   "source": [
    "##### LlamaIndex SentenceWindowNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d351c",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324ddcb",
   "metadata": {},
   "source": [
    "##### LangChain OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab8dda",
   "metadata": {},
   "source": [
    "##### LangChain HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7aca",
   "metadata": {},
   "source": [
    "##### LlamaIndex OpenAIEmbedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47ab6a",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d238e7",
   "metadata": {},
   "source": [
    "### Storing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477119b6",
   "metadata": {},
   "source": [
    "#### Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30172c54",
   "metadata": {},
   "source": [
    "##### LangChain Chroma Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a76543",
   "metadata": {},
   "source": [
    "##### LangChain Pinecone Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b117522",
   "metadata": {},
   "source": [
    "##### LangChain FAISS Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d8c52",
   "metadata": {},
   "source": [
    "##### LlamaIndex ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a255cc0",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608dc711",
   "metadata": {},
   "source": [
    "#### Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e862dd1",
   "metadata": {},
   "source": [
    "##### LangGraph StateGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78022c",
   "metadata": {},
   "source": [
    "##### LangChain Neo4jGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45991c4f",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex KnowledgeGraphIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090d616",
   "metadata": {},
   "source": [
    "#### SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553ab93",
   "metadata": {},
   "source": [
    "##### LangChain SQLDatabase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465957c",
   "metadata": {},
   "source": [
    "##### LangChain SQLDatabaseChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154a1a8",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex SQLStructStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8555b",
   "metadata": {},
   "source": [
    "### Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df26eae",
   "metadata": {},
   "source": [
    "#### Vector search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e03ebd",
   "metadata": {},
   "source": [
    "##### LangChain VectorStoreRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668dc4c",
   "metadata": {},
   "source": [
    "##### LangChain MultiVectorRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8b81f",
   "metadata": {},
   "source": [
    "##### LlamaIndex VectorIndexRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ecc5a",
   "metadata": {},
   "source": [
    "\n",
    "##### LlamaIndex VectorIndexAutoRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcdc80",
   "metadata": {},
   "source": [
    "#### Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ccf6a",
   "metadata": {},
   "source": [
    "#### Node Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c0bcc",
   "metadata": {},
   "source": [
    "#### Hybrid Search\n",
    "\n",
    "##### LangChain EnsembleRetriever\n",
    "##### LangChain BM25Retriever\n",
    "##### LlamaIndex QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9945c",
   "metadata": {},
   "source": [
    "##### LangChain ConditionalEdge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3f5c5",
   "metadata": {},
   "source": [
    "##### LangGraph Router Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99707a7f",
   "metadata": {},
   "source": [
    "##### LlamaIndex RouterQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbce248",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2bacb9",
   "metadata": {},
   "source": [
    "#### Faithfulness & Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b7c07",
   "metadata": {},
   "source": [
    "#### RAGAS (RAG Assessment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d187128",
   "metadata": {},
   "source": [
    "##### LangSmith + RAGAS Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3d6e4",
   "metadata": {},
   "source": [
    "##### LangChain Evaluation Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514c762",
   "metadata": {},
   "source": [
    "#### TruLens RAG Triad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2ef4c",
   "metadata": {},
   "source": [
    "#### Multi-Agent Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315a255",
   "metadata": {},
   "source": [
    "#### Advanced Agentic Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2815e8c",
   "metadata": {},
   "source": [
    "#### Human Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e2760",
   "metadata": {},
   "source": [
    "#### LLM-as-Judge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcbe94",
   "metadata": {},
   "source": [
    "##### LangSmith Tracing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a7f4c",
   "metadata": {},
   "source": [
    "##### LangSmith Evaluation Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb866f",
   "metadata": {},
   "source": [
    "##### LangSmith Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f82ea2",
   "metadata": {},
   "source": [
    "## A Complete Agentic System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a72785",
   "metadata": {},
   "source": [
    "##### LangGraph Agent Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9799f18",
   "metadata": {},
   "source": [
    "##### LangChain Agent Types (ReAct, Plan-and-Execute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31925586",
   "metadata": {},
   "source": [
    "##### LangSmith Agent Monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5da5a",
   "metadata": {},
   "source": [
    "##### LlamaIndex Multi-Agent Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11448f82",
   "metadata": {},
   "source": [
    "## Limitations & Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ba15",
   "metadata": {},
   "source": [
    "#### RAPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a3d4e",
   "metadata": {},
   "source": [
    "#### Self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d098",
   "metadata": {},
   "source": [
    "#### CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d56e81",
   "metadata": {},
   "source": [
    "#### Adaptive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adba2b0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6403629",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
