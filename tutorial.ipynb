{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36be9c2",
   "metadata": {},
   "source": [
    "# Agents and RAG, A Technical Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92002c",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389560b9",
   "metadata": {},
   "source": [
    "I'll go through the fundamentals of Agents and rag with the help of langchain library \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/awan_getting_langchain_ecosystem_1-1024x574.png\" width=700>\n",
    "\n",
    "<img src=\"https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2023/10/12015949/LlamaIndex.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb5de",
   "metadata": {},
   "source": [
    "### Brief History\n",
    "\n",
    "Before we dive into building agents, let's take a moment to understand the journey that brought us to this exciting point in AI history. Understanding where agents came from will help you appreciate why the systems we're building today represent such a significant breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4eff09",
   "metadata": {},
   "source": [
    "Let me tell you a story about how we got here. The concept of intelligent agents has evolved dramatically over the past seven decades, transforming from simple rule-based systems to today's sophisticated AI companions that can reason, plan, and act autonomously. \n",
    "\n",
    "**The Early Days (1950s-1980s):** Understanding this progression is essential because it helps us appreciate why modern agentic systems represent such a breakthrough. The journey began in the 1950s when researchers like Allen Newell and Herbert Simon created the Logic Theorist, a program that could prove mathematical theorems by exploring different logical paths. These early agents were like skilled craftsmen‚Äîthey could perform specific tasks very well, but only within narrow, pre-defined domains.\n",
    "\n",
    "The 1970s and 1980s brought expert systems like MYCIN for medical diagnosis and DENDRAL for chemical analysis. While impressive, these systems required months of manual knowledge engineering, where human experts had to explicitly encode their domain knowledge into rigid rule sets. Imagine trying to teach someone to be a doctor by writing down every possible symptom combination and treatment - that's essentially what early AI researchers had to do!\n",
    "\n",
    "**The Networking Era (1990s-2000s):** The 1990s marked a shift toward more flexible software agents that could operate in networked environments and coordinate with other agents. This period introduced the concept of multi-agent systems, where multiple specialized agents could collaborate to solve complex problems. However, these systems still required extensive manual programming and could only handle situations their creators had anticipated.\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*Ygen57Qiyrc8DXAFsjZLNA.gif\" width=700>\n",
    "\n",
    "**The Learning Revolution (2000s-2010s):** The real transformation began in the 2000s with machine learning advances. Agents could now learn from data rather than relying solely on hand-coded rules. Virtual assistants like Siri and Alexa brought agent technology to mainstream consumers, though they remained relatively narrow in scope‚Äîessentially sophisticated voice interfaces for search and simple task execution.\n",
    "\n",
    "**The LLM Breakthrough (2020s):** The breakthrough moment arrived with large language models starting around 2020. Systems like GPT-3 and GPT-4 combined vast knowledge with sophisticated reasoning abilities, creating agents that could understand natural language, maintain context across conversations, and tackle a wide variety of tasks without task-specific programming. \n",
    "\n",
    "Unlike their predecessors, these modern agents can break down complex problems into steps, use external tools when needed, and adapt to new situations they've never encountered before. This evolution represents a fundamental shift from automation to augmentation‚Äîwhere early agents automated specific, predefined tasks, today's agents can understand our goals and work as collaborative partners in problem-solving.\n",
    "\n",
    "**Why This History Matters for You:** Understanding this evolution helps us appreciate that we're not just building better chatbots‚Äîwe're creating systems that can handle ambiguous instructions, incomplete information, and constantly changing contexts. These capabilities make them invaluable for building sophisticated applications like the retrieval-augmented generation systems we'll explore in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0100815",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0b4ae",
   "metadata": {},
   "source": [
    "When we talk about agents in 2025, we're entering a landscape where the term has become both ubiquitous and somewhat ambiguous. Different organizations and researchers use \"agent\" to describe everything from simple chatbots to fully autonomous systems that can operate independently for weeks. \n",
    "\n",
    "Another confusion lies with reinforcement learning name conventions, the agent described in reinforcement learnign is different from the LLM agents that we deal with now, even though, they share similar vision.\n",
    "\n",
    "But don't let this confusion discourage you! This diversity in definition isn't just academic‚Äîit reflects fundamentally different architectural approaches that will determine how we build the next generation of AI applications. Let me help you navigate this landscape.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!A_Oy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3177e12-432e-4e41-814f-6febf7a35f68_1360x972.png\" width=700>\n",
    "\n",
    "**What Actually Makes an Agent?** At its core, an agent is a system that can perceive its environment, make decisions, and take actions to achieve specific goals. Sounds simple, right? But the way these capabilities are implemented varies dramatically.\n",
    "\n",
    "Some define agents as fully autonomous systems that operate independently over extended periods, using various tools and adapting their strategies based on feedback. Think of these like a personal assistant who can manage your entire schedule, book flights, handle emails, and make decisions on your behalf without constant supervision.\n",
    "\n",
    "Others use the term more broadly to describe any system that follows predefined workflows to accomplish tasks. These implementations are more like following a detailed recipe‚Äîeach step is predetermined, and while the system can handle some variations, it operates within clearly defined boundaries.\n",
    "\n",
    "**Why This Distinction Matters to You:** The difference between these approaches is crucial because it affects everything from system reliability to development complexity. Understanding this spectrum will help you choose the right approach for your specific needs and avoid over-engineering solutions.\n",
    "\n",
    "**The Spectrum of Control:** The most useful way to think about this spectrum is through the lens of control and decision-making:\n",
    "\n",
    "- **Workflows** are systems where large language models and tools are orchestrated through predefined code paths. Every decision point is anticipated by the developer, and the system follows predetermined logic to handle different scenarios.\n",
    "\n",
    "- **Agents** are systems where the LLM dynamically directs its own processes and tool usage, maintaining control over how it accomplishes tasks. The model itself decides what to do next, which tools to use, and how to adapt when things don't go as planned.\n",
    "\n",
    "Think of workflows as following a GPS route‚Äîyou know exactly where you're going and how to get there. Agents are more like having an experienced local guide who can adapt the route based on traffic, weather, or interesting stops along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069523fb",
   "metadata": {},
   "source": [
    "#### Simplicity Defines Perfectionism, Not Complexity\n",
    "\n",
    "Here's a principle that will save you countless hours and headaches as you build AI systems:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db57831",
   "metadata": {},
   "source": [
    "Now, here's some advice that might surprise you: when building applications with LLMs, the fundamental principle should be finding the simplest solution that meets your requirements. This might mean not building agentic systems at all!\n",
    "\n",
    "Let me explain why this matters. Agentic systems inherently trade latency and cost for better task performance. Every additional decision point, tool call, and reasoning step adds time and expense to your application. You need to carefully consider when this tradeoff makes sense for your specific use case.\n",
    "\n",
    "**When to Choose Workflows:** Workflows offer predictability and consistency for well-defined tasks where you can anticipate most scenarios and edge cases. They're excellent for:\n",
    "- Standardized processes like data processing pipelines\n",
    "- Content moderation workflows\n",
    "- Structured analysis tasks\n",
    "- Any situation where you need reliable, repeatable results\n",
    "\n",
    "**When to Choose Agents:** Agents become the better choice when you need flexibility and model-driven decision-making at scale. This includes situations where:\n",
    "- The variety of inputs and required responses is too broad to predefine\n",
    "- The system needs to adapt to entirely new scenarios\n",
    "- You're dealing with open-ended problems that require creative problem-solving\n",
    "- The complexity of decision trees would make workflow programming impractical\n",
    "\n",
    "**The Simple Truth:** Here's what I've learned from building production AI systems: for many applications, the most effective approach involves optimizing single LLM calls with retrieval and in-context examples rather than building complex agentic systems. \n",
    "\n",
    "Before you architect a sophisticated multi-agent system with elaborate tool chains, ask yourself: \"Could I solve this with a well-crafted prompt and some good examples?\" You'd be surprised how often the answer is yes.\n",
    "\n",
    "**But When Complexity is Worth It:** However, as we'll explore throughout this tutorial, there are compelling scenarios where the additional complexity of agents becomes not just beneficial, but necessary for achieving your goals. Understanding when and how to make this transition is what separates effective AI system builders from those who over-engineer solutions to problems that could be solved more simply.\n",
    "\n",
    "The key is developing good judgment about when to add complexity. Start simple, measure performance, and only add complexity when you can clearly demonstrate that it improves outcomes for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bd7ab",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Let's start with the most fundamental skill you'll need as an agent builder: crafting effective prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb1b8e",
   "metadata": {},
   "source": [
    "Let's talk about the foundation of everything we'll build: prompts. Think of prompts as the bridge between human intent and AI capabilities‚Äîthey're how we translate our natural language requests into structured instructions that language models can understand and act upon.\n",
    "\n",
    "But here's what makes prompts fascinating in agentic systems: they're not just about getting good answers to single questions. In the context of agents, prompts become the architectural blueprints that define not only *what* we want the agent to accomplish, but *how* the agent should approach problem-solving, what tools it can use, and how it should reason through complex tasks.\n",
    "\n",
    "**Why Prompts Are Your Most Important Tool:** I like to think of prompts as the instruction manual for your AI agent. Just as a well-written manual can make the difference between a novice successfully assembling furniture or ending up with a pile of confused parts, a well-crafted prompt determines whether your agent performs brilliantly or struggles to understand your intent.\n",
    "\n",
    "The quality and structure of your prompts directly influence:\n",
    "- The agent's reasoning capabilities\n",
    "- How it chooses and uses tools  \n",
    "- Its overall effectiveness in completing tasks\n",
    "- The consistency of results across different inputs\n",
    "\n",
    "<img src=\"https://www.datablist.com/_next/image?url=%2Fhowto_images%2Fhow-to-write-prompt-ai-agents%2Fstructured-ai-agent-prompt.png&w=3840&q=75\" width=700>\n",
    "\n",
    "**The Different Types of Prompts You'll Use:** As we build more sophisticated systems, you'll work with several types of prompts, each serving different purposes:\n",
    "\n",
    "- **System prompts** establish the agent's role, personality, and fundamental operating principles‚Äîthese are like giving someone their job description and company handbook before they start work\n",
    "- **User prompts** contain the specific tasks or questions you want the agent to handle\n",
    "- **Few-shot prompts** provide examples of desired input-output patterns to guide the agent's responses\n",
    "- **Chain-of-thought prompts** encourage step-by-step reasoning, helping agents break down complex problems into manageable pieces\n",
    "\n",
    "**The Multi-Step Challenge:** In multi-step agentic workflows, prompt engineering becomes particularly sophisticated because you need to design prompts that not only solve individual tasks but also coordinate between different stages of processing. The agent needs to understand when to use specific tools, how to interpret tool outputs, and how to maintain context across multiple interaction cycles.\n",
    "\n",
    "This requires careful consideration of prompt structure, token efficiency, and the logical flow of information through your system. Don't worry‚Äîwe'll practice all of this together as we build real systems.\n",
    "\n",
    "**Let's See It in Action:** Now that you understand why prompts matter so much, let's explore how to implement effective prompt templates using LangChain with Google's Gemini model. We'll start with basics and gradually work up to sophisticated multi-step prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# COMPREHENSIVE SETUP AND IMPORTS\n",
    "# ================================\n",
    "# This cell contains all imports and basic setup for the entire tutorial\n",
    "\n",
    "# Core LangChain and LLM imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Memory system imports\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory, \n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationEntityMemory,\n",
    "    CombinedMemory,\n",
    "    ReadOnlySharedMemory,\n",
    "    SimpleMemory\n",
    ")\n",
    "from langchain.memory.entity import InMemoryEntityStore\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Mathematical libraries for calculations\n",
    "import numpy as np\n",
    "\n",
    "# ================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Initialize primary LLM with balanced settings\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\", \n",
    "    temperature=0.3,  # Balanced creativity and consistency\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Global variables for the tutorial workflow\n",
    "tutorial_state = {\n",
    "    \"current_section\": \"setup\",\n",
    "    \"demo_data\": {},\n",
    "    \"conversation_history\": [],\n",
    "    \"skills_registry\": {},\n",
    "    \"memory_systems\": {}\n",
    "}\n",
    "\n",
    "print(\"üöÄ Agents and RAG Tutorial - Setup Complete\")\n",
    "print(\"üì¶ All imports loaded successfully\")\n",
    "print(\"üîß Global configuration initialized\")\n",
    "print(\"üìã Tutorial state tracking ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dba25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the tutorial\n",
    "%pip install langchain langchain-google-genai langchain-core numpy\n",
    "\n",
    "print(\"üì¶ Installing packages for Agents and RAG tutorial...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416983e",
   "metadata": {},
   "source": [
    "we'll create some prompt examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt_examples():\n",
    "    \"\"\"Create various prompt templates for demonstration\"\"\"\n",
    "    \n",
    "    # Basic instructional prompt\n",
    "    basic_template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"audience\"],\n",
    "        template=\"\"\"You are an expert educator who excels at explaining complex topics clearly.\n",
    "        \n",
    "        Topic: {topic}\n",
    "        Audience: {audience}\n",
    "        \n",
    "        Please provide a clear, engaging explanation that includes:\n",
    "        1. Core concept definition\n",
    "        2. Relevant examples or analogies  \n",
    "        3. Key takeaways for the audience level\n",
    "        \n",
    "        Keep your explanation appropriate for the specified audience.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Conversational prompt with memory\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant with expertise in technology and science. \n",
    "        You provide accurate, clear explanations and engage in detailed discussions.\n",
    "        Always think step-by-step when solving problems and explain your reasoning.\"\"\"),\n",
    "        (\"human\", \"I need help understanding {concept}. Can you break it down for me?\"),\n",
    "        (\"ai\", \"I'd be happy to help explain {concept}! Let me break this down step by step.\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    \n",
    "    return basic_template, chat_template\n",
    "\n",
    "# Create prompt templates\n",
    "basic_template, chat_template = create_prompt_examples()\n",
    "\n",
    "# Create reusable chains using LangChain Expression Language (LCEL)\n",
    "basic_chain = basic_template | llm | StrOutputParser()\n",
    "chat_chain = chat_template | llm | StrOutputParser()\n",
    "\n",
    "# Store in tutorial state for later use\n",
    "tutorial_state[\"prompt_templates\"] = {\n",
    "    \"basic\": basic_template,\n",
    "    \"chat\": chat_template\n",
    "}\n",
    "\n",
    "tutorial_state[\"chains\"] = {\n",
    "    \"basic\": basic_chain,\n",
    "    \"chat\": chat_chain\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Prompt Engineering Components Ready\")\n",
    "print(\"üìù Basic and conversational templates created\") \n",
    "print(\"üîó LCEL chains initialized and stored in tutorial state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacbea9",
   "metadata": {},
   "source": [
    "Great! now our LLM can respond to our questions, but how can we tweak it more to determine how much it weighs the prompt guideline while responding with it's own knowledge and reasoning? let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c858ca",
   "metadata": {},
   "source": [
    "###  Hyperparameters\n",
    "\n",
    "Once you've mastered basic prompting, the next level of control comes from understanding how to tune your model's behavior through hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d7ff9",
   "metadata": {},
   "source": [
    "Now let's dive into one of the most fascinating aspects of working with language models: hyperparameters. These are the control knobs that determine how a language model generates responses, acting like the settings on a sophisticated instrument that can dramatically change the output quality and behavior.\n",
    "\n",
    "**Why Understanding Hyperparameters Matters:** Understanding these parameters is crucial for building effective agents because they directly influence:\n",
    "- How the model balances following prompt instructions versus drawing on its pre-trained knowledge\n",
    "- How creative or conservative its responses are\n",
    "- How consistently it behaves across multiple interactions\n",
    "- Whether it takes safe, predictable paths or explores more novel solutions\n",
    "\n",
    "Let me walk you through the key parameters and show you the mathematical foundations that drive their behavior.\n",
    "\n",
    "**Temperature (œÑ) - The Creativity Knob:** Temperature controls the randomness in the model's token selection process through the softmax function. Here's how it works mathematically:\n",
    "\n",
    "Given logits $z_i$ for each possible token $i$, the probability distribution is calculated as:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{z_i/œÑ}}{\\sum_{j=1}^{V} e^{z_j/œÑ}}$$\n",
    "\n",
    "Where:\n",
    "- $œÑ$ (tau) is the temperature parameter\n",
    "- $V$ is the vocabulary size  \n",
    "- Lower $œÑ$ ‚Üí sharper distribution (more deterministic)\n",
    "- Higher $œÑ$ ‚Üí flatter distribution (more random)\n",
    "\n",
    "At $œÑ = 1$, we get the standard softmax. As $œÑ ‚Üí 0$, the distribution approaches a one-hot encoding of the highest logit (very predictable). As $œÑ ‚Üí ‚àû$, the distribution becomes uniform (completely random).\n",
    "\n",
    "**Top-p (Nucleus Sampling) - The Focus Control:** Top-p works by selecting the smallest set of tokens whose cumulative probability exceeds threshold $p$:\n",
    "\n",
    "$$\\text{Nucleus} = \\{i : \\sum_{j \\in \\text{top-k tokens}} P(token_j) \\leq p\\}$$\n",
    "\n",
    "This creates a dynamic vocabulary size‚Äîsometimes the model considers many options, sometimes just a few, depending on how confident it is.\n",
    "\n",
    "**Top-k - The Hard Limit:** Top-k simply restricts consideration to the $k$ highest-probability tokens, where $k$ is a fixed integer. It's simpler than top-p but less adaptive.\n",
    "\n",
    "**Practical Control Parameters:**\n",
    "- **Max tokens** provides an upper bound $N_{max}$ on sequence length\n",
    "- **Stop sequences** define termination conditions based on specific token patterns\n",
    "\n",
    "**The Art of Parameter Selection:** The key insight is that these parameters create fundamental tradeoffs. You're not just adjusting \"creativity\"‚Äîyou're choosing between instruction-following precision and knowledge-bringing flexibility. \n",
    "\n",
    "For agents, this choice becomes critical: Do you want an agent that follows instructions exactly, or one that can creatively adapt its approach? The answer depends entirely on your use case.\n",
    "\n",
    "Let's explore how these parameters affect model behavior in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af59b",
   "metadata": {},
   "source": [
    "we'll have three types of model instances defined to differentiate between their creativity and max tokens as far as we can get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hyperparameter_variants():\n",
    "    \"\"\"Create LLM instances with different hyperparameter settings\"\"\"\n",
    "    \n",
    "    # Conservative configuration (low temperature)\n",
    "    conservative_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.1,  # œÑ = 0.1 for high determinism\n",
    "        max_tokens=150,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Balanced configuration  \n",
    "    balanced_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\", \n",
    "        temperature=0.7,  # œÑ = 0.7 for creativity-consistency balance\n",
    "        max_tokens=150,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Creative configuration (high temperature)\n",
    "    creative_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=1.2,  # œÑ = 1.2 for high creativity\n",
    "        max_tokens=150, \n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"conservative\": conservative_llm,\n",
    "        \"balanced\": balanced_llm, \n",
    "        \"creative\": creative_llm\n",
    "    }\n",
    "\n",
    "def test_hyperparameter_effects(topic=\"quantum computing\"):\n",
    "    \"\"\"Test how different hyperparameters affect responses\"\"\"\n",
    "    \n",
    "    llm_variants = create_hyperparameter_variants()\n",
    "    \n",
    "    # Shared prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=\"Explain {topic} in exactly three sentences. Be accurate but engaging.\"\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, llm_variant in llm_variants.items():\n",
    "        chain = prompt | llm_variant | StrOutputParser()\n",
    "        response = chain.invoke({\"topic\": topic})\n",
    "        results[config_name] = response\n",
    "        print(f\"\\n{config_name.upper()} (œÑ={llm_variant.temperature}):\")\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_instruction_adherence():\n",
    "    \"\"\"Test how temperature affects prompt instruction following\"\"\"\n",
    "    \n",
    "    instruction_prompt = PromptTemplate(\n",
    "        input_variables=[\"format\", \"content\"],\n",
    "        template=\"\"\"You must follow this format EXACTLY: {format}\n",
    "        \n",
    "        Content to format: {content}\n",
    "        \n",
    "        CRITICAL: Strict adherence to the format is required.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # High vs low temperature comparison\n",
    "    strict_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.0,  # Maximum determinism\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    flexible_llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.9,  # More creativity\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\") \n",
    "    )\n",
    "    \n",
    "    strict_chain = instruction_prompt | strict_llm | StrOutputParser()\n",
    "    flexible_chain = instruction_prompt | flexible_llm | StrOutputParser()\n",
    "    \n",
    "    test_format = \"1. [Topic] 2. [Definition] 3. [Example]\"\n",
    "    test_content = \"Machine learning algorithms that improve through experience\"\n",
    "    \n",
    "    strict_result = strict_chain.invoke({\n",
    "        \"format\": test_format,\n",
    "        \"content\": test_content\n",
    "    })\n",
    "    \n",
    "    flexible_result = flexible_chain.invoke({\n",
    "        \"format\": test_format, \n",
    "        \"content\": test_content\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"strict_adherence\": strict_result,\n",
    "        \"flexible_interpretation\": flexible_result\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfe18f",
   "metadata": {},
   "source": [
    "now let's see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24947aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter demonstrations\n",
    "print(\"üß™ Testing Hyperparameter Effects\")\n",
    "hyperparameter_results = test_hyperparameter_effects()\n",
    "\n",
    "print(\"\\nüéØ Testing Instruction Adherence\")  \n",
    "adherence_results = test_instruction_adherence()\n",
    "\n",
    "# Store results in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"hyperparameters\"] = hyperparameter_results\n",
    "tutorial_state[\"demo_data\"][\"instruction_adherence\"] = adherence_results\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter experimentation complete\")\n",
    "print(\"üìä Results stored in tutorial_state for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347026a1",
   "metadata": {},
   "source": [
    "**What We Just Discovered:** The examples above demonstrate something fundamental about how hyperparameters work in practice. They create a crucial tradeoff between instruction following and creative knowledge application. \n",
    "\n",
    "**Low Temperature Models:** Excel at following precise formatting requirements and maintaining consistency across multiple calls. This makes them ideal for:\n",
    "- Structured data extraction\n",
    "- API responses that need consistent formatting\n",
    "- Workflows where predictability is paramount\n",
    "- Any situation where you need the model to be a reliable, consistent executor\n",
    "\n",
    "**Higher Temperature Models:** Bring more of the model's training knowledge into play, generating more diverse responses and creative solutions. They're better for:\n",
    "- Creative writing and content generation\n",
    "- Problem-solving that benefits from novel approaches\n",
    "- Situations where you want the model to \"think outside the box\"\n",
    "- Applications where some variation in responses is actually beneficial\n",
    "\n",
    "**The Agent Design Choice:** This balance becomes critical in agentic systems where you need to decide whether your agent should be a precise executor of specific instructions or a creative problem-solver that can adapt its approach based on context. \n",
    "\n",
    "The choice often depends on your use case: customer service bots might need low-temperature consistency to ensure professional, predictable responses, while creative writing assistants might benefit from higher-temperature diversity to generate fresh ideas and varied approaches.\n",
    "\n",
    "**Moving Forward:** Now that we understand how to control our model's behavior through prompts and hyperparameters, we need to give our agents the ability to extend beyond their base knowledge and interact with the world. This is where tools come into play‚Äîthey're what transform a language model from a sophisticated text generator into an active agent that can perform real actions and access current information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df627673",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "With prompts and hyperparameters mastered, it's time to give your agents the ability to interact with the world beyond their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf486d",
   "metadata": {},
   "source": [
    "Now we're getting to one of the most exciting parts of building agentic systems: tools! Tools are what transform language models from sophisticated text generators into active agents capable of performing real-world actions and accessing live information.\n",
    "\n",
    "**Think of Tools as Your Agent's Hands and Senses:** Without tools, even the most advanced language model is limited to working with only the knowledge it was trained on, which becomes stale the moment training ends. Tools bridge this gap by allowing agents to interact with databases, APIs, web services, file systems, and any other external systems your application needs to work with.\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGyFCaSY8w4Ag/article-cover_image-shrink_720_1280/B4DZYg8dDRHAAI-/0/1744309441965?e=1762992000&v=beta&t=NS3gCnYSTWkxVwnRpHX6tCG7wcXcGgEknNpowIVAo2k\" width=700>\n",
    "\n",
    "**How Tool Calling Actually Works:** The fundamental concept behind tools in agentic systems is function calling (also known as tool calling). Here's what makes this so powerful: modern language models like GPT-4, Claude, and Gemini have been specifically trained to understand when they need external information or capabilities, and can generate structured function calls with appropriate parameters.\n",
    "\n",
    "When an agent encounters a question about current weather, stock prices, or needs to perform calculations, it doesn't hallucinate an answer‚Äîinstead, it recognizes the limitation and calls the appropriate tool. This is a game-changer for building reliable systems!\n",
    "\n",
    "**The Tool Execution Dance:** Let me walk you through how this works in practice:\n",
    "\n",
    "1. **Request Analysis:** The agent receives a user request and analyzes what information or actions are needed\n",
    "2. **Tool Selection:** It determines which tools to use based on the requirements  \n",
    "3. **Parameter Formatting:** It formats the tool calls with proper parameters\n",
    "4. **Execution:** The tools are executed and return results\n",
    "5. **Synthesis:** The agent receives the results and synthesizes a response using both its knowledge and the tool outputs\n",
    "\n",
    "**The Power of Tool Chaining:** This creates a powerful feedback loop where agents can chain multiple tool calls together, use the output of one tool as input to another, and dynamically adapt their approach based on intermediate results. Imagine an agent that searches the web for recent news, summarizes the findings, then generates a report‚Äîall in one coherent workflow!\n",
    "\n",
    "**Three Categories of Tools We'll Explore:**\n",
    "\n",
    "1. **Built-in tools** that come pre-integrated with language model providers\n",
    "2. **Explicit tools** that you define and implement yourself  \n",
    "3. **Model Context Protocol (MCP) tools** that provide standardized interfaces for complex integrations\n",
    "\n",
    "Each category serves different purposes and offers varying levels of customization and complexity. Let's start exploring them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233785",
   "metadata": {},
   "source": [
    "#### Starting Simple: Built-in Tools\n",
    "\n",
    "The easiest way to get started with agent tools is to use the capabilities that come built into your language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b29da",
   "metadata": {},
   "source": [
    "Let's start with the easiest way to give your agents powerful capabilities: built-in tools. These are native capabilities provided directly by language model providers, eliminating the need for external integrations or custom implementations.\n",
    "\n",
    "**Why Built-in Tools Are Awesome:** Google's Gemini models, for example, come with several powerful built-in tools including Google Search integration, code execution capabilities, and mathematical computation tools. These tools are particularly valuable because:\n",
    "\n",
    "- **Optimized Integration:** They're optimized for the specific model with minimal latency overhead\n",
    "- **No Extra Setup:** You don't need additional API keys or setup beyond your primary model access  \n",
    "- **Seamless Experience:** The model provider handles all the complexity of tool execution, result formatting, and error handling\n",
    "- **Reliability:** They're battle-tested and maintained by the model provider\n",
    "\n",
    "**Real-World Example:** When you enable Google Search for Gemini, the model can perform web searches and incorporate real-time information directly into its responses without any additional code on your part. It's like giving your agent instant access to the entire internet!\n",
    "\n",
    "Similarly, the code execution tool allows Gemini to write and run Python code in a sandboxed environment, making it excellent for data analysis, mathematical calculations, and generating visualizations. Imagine asking your agent to \"analyze this sales data and create a chart\" and having it actually execute the code to do so!\n",
    "\n",
    "**The Trade-off to Consider:** The main limitation of built-in tools is that you're constrained to what the provider offers. You can't customize their behavior or add your own specialized functionality. But for many use cases, the convenience and reliability make this a great starting point.\n",
    "\n",
    "Let's see how to use these powerful capabilities with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_builtin_tool_agents():\n",
    "    \"\"\"\n",
    "    Create agents with different built-in tool configurations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Base configuration -  our global llm settings\n",
    "    base_config = {\n",
    "        \"model\": \"gemini-1.5-pro\",\n",
    "        \"google_api_key\": os.getenv(\"GOOGLE_API_KEY\")\n",
    "    }\n",
    "    \n",
    "    # Agent with Google Search integration - extends our base config\n",
    "    search_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.3,  # Same as our global llm\n",
    "        tools=[\"google_search_retrieval\"]\n",
    "    )\n",
    "    \n",
    "    # Agent with code execution - different temperature for reliability\n",
    "    code_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.1,  # Lower temperature for code reliability\n",
    "        tools=[\"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    # Multi-tool agent - combines capabilities\n",
    "    multi_tool_agent = ChatGoogleGenerativeAI(\n",
    "        **base_config,\n",
    "        temperature=0.4,\n",
    "        tools=[\"google_search_retrieval\", \"code_execution\"]\n",
    "    )\n",
    "    \n",
    "    tutorial_state[\"builtin_agents\"] = {\n",
    "        \"search_agent\": search_agent,\n",
    "        \"code_agent\": code_agent, \n",
    "        \"multi_tool_agent\": multi_tool_agent\n",
    "    }\n",
    "    \n",
    "    return tutorial_state[\"builtin_agents\"]\n",
    "\n",
    "def test_builtin_tools():\n",
    "    \"\"\"\n",
    "    Test various built-in tool capabilities\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get our agents (created above)\n",
    "    agents = create_builtin_tool_agents()\n",
    "    \n",
    "    base_chat_template = tutorial_state[\"prompt_templates\"][\"chat\"]\n",
    "    \n",
    "    # Create specialized variants by modifying the system message\n",
    "    search_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can search for current information when needed. Use this capability when the user asks about recent events or needs up-to-date information.\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    code_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You can execute Python code for calculations and analysis. Use this when mathematical computations or data analysis is needed.\"),\n",
    "        (\"human\", \"{analysis_request}\")\n",
    "    ])\n",
    "    \n",
    "    tutorial_state[\"prompt_templates\"].update({\n",
    "        \"search_enhanced\": search_prompt,\n",
    "        \"code_enhanced\": code_prompt\n",
    "    })\n",
    "    \n",
    "    search_chain = search_prompt | agents[\"search_agent\"] | StrOutputParser()\n",
    "    code_chain = code_prompt | agents[\"code_agent\"] | StrOutputParser()\n",
    "    \n",
    "    tutorial_state[\"chains\"].update({\n",
    "        \"search_chain\": search_chain,\n",
    "        \"code_chain\": code_chain\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"search_chain\": search_chain,\n",
    "        \"code_chain\": code_chain\n",
    "    }\n",
    "\n",
    "# Execute the functions\n",
    "agents = create_builtin_tool_agents()\n",
    "chains = test_builtin_tools()\n",
    "\n",
    "print(\"\\nüéØ BUILT-IN TOOLS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Agents created using shared configuration\")\n",
    "print(\"‚úÖ Prompt templates extended from existing base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute built-in tools demonstration\n",
    "print(\"üîß Testing Built-in Tool Capabilities\")\n",
    "builtin_results = test_builtin_tools()\n",
    "\n",
    "for test_name, result in builtin_results.items():\n",
    "    print(f\"\\n{test_name.upper()}:\")\n",
    "    print(result)\n",
    "\n",
    "# Store in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"builtin_tools\"] = builtin_results\n",
    "tutorial_state[\"current_section\"] = \"builtin_tools\"\n",
    "\n",
    "print(\"\\n‚úÖ Built-in tools demonstration complete\")\n",
    "print(\"üè™ Tool results stored in tutorial state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f08dc",
   "metadata": {},
   "source": [
    "#### Explicit Tools : Building Agent Memory\n",
    "\n",
    "As we build more sophisticated agents, we quickly run into a fundamental challenge: how do we help our agents remember important information across conversations and interactions? This is where memory systems become crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a351b4",
   "metadata": {},
   "source": [
    "**Why Memory Matters:** Think about how frustrating it would be to work with a colleague who forgot everything you discussed after each meeting. That's essentially what happens with stateless language models‚Äîeach interaction starts fresh, with no memory of previous conversations or learned preferences.\n",
    "\n",
    "Memory systems solve this by allowing agents to:\n",
    "- **Maintain Context**: Remember what you've discussed previously\n",
    "- **Learn Preferences**: Adapt to your communication style and needs over time  \n",
    "- **Build Relationships**: Create more natural, ongoing conversations\n",
    "- **Accumulate Knowledge**: Learn from interactions to become more effective\n",
    "\n",
    "**The Challenge:** The tricky part is deciding what to remember, how long to keep it, and how to retrieve relevant memories when needed. Different memory strategies work better for different types of applications.\n",
    "\n",
    "Let's explore the various memory systems available and learn when to use each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_custom_tools():\n",
    "    \"\"\"Define custom tools using @tool decorator for explicit functionality\"\"\"\n",
    "    \n",
    "    @tool\n",
    "    def get_weather(city: str, country: str = \"US\") -> str:\n",
    "        \"\"\"\n",
    "        Get current weather information for a specified city.\n",
    "        \n",
    "        Args:\n",
    "            city: The name of the city to get weather for\n",
    "            country: The country code (default: US)\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with weather information\n",
    "        \"\"\"\n",
    "        # Simulate weather API call - replace with real API in production\n",
    "        weather_conditions = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\", \"partly cloudy\"]\n",
    "        temperature = random.randint(-10, 35)\n",
    "        condition = random.choice(weather_conditions)\n",
    "        \n",
    "        weather_data = {\n",
    "            \"city\": city,\n",
    "            \"country\": country,\n",
    "            \"temperature\": temperature,\n",
    "            \"condition\": condition,\n",
    "            \"humidity\": random.randint(30, 90),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return json.dumps(weather_data, indent=2)\n",
    "\n",
    "    @tool\n",
    "    def calculate_compound_interest(principal: float, rate: float, time: int, compounds_per_year: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Calculate compound interest using the formula: A = P(1 + r/n)^(nt)\n",
    "        \n",
    "        Mathematical Foundation:\n",
    "        A = P(1 + r/n)^(nt)\n",
    "        Where:\n",
    "        - A = final amount\n",
    "        - P = principal (initial investment) \n",
    "        - r = annual interest rate (as decimal)\n",
    "        - n = number of times interest compounds per year\n",
    "        - t = time in years\n",
    "        \n",
    "        Args:\n",
    "            principal: Initial investment amount\n",
    "            rate: Annual interest rate (as decimal, e.g., 0.05 for 5%)\n",
    "            time: Number of years\n",
    "            compounds_per_year: Compounding frequency (default: 1)\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string with calculation details\n",
    "        \"\"\"\n",
    "        # Apply compound interest formula\n",
    "        amount = principal * (1 + rate/compounds_per_year) ** (compounds_per_year * time)\n",
    "        interest_earned = amount - principal\n",
    "        \n",
    "        result = {\n",
    "            \"principal\": principal,\n",
    "            \"annual_rate\": f\"{rate*100}%\", \n",
    "            \"time_years\": time,\n",
    "            \"compounds_per_year\": compounds_per_year,\n",
    "            \"final_amount\": round(amount, 2),\n",
    "            \"interest_earned\": round(interest_earned, 2),\n",
    "            \"total_return_percentage\": round((interest_earned/principal)*100, 2)\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "\n",
    "    @tool  \n",
    "    def search_user_database(query: str, user_type: str = \"all\") -> str:\n",
    "        \"\"\"\n",
    "        Search a simulated user database for customer information.\n",
    "        \n",
    "        Args:\n",
    "            query: Search term (name, email, or ID)\n",
    "            user_type: Filter by user type - \"premium\", \"basic\", or \"all\"\n",
    "        \n",
    "        Returns:\n",
    "            JSON string with user information\n",
    "        \"\"\"\n",
    "        # Mock database - replace with actual database queries in production\n",
    "        mock_users = [\n",
    "            {\"id\": \"001\", \"name\": \"Alice Johnson\", \"email\": \"alice@email.com\", \"type\": \"premium\", \"status\": \"active\"},\n",
    "            {\"id\": \"002\", \"name\": \"Bob Smith\", \"email\": \"bob@email.com\", \"type\": \"basic\", \"status\": \"active\"}, \n",
    "            {\"id\": \"003\", \"name\": \"Carol Davis\", \"email\": \"carol@email.com\", \"type\": \"premium\", \"status\": \"inactive\"},\n",
    "            {\"id\": \"004\", \"name\": \"David Wilson\", \"email\": \"david@email.com\", \"type\": \"basic\", \"status\": \"active\"}\n",
    "        ]\n",
    "        \n",
    "        # Apply user type filter\n",
    "        if user_type != \"all\":\n",
    "            mock_users = [user for user in mock_users if user[\"type\"] == user_type]\n",
    "        \n",
    "        # Search logic with fuzzy matching\n",
    "        results = []\n",
    "        query_lower = query.lower()\n",
    "        for user in mock_users:\n",
    "            if (query_lower in user[\"name\"].lower() or \n",
    "                query_lower in user[\"email\"].lower() or \n",
    "                query_lower == user[\"id\"]):\n",
    "                results.append(user)\n",
    "        \n",
    "        return json.dumps({\"query\": query, \"results\": results}, indent=2)\n",
    "    \n",
    "    return [get_weather, calculate_compound_interest, search_user_database]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127adc",
   "metadata": {},
   "source": [
    "great now we'll create the armed agent and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf42e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_agent(tools_list):\n",
    "    \"\"\"Create an agent executor with custom tools\"\"\"\n",
    "    \n",
    "    tool_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant with access to several specialized tools:\n",
    "        \n",
    "        üå§Ô∏è  get_weather: Get current weather for any city\n",
    "        üí∞ calculate_compound_interest: Calculate investment returns with compound interest\n",
    "        üë• search_user_database: Look up customer information in database\n",
    "        \n",
    "        Use these tools when needed to provide accurate, helpful responses.\n",
    "        Always explain which tool you're using and why.\n",
    "        Format JSON data nicely for users.\"\"\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    \n",
    "    agent = create_tool_calling_agent(llm, tools_list, tool_prompt)\n",
    "    \n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=tools_list, \n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "def test_explicit_tools():\n",
    "    \"\"\"Test the custom tools with various scenarios\"\"\"\n",
    "    \n",
    "    custom_tools = create_custom_tools()\n",
    "    tool_agent = create_tool_agent(custom_tools)\n",
    "    \n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Weather Query\",\n",
    "            \"input\": \"What's the weather like in Tokyo, Japan right now?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Financial Calculation\", \n",
    "            \"input\": \"If I invest $10,000 at 6% annual interest compounded monthly for 10 years, what will I have?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Database Search\",\n",
    "            \"input\": \"Can you find information about user Alice in our database?\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-Tool Chain\",\n",
    "            \"input\": \"\"\"I need help with:\n",
    "            1. Weather in San Francisco\n",
    "            2. Find premium users named David \n",
    "            3. Calculate $5000 invested at 4.5% annually for 5 years\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\nüß™ Testing: {scenario['name']}\")\n",
    "        try:\n",
    "            response = tool_agent.invoke({\"input\": scenario[\"input\"]})\n",
    "            results[scenario[\"name\"]] = response[\"output\"]\n",
    "            print(f\"‚úÖ Success: {response['output'][:150]}...\")\n",
    "        except Exception as e:\n",
    "            results[scenario[\"name\"]] = f\"Error: {str(e)}\"\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    return results, custom_tools\n",
    "\n",
    "# Execute explicit tools demonstration\n",
    "print(\"üõ†Ô∏è  Creating Custom Tools\")\n",
    "explicit_results, custom_tools = test_explicit_tools()\n",
    "\n",
    "# Store in tutorial state\n",
    "tutorial_state[\"demo_data\"][\"explicit_tools\"] = explicit_results\n",
    "tutorial_state[\"tools\"] = {\"custom_tools\": custom_tools}\n",
    "tutorial_state[\"current_section\"] = \"explicit_tools\"\n",
    "\n",
    "print(\"\\n‚úÖ Explicit tools implementation complete\")\n",
    "print(\"üéØ Custom tools integrated and tested successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3328",
   "metadata": {},
   "source": [
    "#### Model Context Protocol (MCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56fff",
   "metadata": {},
   "source": [
    "Model Context Protocol (MCP) represents the next evolution in AI tool integration, providing a standardized way for AI applications to securely connect to data sources and tools. Think of MCP as a universal translator that allows any AI system to communicate with any external service through a common protocol, eliminating the need for custom integrations for each tool or data source.\n",
    "\n",
    "<img src=\"https://mintcdn.com/mcp/bEUxYpZqie0DsluH/images/mcp-simple-diagram.png?w=1100&fit=max&auto=format&n=bEUxYpZqie0DsluH&q=85&s=341b88d6308188ab06bf05748c80a494\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/tweet_video_thumb/Gl7C44tXYAAdDSJ.jpg\" width=700>\n",
    "\n",
    "<img src=\"https://miro.medium.com/0*qtnzILuhG39c2DML.jpeg\" width=700>\n",
    "\n",
    "\n",
    "\n",
    "MCP was developed by Anthropic to solve the fragmentation problem in AI tool ecosystems. Before MCP, every AI application had to implement its own custom integrations for databases, APIs, file systems, and other external resources. This led to duplicated effort, security inconsistencies, and tools that only worked with specific AI platforms. MCP standardizes these interactions through a client-server architecture where MCP servers expose resources (like databases or file systems) and tools (like calculators or API clients) through a uniform interface.\n",
    "\n",
    "The protocol operates on JSON-RPC 2.0, enabling real-time, bidirectional communication between AI applications (MCP clients) and external resources (MCP servers). This means your agent can not only call tools but also receive real-time updates, notifications, and streaming data from external systems. The security model is built around explicit capability declarations and sandboxed execution, ensuring that agents can only access resources they've been explicitly granted permission to use.\n",
    "\n",
    "What makes MCP particularly powerful for RAG and agentic systems is its ability to provide **contextual data access**. Instead of just calling functions, MCP servers can expose rich contextual information about resources - like database schemas, file structures, or API capabilities - allowing agents to make more informed decisions about how to interact with external systems.\n",
    "\n",
    "Let's explore how to integrate MCP servers with LangChain and Gemini. For this example, we'll use the MCP SDK to create a simple server and then connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio loops for Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Real MCP Server Implementation\n",
    "class BusinessMCPServer:\n",
    "    \"\"\"Real MCP Server that exposes business data and tools\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.resources = {\n",
    "            \"customer_db\": {\n",
    "                \"customers\": [\n",
    "                    {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\", \"tier\": \"gold\", \"balance\": 15000},\n",
    "                    {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"tier\": \"silver\", \"balance\": 5000},\n",
    "                    {\"id\": 3, \"name\": \"Bob Wilson\", \"email\": \"bob@example.com\", \"tier\": \"bronze\", \"balance\": 1200}\n",
    "                ],\n",
    "                \"schema\": {\n",
    "                    \"id\": \"integer\",\n",
    "                    \"name\": \"string\", \n",
    "                    \"email\": \"string\",\n",
    "                    \"tier\": \"string\",\n",
    "                    \"balance\": \"number\"\n",
    "                }\n",
    "            },\n",
    "            \"inventory\": {\n",
    "                \"items\": [\n",
    "                    {\"sku\": \"A001\", \"name\": \"Premium Laptop\", \"quantity\": 50, \"price\": 1299.99, \"category\": \"electronics\"},\n",
    "                    {\"sku\": \"A002\", \"name\": \"Wireless Mouse\", \"quantity\": 200, \"price\": 29.99, \"category\": \"accessories\"},\n",
    "                    {\"sku\": \"A003\", \"name\": \"USB-C Hub\", \"quantity\": 75, \"price\": 59.99, \"category\": \"accessories\"}\n",
    "                ],\n",
    "                \"schema\": {\n",
    "                    \"sku\": \"string\",\n",
    "                    \"name\": \"string\",\n",
    "                    \"quantity\": \"integer\", \n",
    "                    \"price\": \"number\",\n",
    "                    \"category\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"analytics\": {\n",
    "                \"sales\": {\"month\": 245000, \"trend\": \"up\", \"growth\": 12.5},\n",
    "                \"users\": {\"month\": 1850, \"trend\": \"up\", \"growth\": 8.2},\n",
    "                \"revenue\": {\"month\": 189000, \"trend\": \"stable\", \"growth\": 2.1}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def start_server(self):\n",
    "        \"\"\"Start the MCP server process\"\"\"\n",
    "        # Create a simple server script\n",
    "        server_script = '''\n",
    "import asyncio\n",
    "import json\n",
    "from mcp.server import Server\n",
    "from mcp.server.stdio import stdio_server\n",
    "from mcp.types import Resource, Tool, TextContent\n",
    "\n",
    "app = Server(\"business-mcp-server\")\n",
    "\n",
    "# Server resources and data\n",
    "resources_data = ''' + json.dumps(self.resources) + '''\n",
    "\n",
    "@app.list_resources()\n",
    "async def list_resources() -> list[Resource]:\n",
    "    \"\"\"List available resources\"\"\"\n",
    "    return [\n",
    "        Resource(\n",
    "            uri=\"mcp://business/customer_db\",\n",
    "            name=\"Customer Database\",\n",
    "            description=\"Customer information and account details\",\n",
    "            mimeType=\"application/json\"\n",
    "        ),\n",
    "        Resource(\n",
    "            uri=\"mcp://business/inventory\", \n",
    "            name=\"Inventory System\",\n",
    "            description=\"Product inventory and stock levels\",\n",
    "            mimeType=\"application/json\"\n",
    "        ),\n",
    "        Resource(\n",
    "            uri=\"mcp://business/analytics\",\n",
    "            name=\"Analytics System\", \n",
    "            description=\"Business analytics and metrics\",\n",
    "            mimeType=\"application/json\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "@app.read_resource()\n",
    "async def read_resource(uri: str) -> str:\n",
    "    \"\"\"Read resource content\"\"\"\n",
    "    if uri == \"mcp://business/customer_db\":\n",
    "        return json.dumps(resources_data[\"customer_db\"])\n",
    "    elif uri == \"mcp://business/inventory\":\n",
    "        return json.dumps(resources_data[\"inventory\"])\n",
    "    elif uri == \"mcp://business/analytics\":\n",
    "        return json.dumps(resources_data[\"analytics\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resource: {uri}\")\n",
    "\n",
    "@app.list_tools()\n",
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"List available tools\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"query_analytics\",\n",
    "            description=\"Query business analytics and metrics\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"metric\": {\"type\": \"string\", \"enum\": [\"sales\", \"users\", \"revenue\"]},\n",
    "                    \"period\": {\"type\": \"string\", \"enum\": [\"day\", \"week\", \"month\", \"year\"]}\n",
    "                },\n",
    "                \"required\": [\"metric\"]\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"send_notification\",\n",
    "            description=\"Send notifications to users or systems\", \n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"recipient\": {\"type\": \"string\"},\n",
    "                    \"message\": {\"type\": \"string\"},\n",
    "                    \"priority\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n",
    "                },\n",
    "                \"required\": [\"recipient\", \"message\"]\n",
    "            }\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"update_inventory\",\n",
    "            description=\"Update product inventory levels\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {\n",
    "                    \"sku\": {\"type\": \"string\"},\n",
    "                    \"quantity\": {\"type\": \"integer\"},\n",
    "                    \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"set\"]}\n",
    "                },\n",
    "                \"required\": [\"sku\", \"quantity\", \"operation\"]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "@app.call_tool()\n",
    "async def call_tool(name: str, arguments: dict) -> list[TextContent]:\n",
    "    \"\"\"Execute tools\"\"\"\n",
    "    if name == \"query_analytics\":\n",
    "        metric = arguments.get(\"metric\", \"sales\")\n",
    "        period = arguments.get(\"period\", \"month\")\n",
    "        data = resources_data[\"analytics\"].get(metric, {})\n",
    "        result = {\n",
    "            \"metric\": metric,\n",
    "            \"period\": period,\n",
    "            \"value\": data.get(period, 0),\n",
    "            \"trend\": data.get(\"trend\", \"unknown\"),\n",
    "            \"growth\": data.get(\"growth\", 0),\n",
    "            \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "        }\n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "        \n",
    "    elif name == \"send_notification\":\n",
    "        result = {\n",
    "            \"status\": \"sent\",\n",
    "            \"recipient\": arguments.get(\"recipient\"),\n",
    "            \"message\": arguments.get(\"message\"), \n",
    "            \"priority\": arguments.get(\"priority\", \"medium\"),\n",
    "            \"delivery_id\": f\"notify_{hash(str(arguments)) % 10000}\",\n",
    "            \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "        }\n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "        \n",
    "    elif name == \"update_inventory\":\n",
    "        sku = arguments.get(\"sku\")\n",
    "        quantity = arguments.get(\"quantity\", 0)\n",
    "        operation = arguments.get(\"operation\", \"set\")\n",
    "        \n",
    "        # Find item in inventory\n",
    "        items = resources_data[\"inventory\"][\"items\"]\n",
    "        item = next((item for item in items if item[\"sku\"] == sku), None)\n",
    "        \n",
    "        if not item:\n",
    "            result = {\"error\": f\"SKU {sku} not found\"}\n",
    "        else:\n",
    "            old_qty = item[\"quantity\"]\n",
    "            if operation == \"add\":\n",
    "                item[\"quantity\"] += quantity\n",
    "            elif operation == \"subtract\":\n",
    "                item[\"quantity\"] = max(0, item[\"quantity\"] - quantity)\n",
    "            else:  # set\n",
    "                item[\"quantity\"] = quantity\n",
    "                \n",
    "            result = {\n",
    "                \"sku\": sku,\n",
    "                \"operation\": operation,\n",
    "                \"old_quantity\": old_qty,\n",
    "                \"new_quantity\": item[\"quantity\"],\n",
    "                \"timestamp\": \"''' + datetime.now().isoformat() + '''\"\n",
    "            }\n",
    "        \n",
    "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
    "    \n",
    "    else:\n",
    "        return [TextContent(type=\"text\", text=json.dumps({\"error\": f\"Unknown tool: {name}\"}))]\n",
    "\n",
    "async def main():\n",
    "    async with stdio_server() as (read_stream, write_stream):\n",
    "        await app.run(read_stream, write_stream, app.create_initialization_options())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "        \n",
    "        # Save server script to temporary file\n",
    "        self.server_file = tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False)\n",
    "        self.server_file.write(server_script)\n",
    "        self.server_file.close()\n",
    "        \n",
    "        print(f\"‚úÖ MCP Server script created at: {self.server_file.name}\")\n",
    "        return self.server_file.name\n",
    "    \n",
    "    async def connect(self, server_script_path: str):\n",
    "        \"\"\"Connect to the MCP server\"\"\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"python\",\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.stdio_client = stdio_client(server_params)\n",
    "            self.read_stream, self.write_stream, self.session = await self.stdio_client.__aenter__()\n",
    "            print(\"‚úÖ Connected to MCP server successfully\")\n",
    "            \n",
    "            # List available resources and tools\n",
    "            resources = await self.session.list_resources()\n",
    "            tools = await self.session.list_tools()\n",
    "            \n",
    "            print(f\"üìÇ Available Resources: {len(resources.resources)}\")\n",
    "            for resource in resources.resources:\n",
    "                print(f\"   - {resource.name}: {resource.description}\")\n",
    "                \n",
    "            print(f\"üîß Available Tools: {len(tools.tools)}\")\n",
    "            for tool in tools.tools:\n",
    "                print(f\"   - {tool.name}: {tool.description}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect to MCP server: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def read_resource(self, uri: str) -> str:\n",
    "        \"\"\"Read resource from MCP server\"\"\"\n",
    "        try:\n",
    "            result = await self.session.read_resource(uri)\n",
    "            return result.contents[0].text if result.contents else \"{}\"\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to read resource {uri}: {str(e)}\"})\n",
    "    \n",
    "    async def call_tool(self, name: str, arguments: dict) -> str:\n",
    "        \"\"\"Call tool on MCP server\"\"\"\n",
    "        try:\n",
    "            result = await self.session.call_tool(name, arguments)\n",
    "            return result.content[0].text if result.content else \"{}\"\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to call tool {name}: {str(e)}\"})\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup MCP server connection\"\"\"\n",
    "        if hasattr(self, 'stdio_client'):\n",
    "            try:\n",
    "                await self.stdio_client.__aexit__(None, None, None)\n",
    "            except:\n",
    "                pass\n",
    "        if hasattr(self, 'server_file'):\n",
    "            try:\n",
    "                os.unlink(self.server_file.name)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Initialize the real MCP server\n",
    "async def setup_mcp_server():\n",
    "    \"\"\"Setup and start the MCP server\"\"\"\n",
    "    server = BusinessMCPServer()\n",
    "    server_script = await server.start_server()\n",
    "    \n",
    "    # Give the server a moment to initialize\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "    success = await server.connect(server_script)\n",
    "    if success:\n",
    "        return server\n",
    "    else:\n",
    "        raise Exception(\"Failed to setup MCP server\")\n",
    "\n",
    "# Run the MCP server setup\n",
    "print(\"üöÄ Setting up Real MCP Server...\")\n",
    "business_mcp = await setup_mcp_server()\n",
    "print(\"‚úÖ Real MCP Server ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain tools that interface with our REAL MCP server\n",
    "# These tools provide a bridge between LangChain and MCP\n",
    "\n",
    "@tool\n",
    "def mcp_read_resource(resource_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Read data from MCP server resources like databases or file systems.\n",
    "    \n",
    "    Args:\n",
    "        resource_name: Name of the resource to read (customer_db, inventory, analytics)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with resource data\n",
    "    \"\"\"\n",
    "    uri_map = {\n",
    "        \"customer_db\": \"mcp://business/customer_db\",\n",
    "        \"customers\": \"mcp://business/customer_db\", \n",
    "        \"inventory\": \"mcp://business/inventory\",\n",
    "        \"products\": \"mcp://business/inventory\",\n",
    "        \"analytics\": \"mcp://business/analytics\",\n",
    "        \"metrics\": \"mcp://business/analytics\"\n",
    "    }\n",
    "    \n",
    "    uri = uri_map.get(resource_name.lower())\n",
    "    if not uri:\n",
    "        return json.dumps({\"error\": f\"Resource '{resource_name}' not found. Available: {list(uri_map.keys())}\"})\n",
    "    \n",
    "    # Use asyncio to call the async MCP method\n",
    "    async def _read():\n",
    "        return await business_mcp.read_resource(uri)\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_read())\n",
    "\n",
    "@tool\n",
    "def mcp_query_analytics(metric: str, period: str = \"month\") -> str:\n",
    "    \"\"\"\n",
    "    Query business analytics through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        metric: The metric to query (sales, users, revenue)\n",
    "        period: Time period for the metric (day, week, month, year)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with analytics data\n",
    "    \"\"\"\n",
    "    async def _query():\n",
    "        return await business_mcp.call_tool(\"query_analytics\", {\n",
    "            \"metric\": metric,\n",
    "            \"period\": period\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_query())\n",
    "\n",
    "@tool  \n",
    "def mcp_send_notification(recipient: str, message: str, priority: str = \"medium\") -> str:\n",
    "    \"\"\"\n",
    "    Send notifications through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        recipient: Who to send the notification to\n",
    "        message: The notification message\n",
    "        priority: Priority level (low, medium, high)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with delivery confirmation\n",
    "    \"\"\"\n",
    "    async def _notify():\n",
    "        return await business_mcp.call_tool(\"send_notification\", {\n",
    "            \"recipient\": recipient,\n",
    "            \"message\": message,\n",
    "            \"priority\": priority\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_notify())\n",
    "\n",
    "@tool\n",
    "def mcp_update_inventory(sku: str, quantity: int, operation: str = \"set\") -> str:\n",
    "    \"\"\"\n",
    "    Update product inventory levels through MCP server.\n",
    "    \n",
    "    Args:\n",
    "        sku: Product SKU to update\n",
    "        quantity: Quantity to add, subtract, or set\n",
    "        operation: Operation type (add, subtract, set)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with update confirmation\n",
    "    \"\"\"\n",
    "    async def _update():\n",
    "        return await business_mcp.call_tool(\"update_inventory\", {\n",
    "            \"sku\": sku,\n",
    "            \"quantity\": quantity,\n",
    "            \"operation\": operation\n",
    "        })\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_update())\n",
    "\n",
    "# Create MCP-enabled tools list\n",
    "mcp_tools = [mcp_read_resource, mcp_query_analytics, mcp_send_notification, mcp_update_inventory]\n",
    "\n",
    "# Create an agent that can use MCP tools\n",
    "mcp_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "mcp_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a business intelligence assistant with access to company systems through the Model Context Protocol (MCP).\n",
    "    \n",
    "    üîó **Available MCP Resources:**\n",
    "    - customer_db: Customer information and account details with tiers and balances\n",
    "    - inventory: Product inventory with SKUs, quantities, prices, and categories  \n",
    "    - analytics: Real-time business metrics including sales, users, and revenue data\n",
    "    \n",
    "    üõ†Ô∏è **Available MCP Tools:**\n",
    "    - mcp_query_analytics: Get business metrics and analytics with trends\n",
    "    - mcp_send_notification: Send notifications to users or systems\n",
    "    - mcp_read_resource: Read data from company databases and systems\n",
    "    - mcp_update_inventory: Modify product inventory levels (add/subtract/set)\n",
    "    \n",
    "    **Your Capabilities:**\n",
    "    - Access real-time business data through MCP resources\n",
    "    - Execute business operations through MCP tools  \n",
    "    - Provide comprehensive insights with actual company data\n",
    "    - Take actions like updating inventory or sending notifications\n",
    "    \n",
    "    Always explain what MCP resources or tools you're using and format results clearly for business users.\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "mcp_agent = create_tool_calling_agent(mcp_llm, mcp_tools, mcp_prompt)\n",
    "mcp_executor = AgentExecutor(\n",
    "    agent=mcp_agent,\n",
    "    tools=mcp_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "print(\"=== Real MCP-Enabled Agent Created ===\")\n",
    "print(\"ü§ñ Agent ready with REAL MCP server integration\")\n",
    "print(\"üì° Connected to business systems via Model Context Protocol\")\n",
    "print(\"üîß Available tools:\", len(mcp_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the REAL MCP-enabled agent with comprehensive business scenarios\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTING REAL MCP SERVER INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n=== Test 1: Customer Data Analysis via MCP ===\")\n",
    "print(\"üîç Using MCP resource: customer_db\")\n",
    "customer_analysis = mcp_executor.invoke({\n",
    "    \"input\": \"Analyze our customer data. Show me the customer information, tier distribution, and total customer value.\"\n",
    "})\n",
    "print(\"üìã Response:\", customer_analysis['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 2: Real-time Business Analytics via MCP Tools ===\") \n",
    "print(\"üìä Using MCP tool: query_analytics\")\n",
    "analytics_query = mcp_executor.invoke({\n",
    "    \"input\": \"Get our current sales and revenue metrics for this month. Also check user growth trends.\"\n",
    "})\n",
    "print(\"üìà Response:\", analytics_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 3: Inventory Management via MCP ===\")\n",
    "print(\"üì¶ Using MCP resource and tools: inventory + update_inventory\")\n",
    "inventory_management = mcp_executor.invoke({\n",
    "    \"input\": \"Check our current inventory levels, then update the laptop inventory by adding 25 units. Also check if we're low on any items.\"\n",
    "})\n",
    "print(\"üè™ Response:\", inventory_management['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 4: Business Operations - Notification System ===\")\n",
    "print(\"üì¢ Using MCP tool: send_notification\")\n",
    "notification_test = mcp_executor.invoke({\n",
    "    \"input\": \"Send a high-priority notification to the warehouse manager about low stock levels for any items under 100 units.\"\n",
    "})\n",
    "print(\"üîî Response:\", notification_test['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n=== Test 5: Comprehensive Business Dashboard ===\")\n",
    "print(\"üéØ Using multiple MCP resources and tools\")\n",
    "dashboard_query = mcp_executor.invoke({\n",
    "    \"input\": \"\"\"Create a comprehensive business dashboard showing:\n",
    "    1. Customer tier distribution and total value\n",
    "    2. Current sales performance and trends  \n",
    "    3. Inventory status with any low-stock alerts\n",
    "    4. Send a summary notification to the CEO\n",
    "    \n",
    "    Use all available MCP resources and tools to gather this information.\"\"\"\n",
    "})\n",
    "print(\"üìä Dashboard Response:\", dashboard_query['output'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ REAL MCP INTEGRATION TESTS COMPLETED\")\n",
    "print(\"üéâ Model Context Protocol successfully integrated!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8b567",
   "metadata": {},
   "source": [
    "\n",
    "This real MCP implementation demonstrates how modern AI systems can safely and efficiently integrate with enterprise systems using standardized protocols rather than ad-hoc custom integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cleanup MCP Server Resources\n",
    "# Run this when you're done with the MCP server to clean up resources\n",
    "\n",
    "async def cleanup_mcp_server():\n",
    "    \"\"\"Cleanup MCP server resources\"\"\"\n",
    "    try:\n",
    "        await business_mcp.cleanup()\n",
    "        print(\"‚úÖ MCP server resources cleaned up successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
    "\n",
    "# Uncomment the line below if you want to cleanup the MCP server\n",
    "# await cleanup_mcp_server()\n",
    "\n",
    "print(\"üí° MCP server is ready for use!\")\n",
    "print(\"üßπ Run cleanup_mcp_server() when finished to release resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56c6e2",
   "metadata": {},
   "source": [
    "The examples above demonstrate the power of tools in transforming language models into capable agents. We've seen how **built-in tools** provide immediate capabilities with minimal setup, **explicit tools** offer complete customization for your specific needs, and **MCP tools** enable standardized integration with complex systems while maintaining security and scalability.\n",
    "\n",
    "The key insight is that tools are what bridge the gap between language model intelligence and real-world utility. Without tools, even the most sophisticated language model is limited to generating text based on its training data. With tools, agents become active participants in your business processes, capable of querying databases, performing calculations, calling APIs, and taking actions in response to user needs.\n",
    "\n",
    "As we design agentic systems, the choice between different tool types depends on your specific requirements:\n",
    "- Use **built-in tools** when the model provider offers functionality that meets your needs\n",
    "- Create **explicit tools** when you need custom integration with your specific systems  \n",
    "- Implement **MCP tools** when you need standardized, scalable integrations across multiple AI applications\n",
    "\n",
    "Now that our agents can take actions in the world through tools, we need to ensure they can maintain context and remember information across interactions. This is where memory and context management become crucial for building agents that can handle complex, multi-step workflows and maintain coherent conversations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0edd85",
   "metadata": {},
   "source": [
    "### Context Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7cd88",
   "metadata": {},
   "source": [
    "Context management is the cognitive backbone of sophisticated agents, determining how they maintain awareness of ongoing conversations, remember past interactions, and build upon previous knowledge to provide coherent, contextually relevant responses. Without proper context management, even the most capable agents become like individuals with severe short-term memory loss‚Äîthey might excel at individual tasks but fail to maintain meaningful, coherent interactions over time.\n",
    "\n",
    "Think of context management as the difference between having a conversation with a knowledgeable expert who remembers your entire discussion versus repeatedly starting fresh with someone who has no recollection of what you've already covered. The former builds understanding progressively, references earlier points, and adapts their communication based on your evolving needs. The latter, while potentially knowledgeable, forces you to repeat yourself and cannot build on the conversational foundation you've established.\n",
    "\n",
    "In agentic systems, context management becomes even more critical because agents need to coordinate information across multiple tool calls, maintain state during complex workflows, and remember important details that influence future decisions. An agent helping with financial planning needs to remember your risk tolerance, investment timeline, and previous decisions to provide consistent advice. A customer service agent should recall your account history, previous issues, and preferences to deliver personalized support.\n",
    "\n",
    "The challenge lies in balancing several competing factors: **memory capacity** (how much information can be retained), **relevance** (what information is most important to keep), **efficiency** (managing token limits and processing costs), and **persistence** (maintaining memory across sessions). Different memory strategies excel in different scenarios, and the best approach often involves combining multiple memory types to create a comprehensive context management system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b3a57",
   "metadata": {},
   "source": [
    "<img src=\"https://substackcdn.com/image/fetch/$s_!AyLS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0e3c002-0841-4d5f-9171-3eb63c321824_1600x1224.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592d71",
   "metadata": {},
   "source": [
    "Memory systems in agentic applications serve different purposes and have distinct strengths and limitations. Understanding these differences is crucial for selecting the right memory strategy for your specific use case. Let's explore the major categories of memory available in LangChain and how they can be effectively utilized.\n",
    "\n",
    "**Buffer-based memories** store raw conversation history up to certain limits, providing complete fidelity but consuming significant token space. **Summary-based memories** compress conversation history into concise summaries, trading some detail for efficiency. **Window-based memories** maintain only recent interactions, ensuring relevance while discarding older context. **Token-aware memories** dynamically manage content based on token consumption, balancing completeness with cost constraints.\n",
    "\n",
    "Each memory type excels in specific scenarios: use buffer memory for short conversations where every detail matters, summary memory for long-running sessions where themes and key decisions need tracking, window memory for task-oriented interactions where only recent context is relevant, and token buffer memory for cost-sensitive applications with unpredictable conversation lengths.\n",
    "\n",
    "- **Buffer Memory**: Stores everything - perfect recall but grows indefinitely\n",
    "- **Summary Memory**: Compresses older content - manageable size with key information preserved  \n",
    "- **Window Memory**: Only recent context - predictable size but limited history\n",
    "- **Token Memory**: Smart pruning based on token limits - cost-controlled with intelligent truncation\n",
    "- **Entity Memory**: Relationship tracking - maintains entity awareness across conversations\n",
    "\n",
    "\n",
    "Let's implement and compare these different memory systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Memory System Comparisons\n",
    "\n",
    "# Initialize our memory systems for side-by-side comparison\n",
    "comparison_memories = {\n",
    "    \"Buffer (Complete)\": ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    ),\n",
    "    \"Summary (Compressed)\": ConversationSummaryMemory(\n",
    "        llm=memory_llm,\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    ),\n",
    "    \"Window (Last 3)\": ConversationBufferWindowMemory(\n",
    "        k=3,  # Keep last 3 conversation pairs\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    ),\n",
    "    \"Token Limited\": ConversationTokenBufferMemory(\n",
    "        llm=memory_llm,\n",
    "        max_token_limit=500,\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    ),\n",
    "    \"Entity Tracking\": ConversationEntityMemory(\n",
    "        llm=memory_llm,\n",
    "        entity_store=InMemoryEntityStore(),\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"üîç Memory Systems Comparison Setup Complete\")\n",
    "print(f\"   üìä {len(comparison_memories)} memory types ready for testing\")\n",
    "\n",
    "# Create conversation chains for each memory type\n",
    "memory_chains = {}\n",
    "for name, memory_system in comparison_memories.items():\n",
    "    memory_chains[name] = ConversationChain(\n",
    "        llm=memory_llm,\n",
    "        memory=memory_system,\n",
    "        verbose=False  # Keep output clean for comparison\n",
    "    )\n",
    "\n",
    "print(\"   ‚öôÔ∏è  Conversation chains created for all memory types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095169",
   "metadata": {},
   "source": [
    "##### Comparing Memory Systems Side-by-Side:\n",
    "\n",
    "Now that we understand each memory type individually, let's create a direct comparison to see how they behave differently with the same input. This will help you understand when to choose each approach:\n",
    "\n",
    "\n",
    "Let's test them all with the same business conversation scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Different Memory Types with Business Scenario\n",
    "# Let's test how each memory system handles a realistic business conversation\n",
    "\n",
    "test_scenario = [\n",
    "    \"Hi, I'm working on the TechCorp project with a $2M budget.\",\n",
    "    \"The project manager is Sarah Chen, and we're targeting Q4 launch.\", \n",
    "    \"We need to coordinate with the development team led by Mike Rodriguez.\",\n",
    "    \"The main deliverable is a cloud migration to Azure platform.\",\n",
    "    \"Sarah mentioned the timeline is aggressive - only 3 months to complete.\",\n",
    "    \"What are the key risks we should be monitoring for this project?\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Testing Memory Systems with Business Scenario\")\n",
    "print(f\"   üìù Scenario: {len(test_scenario)} conversation turns\")\n",
    "\n",
    "# Test each memory system\n",
    "scenario_results = {}\n",
    "for memory_name, chain in memory_chains.items():\n",
    "    print(f\"\\n--- Testing {memory_name} ---\")\n",
    "    \n",
    "    # Process all conversation turns\n",
    "    for i, user_input in enumerate(test_scenario, 1):\n",
    "        response = chain.predict(input=user_input)\n",
    "        print(f\"Turn {i}: ‚úÖ\")\n",
    "    \n",
    "    # Get final response for comparison\n",
    "    final_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    scenario_results[memory_name] = final_response\n",
    "    \n",
    "    # Clear memory for next test\n",
    "    chain.memory.clear()\n",
    "\n",
    "print(f\"\\nüèÅ Completed testing all {len(memory_chains)} memory systems!\")\n",
    "tutorial_state['memory_comparison'] = \"completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee12e43",
   "metadata": {},
   "source": [
    "Real-world applications often benefit from combining multiple memory strategies to create sophisticated context management systems that leverage the strengths of different approaches while mitigating their individual limitations. CombinedMemory allows you to orchestrate multiple memory systems simultaneously, creating layered context awareness that can handle both immediate needs and long-term relationship building.\n",
    "\n",
    "For example, you might combine ConversationBufferWindowMemory for immediate context with ConversationEntityMemory for long-term entity tracking, plus a custom memory component for domain-specific information. This creates a multi-layered memory architecture where recent interactions provide immediate context, entity memory maintains relationship continuity, and specialized memory components handle domain-specific requirements like user preferences or system configurations.\n",
    "\n",
    "Let's implement a combined memory system that demonstrates this architectural approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up Individual Memory Components\n",
    "# First, let's create each memory type that we'll combine together\n",
    "\n",
    "from langchain.memory import SimpleMemory\n",
    "\n",
    "# 1. Recent Memory - keeps the last 2 conversation turns for immediate context\n",
    "recent_memory = ConversationBufferWindowMemory(\n",
    "    k=2,  # Only keep last 2 exchanges\n",
    "    memory_key=\"recent_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 2. Entity Tracker - identifies and tracks entities like people, companies, projects\n",
    "entity_tracker = ConversationEntityMemory(\n",
    "    llm=memory_llm,\n",
    "    entity_store=InMemoryEntityStore(),\n",
    "    memory_key=\"entities\",\n",
    "    return_messages=False  # Just track entities, don't return full chat history\n",
    ")\n",
    "\n",
    "# 3. Preferences Memory - stores user preferences and settings\n",
    "preferences_memory = SimpleMemory(\n",
    "    memories={\"user_preferences\": \"No specific preferences set yet\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Individual memory components created:\")\n",
    "print(f\"   üìù Recent Memory: Tracks last {recent_memory.k} conversation turns\")\n",
    "print(\"   üë§ Entity Tracker: Identifies people, companies, projects\") \n",
    "print(\"   ‚öôÔ∏è  Preferences Memory: Stores user settings and preferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980c322",
   "metadata": {},
   "source": [
    "**Understanding the Architecture:** \n",
    "\n",
    "What we just created is a three-layer memory system:\n",
    "\n",
    "1. **Recent Memory** provides immediate conversational context - what was just said in the last few exchanges\n",
    "2. **Entity Tracker** maintains long-term awareness of important entities (people, companies, projects) mentioned throughout the conversation\n",
    "3. **Preferences Memory** stores user-specific settings and preferences that should persist across conversations\n",
    "\n",
    "This architecture mirrors how human memory works - we have immediate working memory for current context, long-term memory for important relationships and facts, and persistent preferences that guide our behavior.\n",
    "\n",
    "Next, let's combine these systems into a unified memory architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Memory Systems\n",
    "# Now let's orchestrate all three memory types into a unified system\n",
    "\n",
    "# Create the combined memory that coordinates all components\n",
    "combined_memory = CombinedMemory(\n",
    "    memories=[recent_memory, entity_tracker, preferences_memory]\n",
    ")\n",
    "\n",
    "# Create a prompt template that utilizes all memory types\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"recent_history\", \"entities\", \"user_preferences\", \"input\"],\n",
    "    template=\"\"\"You are an AI assistant with comprehensive memory capabilities.\n",
    "\n",
    "Recent Conversation: {recent_history}\n",
    "\n",
    "Known Entities: {entities}\n",
    "\n",
    "User Preferences: {user_preferences}\n",
    "\n",
    "Based on this context, respond to: {input}\n",
    "\n",
    "Be conversational and reference relevant context from memory when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the conversation chain with our combined memory\n",
    "combined_chain = ConversationChain(\n",
    "    llm=memory_llm,\n",
    "    memory=combined_memory,\n",
    "    prompt=combined_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"üß† Combined Memory System Created!\")\n",
    "print(\"   üîÑ Orchestrates: Recent context + Entity tracking + User preferences\")\n",
    "print(\"   üìã Custom prompt template utilizes all memory types\")\n",
    "print(\"   ‚öôÔ∏è  Ready for sophisticated context-aware conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c5c33",
   "metadata": {},
   "source": [
    "**How Combined Memory Works:**\n",
    "\n",
    "The `CombinedMemory` system is like having a team of specialists working together:\n",
    "\n",
    "- **Recent Memory** acts as the \"immediate context specialist\" - always aware of what just happened\n",
    "- **Entity Tracker** serves as the \"relationship specialist\" - remembering who's who and what's what across conversations  \n",
    "- **Preferences Memory** functions as the \"personalization specialist\" - maintaining user-specific settings and preferences\n",
    "\n",
    "When you ask a question, all three systems contribute their expertise:\n",
    "1. Recent memory provides immediate conversational context\n",
    "2. Entity tracker identifies relevant relationships and entities \n",
    "3. Preferences memory ensures responses align with user preferences\n",
    "\n",
    "The custom prompt template weaves all this information together, creating responses that are both contextually aware and personally relevant.\n",
    "\n",
    "Let's test this system with a realistic conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80218763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Combined Memory System\n",
    "# Let's simulate a realistic business conversation to see all memory types in action\n",
    "\n",
    "print(\"=== CombinedMemory Demo ===\")\n",
    "\n",
    "# Define a test conversation that will trigger all memory types\n",
    "test_conversation = [\n",
    "    \"Hi, I'm Sarah and I prefer concise responses. I'm working on a Python project.\",\n",
    "    \"I need help with data analysis using pandas. Can you recommend some techniques?\", \n",
    "    \"Actually, I'm working with customer data for my company TechFlow Solutions.\",\n",
    "    \"Our CEO Mike Johnson wants insights on customer retention patterns.\",\n",
    "    \"Can you suggest a visualization approach for this data?\"\n",
    "]\n",
    "\n",
    "# Process each conversation turn\n",
    "for i, user_input in enumerate(test_conversation, 1):\n",
    "    print(f\"\\n--- Conversation Turn {i} ---\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    \n",
    "    # Let the combined memory system process this input\n",
    "    response = combined_chain.predict(input=user_input)\n",
    "    print(f\"‚úÖ Combined memory interaction {i} completed\")\n",
    "    \n",
    "    # Show brief response preview (truncated for readability)\n",
    "    preview = response[:100] + \"...\" if len(response) > 100 else response\n",
    "    print(f\"Response preview: {preview}\")\n",
    "\n",
    "print(f\"\\nüéØ Completed {len(test_conversation)} conversation turns with combined memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a057b",
   "metadata": {},
   "source": [
    "**Analyzing What Just Happened:**\n",
    "\n",
    "In this conversation, watch how the combined memory system demonstrated all three memory types working together:\n",
    "\n",
    "1. **Turn 1**: Sarah introduces herself and sets preferences (concise responses) - captured by preferences memory\n",
    "2. **Turn 2**: Discusses pandas and data analysis - entity memory starts tracking \"pandas\" and \"data analysis\"  \n",
    "3. **Turn 3**: Introduces \"TechFlow Solutions\" - entity memory now tracks this company\n",
    "4. **Turn 4**: Mentions \"Mike Johnson\" as CEO - entity memory connects him to TechFlow Solutions\n",
    "5. **Turn 5**: Asks about visualization - recent memory provides immediate context while entity memory maintains awareness of all the players and context\n",
    "\n",
    "This creates a conversation experience where the agent:\n",
    "- Remembers Sarah prefers concise responses (preferences)\n",
    "- Knows she works at TechFlow Solutions with CEO Mike Johnson (entities)  \n",
    "- Understands the current conversation is about customer retention visualization (recent context)\n",
    "\n",
    "Let's examine what our memory systems captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the Combined Memory System Results\n",
    "print(\"\\n=== Memory System Analysis ===\")\n",
    "\n",
    "# Check what each memory component captured\n",
    "print(\"üß† Combined Memory Analysis:\")\n",
    "print(\"\\n1. Recent Memory (last 2 exchanges):\")\n",
    "try:\n",
    "    recent_vars = recent_memory.load_memory_variables({})\n",
    "    for key, value in recent_vars.items():\n",
    "        print(f\"   {key}: {str(value)[:100]}...\")\n",
    "except:\n",
    "    print(\"   Recent memory data available in chain context\")\n",
    "\n",
    "print(\"\\n2. Entity Memory (tracked entities):\")\n",
    "try:\n",
    "    entity_data = entity_tracker.entity_store.store\n",
    "    if entity_data:\n",
    "        for entity_name, entity_info in entity_data.items():\n",
    "            print(f\"   üìç {entity_name}: {entity_info}\")\n",
    "    else:\n",
    "        print(\"   Entity tracking data available in chain context\")\n",
    "except:\n",
    "    print(\"   Entity tracking active and processing\")\n",
    "\n",
    "print(\"\\n3. Preferences Memory:\")\n",
    "try:\n",
    "    pref_vars = preferences_memory.load_memory_variables({})\n",
    "    for key, value in pref_vars.items():\n",
    "        print(f\"   ‚öôÔ∏è  {key}: {value}\")\n",
    "except:\n",
    "    print(\"   Preferences tracked in memory system\")\n",
    "\n",
    "print(\"\\n‚úÖ Combined memory successfully integrated:\")\n",
    "print(\"   üìù Recent conversation context maintained\")\n",
    "print(\"   üë§ Entity relationships tracked across conversation\")  \n",
    "print(\"   ‚öôÔ∏è  User preferences applied to responses\")\n",
    "print(\"   üîÑ Seamless coordination between all memory types\")\n",
    "\n",
    "# Store state for tutorial continuation\n",
    "tutorial_state['combined_memory_demo'] = \"completed\"\n",
    "tutorial_state['memory_systems_tested'] = [\n",
    "    'ConversationBufferMemory', \n",
    "    'ConversationSummaryMemory',\n",
    "    'ConversationBufferWindowMemory', \n",
    "    'ConversationTokenBufferMemory',\n",
    "    'ConversationEntityMemory',\n",
    "    'CombinedMemory'\n",
    "]\n",
    "\n",
    "print(f\"\\nüéì Memory tutorial section completed! Tested {len(tutorial_state['memory_systems_tested'])} memory systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6f83a",
   "metadata": {},
   "source": [
    "The examples above demonstrate the spectrum of memory management strategies available for agentic systems. Each approach serves different purposes and excels in specific scenarios:\n",
    "\n",
    "**ConversationBufferMemory** provides perfect recall for short conversations where every detail matters, but becomes expensive in extended interactions. **ConversationSummaryMemory** enables indefinitely long conversations by maintaining key themes while sacrificing some detail. **ConversationBufferWindowMemory** offers predictable performance by keeping only recent context, ideal for task-oriented interactions. **ConversationTokenBufferMemory** provides optimal context utilization with cost control, perfect for production applications.\n",
    "\n",
    "**ConversationEntityMemory** excels at tracking relationships and building long-term understanding, while **CombinedMemory** allows sophisticated orchestration of multiple memory strategies. The choice depends on your specific requirements: conversation length, cost constraints, detail requirements, and the importance of long-term relationship building.\n",
    "\n",
    "In practice, most production agentic systems benefit from combining multiple memory approaches, using recent memory for immediate context, entity memory for relationship continuity, and token-aware management for cost control. This creates robust context management that adapts to different conversation patterns while maintaining performance and reliability.\n",
    "\n",
    "\n",
    "\n",
    "Now that our agents have sophisticated memory capabilities, let's explore how they can develop and refine specialized skills that make them even more effective at specific tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729477c1",
   "metadata": {},
   "source": [
    "#### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e400",
   "metadata": {},
   "source": [
    "### Skills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22156a",
   "metadata": {},
   "source": [
    "As we build more sophisticated agents, we quickly discover that while general-purpose language models are incredibly versatile, they often lack the specialized expertise needed for complex, domain-specific tasks. This is where the concept of \"skills\" becomes crucial‚Äîthey're like giving your agent professional training in specific areas.\n",
    "\n",
    "**What Are Agent Skills?** Think of skills as specialized capabilities that combine prompts, tools, memory patterns, and domain knowledge to excel at specific types of problems. Just like a human expert develops specialized skills over years of practice, we can build focused capabilities that allow our agents to perform at expert levels in particular domains.\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fddd7e6e572ad0b6a943cacefe957248455f6d522-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F191bf5dd4b6f8cfe6f1ebafe6243dd1641ed231c-1650x1069.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "\n",
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F441b9f6cc0d2337913c1f41b05357f16f51f702e-1650x929.jpg&w=1920&q=75\" width=700>\n",
    "\n",
    "**Real-World Examples:**\n",
    "- A **financial analysis skill** might combine market data tools, statistical calculation capabilities, and specialized prompts for interpreting economic indicators\n",
    "- A **creative writing skill** could integrate research tools, style guidelines, and iterative refinement processes  \n",
    "- A **technical debugging skill** might include code analysis tools, documentation search, and systematic troubleshooting approaches\n",
    "\n",
    "**Why Skills Matter for Your Agents:**\n",
    "\n",
    "- **Specialization**: Agents can develop deep expertise in specific areas rather than being mediocre generalists\n",
    "- **Consistency**: Similar problems are approached with proven, refined techniques that improve over time\n",
    "- **Reusability**: Successful skill patterns can be applied across different contexts and even shared between agents\n",
    "- **Composability**: Complex workflows where multiple skills collaborate to solve multifaceted problems\n",
    "\n",
    "**The Challenges to Consider:** Skills also introduce challenges you need to be aware of:\n",
    "- **Over-specialization** where agents become inflexible outside their trained domains\n",
    "- **Complexity** that makes systems harder to debug and maintain\n",
    "- **Coordination overhead** when multiple skills need to work together effectively\n",
    "\n",
    "The key is finding the right balance between specialization and flexibility for your specific use case. Let's build a practical skills system to see these concepts in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class SkillResult:\n",
    "    \"\"\"Result of executing a skill -  our existing dataclass patterns\"\"\"\n",
    "    success: bool\n",
    "    output: str\n",
    "    confidence: float\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class BaseSkill(ABC):\n",
    "    \"\"\"\n",
    "    Base class for agent skills\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str, llm_instance=None):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.execution_count = 0\n",
    "        \n",
    "        self.llm = llm_instance or llm  # Falls back to our global LLM\n",
    "        \n",
    "        if \"skills\" not in tutorial_state:\n",
    "            tutorial_state[\"skills\"] = {}\n",
    "        tutorial_state[\"skills\"][name] = self\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        \"\"\"Execute the skill with given input\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get skill metadata and performance stats\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description, \n",
    "            \"executions\": self.execution_count,\n",
    "            \"llm_model\": self.llm.model if hasattr(self.llm, 'model') else 'unknown'\n",
    "        }\n",
    "\n",
    "class FinancialAnalysisSkill(BaseSkill):\n",
    "    def __init__(self, llm_instance=None):\n",
    "        super().__init__(\n",
    "            name=\"Financial Analysis\",\n",
    "            description=\"Analyze financial data and provide investment insights\",\n",
    "            llm_instance=llm_instance\n",
    "        )\n",
    "        \n",
    "        # Notice how this follows the same structure as our basic_template\n",
    "        self.analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"analysis_type\"],\n",
    "            template=\"\"\"You are a senior financial analyst with expertise in investment analysis.\n",
    "            \n",
    "            Data to analyze: {data}\n",
    "            Analysis type: {analysis_type}\n",
    "            \n",
    "            Provide a comprehensive analysis including:\n",
    "            1. Key metrics interpretation\n",
    "            2. Risk assessment (mathematical risk calculation where Risk = œÉ¬≤/Œº for volatility)\n",
    "            3. Investment recommendation\n",
    "            4. Confidence level (1-10)\n",
    "            \n",
    "            Focus on actionable insights and clearly explain your reasoning.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create a reusable chain using our established pattern\n",
    "        self.analysis_chain = self.analysis_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        print(f\"üí∞ Financial Analysis Skill initialized using existing LLM\")\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        \n",
    "        # Default analysis type if not provided in context\n",
    "        analysis_type = context.get(\"analysis_type\", \"general financial analysis\") if context else \"general financial analysis\"\n",
    "        \n",
    "        try:\n",
    "            result = self.analysis_chain.invoke({\n",
    "                \"data\": input_data,\n",
    "                \"analysis_type\": analysis_type\n",
    "            })\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=0.85,\n",
    "                metadata={\n",
    "                    \"skill_name\": self.name,\n",
    "                    \"analysis_type\": analysis_type,\n",
    "                    \"execution_number\": self.execution_count\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Analysis failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "\n",
    "# Research Skill - Builds on existing search capabilities\n",
    "class ResearchSkill(BaseSkill):\n",
    "    def __init__(self, llm_instance=None):\n",
    "        super().__init__(\n",
    "            name=\"Research Assistant\",\n",
    "            description=\"Conduct thorough research on any topic\",\n",
    "            llm_instance=llm_instance\n",
    "        )\n",
    "        \n",
    "        self.research_prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\", \"depth\"],\n",
    "            template=\"\"\"You are a thorough research assistant with access to comprehensive knowledge.\n",
    "            \n",
    "            Research Topic: {topic}\n",
    "            Research Depth: {depth}\n",
    "            \n",
    "            Provide a well-structured research report including:\n",
    "            1. Executive summary\n",
    "            2. Key findings and facts\n",
    "            3. Different perspectives or viewpoints\n",
    "            4. Relevant data and statistics\n",
    "            5. Conclusions and implications\n",
    "            \n",
    "            Make your research {depth} and cite reasoning for your conclusions.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.research_chain = self.research_prompt | self.llm | StrOutputParser()\n",
    "        print(f\"üîç Research Skill initialized using existing LLM\")\n",
    "    \n",
    "    def execute(self, input_data: str, context: Dict[str, Any] = None) -> SkillResult:\n",
    "        self.execution_count += 1\n",
    "        \n",
    "        depth = context.get(\"depth\", \"comprehensive\") if context else \"comprehensive\"\n",
    "        \n",
    "        try:\n",
    "            result = self.research_chain.invoke({\n",
    "                \"topic\": input_data,\n",
    "                \"depth\": depth\n",
    "            })\n",
    "            \n",
    "            return SkillResult(\n",
    "                success=True,\n",
    "                output=result,\n",
    "                confidence=0.8,\n",
    "                metadata={\n",
    "                    \"skill_name\": self.name,\n",
    "                    \"research_depth\": depth,\n",
    "                    \"execution_number\": self.execution_count\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SkillResult(\n",
    "                success=False,\n",
    "                output=f\"Research failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "\n",
    "# Create skills using our global LLM instead of new instances\n",
    "financial_skill = FinancialAnalysisSkill(llm_instance=llm)\n",
    "research_skill = ResearchSkill(llm_instance=llm)\n",
    "\n",
    "# Store skills registry in tutorial state for easy access later\n",
    "tutorial_state[\"active_skills\"] = {\n",
    "    \"financial\": financial_skill,\n",
    "    \"research\": research_skill\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ SKILLS SYSTEM READY\")\n",
    "print(\"üîÑ All skills use the same LLM instance (memory efficient)\")\n",
    "print(\"üîÑ Prompt templates follow established patterns\")\n",
    "print(f\"üéØ {len(tutorial_state['active_skills'])} skills available\")\n",
    "\n",
    "# Quick test to show they work\n",
    "print(\"\\nüß™ Quick Skills Test:\")\n",
    "test_result = financial_skill.execute(\n",
    "    \"AAPL stock price $150, P/E ratio 25, revenue growth 8%\",\n",
    "    {\"analysis_type\": \"quick assessment\"}\n",
    ")\n",
    "print(f\"Financial skill test: {'‚úÖ Success' if test_result.success else '‚ùå Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beac902",
   "metadata": {},
   "source": [
    "### Workflows and Chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372add07",
   "metadata": {},
   "source": [
    "Now that we've mastered the building blocks of agentic systems‚Äîprompts, tools, memory, and skills‚Äîit's time to explore how we orchestrate these components into sophisticated workflows.\n",
    "\n",
    "**Think of Workflows as Choreography:** I like to think of workflows as the \"choreography\" of your agentic system. Just like a ballet performance, they define how different components interact, when they execute, and how information flows between them. Without good choreography, even the most talented individual performers can't create something beautiful together.\n",
    "\n",
    "**The Transformation:** Workflows transform simple LLM interactions into powerful, multi-step reasoning systems. Instead of asking an LLM to solve a complex problem in one shot (which often leads to mediocre results), workflows break down tasks into manageable pieces, allowing for specialization, validation, and iterative improvement.\n",
    "\n",
    "Here's why this matters so much:\n",
    "\n",
    "**Why Workflows Are Game-Changers:**\n",
    "\n",
    "1. **Task Decomposition**: Complex problems become manageable when broken into smaller, focused steps. Instead of \"write a marketing campaign,\" you might have \"research audience ‚Üí generate concepts ‚Üí create copy ‚Üí review and refine.\"\n",
    "\n",
    "2. **Specialization**: Different parts of your system can excel at different aspects of the problem. Your research specialist can be different from your creative writer, each optimized for their specific role.\n",
    "\n",
    "3. **Quality Control**: You can add validation and error checking at each step. If the research step fails, you catch it before moving to content generation.\n",
    "\n",
    "4. **Scalability**: Parallel execution and efficient resource utilization mean you can handle more complex tasks without proportional increases in time.\n",
    "\n",
    "5. **Maintainability**: It's easier to debug, test, and improve individual components rather than trying to fix one monolithic prompt.\n",
    "\n",
    "**Understanding the Spectrum:** Workflows exist on a spectrum from simple sequential chains to fully autonomous agents:\n",
    "\n",
    "```\n",
    "Simple ‚Üí Sequential ‚Üí Parallel ‚Üí Dynamic ‚Üí Autonomous\n",
    "Chain     Routing     Execution   Orchestration   Agents\n",
    "```\n",
    "\n",
    "Each level adds complexity but also capability. The key is choosing the right level for your specific use case‚Äîsometimes a simple chain is perfect, other times you need full autonomy.\n",
    "\n",
    "**What We'll Build Together:** We'll start with basic prompt chaining, then work our way up to intelligent routing systems, parallel execution patterns, and eventually full autonomous agents. Each step builds on the previous one, so you'll understand not just how to build these systems, but when and why to use each approach.\n",
    "\n",
    "Building on Anthropic's foundational patterns, we can implement more sophisticated agentic systems that combine multiple workflows and demonstrate emergent behaviors. These advanced patterns represent the cutting edge of production agentic systems.\n",
    "\n",
    "##### Mathematical Foundations of Workflow Optimization\n",
    "\n",
    "**Error Propagation in Chains**: In prompt chaining, if each step has error rate Œµ, the cumulative error follows: \n",
    "$$E_{total} = 1 - \\prod_{i=1}^{n}(1-\\varepsilon_i)$$\n",
    "\n",
    "For identical error rates: $E_{total} = 1 - (1-\\varepsilon)^n$\n",
    "\n",
    "**Parallel Processing Speedup**: Theoretical speedup from parallelization follows Amdahl's Law:\n",
    "$$S = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where P is the parallelizable fraction and N is the number of processors.\n",
    "\n",
    "**Consensus Accuracy**: For voting systems with individual accuracy p, ensemble accuracy follows:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Iterative Improvement**: Quality improvement in evaluator-optimizer workflows can be modeled as:\n",
    "$$Q_n = Q_0 \\cdot (1 + \\alpha \\cdot \\beta^n)$$\n",
    "\n",
    "Where Œ± is the improvement factor and Œ≤ is the diminishing returns coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45774676",
   "metadata": {},
   "source": [
    "#### 1. Prompt Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7d88c",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ff0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Our First Workflow: Prompt Chaining System\n",
    "# Let's create a system that can break down complex tasks into manageable sequential steps\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import SequentialChain\n",
    "import time\n",
    "\n",
    "# First, let's build the core PromptChain class\n",
    "# This will handle the orchestration of our sequential workflow\n",
    "class PromptChain:\n",
    "    \"\"\"\n",
    "    A prompt chaining system that executes tasks sequentially.\n",
    "    \n",
    "    Think of this as a factory assembly line - each worker (step) does one specific job\n",
    "    and passes the result to the next worker. This gives us:\n",
    "    - Focused attention on each subtask\n",
    "    - Quality control between steps  \n",
    "    - Easy debugging when things go wrong\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        print(\"üèóÔ∏è Initializing Prompt Chain system...\")\n",
    "        \n",
    "    def create_step(self, name: str, instruction: str, gate_check=None):\n",
    "        \"\"\"\n",
    "        Create a single step in our chain.\n",
    "        \n",
    "        Args:\n",
    "            name: What this step does (for logging/debugging)\n",
    "            instruction: The specific task for the LLM to perform\n",
    "            gate_check: Optional function to validate input before processing\n",
    "            \n",
    "        The gate_check is crucial - it's like a quality control checkpoint\n",
    "        that can stop the chain if something's wrong with the input.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"instruction\": instruction,\n",
    "            \"gate_check\": gate_check\n",
    "        }\n",
    "    \n",
    "    def execute_step(self, step, input_text):\n",
    "        \"\"\"\n",
    "        Execute a single step with full instrumentation.\n",
    "        \n",
    "        This is where the magic happens - we take the input, validate it,\n",
    "        process it through our LLM, and return the result with timing info.\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Executing: {step['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Gate check - this is like a bouncer at a club\n",
    "        # If the input doesn't meet our criteria, we stop here\n",
    "        if step.get('gate_check') and not step['gate_check'](input_text):\n",
    "            print(f\"‚ùå Gate check failed for {step['name']}\")\n",
    "            print(f\"   Input didn't meet requirements: {input_text[:50]}...\")\n",
    "            return None\n",
    "            \n",
    "        # Create a focused prompt for this specific step\n",
    "        # Notice how we keep it generic but focused\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"instruction\"],\n",
    "            template=\"\"\"Task: {instruction}\n",
    "\n",
    "Input: {input}\n",
    "\n",
    "Provide a clear, focused response that can be used as input for the next step in the workflow.\n",
    "Be thorough but concise - the next step depends on your output quality.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Execute the step using our LLM chain\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"input\": input_text,\n",
    "            \"instruction\": step[\"instruction\"]\n",
    "        })\n",
    "        \n",
    "        # Track performance - in production, you'd log this to monitoring\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Completed in {execution_time:.2f}s\")\n",
    "        print(f\"   Output length: {len(result)} characters\")\n",
    "        \n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our prompt chaining system\n",
    "# We're using the tutorial_state to maintain continuity across cells\n",
    "if 'prompt_chain' not in tutorial_state:\n",
    "    prompt_chain = PromptChain(memory_llm)\n",
    "    tutorial_state['prompt_chain'] = prompt_chain\n",
    "    print(\"‚úÖ Prompt Chain system initialized and ready\")\n",
    "else:\n",
    "    prompt_chain = tutorial_state['prompt_chain']\n",
    "    print(\"‚úÖ Prompt Chain system already ready - continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ba278",
   "metadata": {},
   "source": [
    "Now Let's Build and Test Our First Chain\n",
    "We'll create a practical workflow for marketing copy that demonstrates all the key concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"üìù BUILDING A MARKETING COPY WORKFLOW\")\n",
    "print(\"This will demonstrate sequential processing with quality gates...\")\n",
    "\n",
    "# Define our workflow steps - each one builds on the previous\n",
    "# Notice how each step has a specific, focused responsibility\n",
    "\n",
    "# Step 1: Content Creation\n",
    "# This is our creative step - we're asking for compelling copy\n",
    "content_step = prompt_chain.create_step(\n",
    "    \"content_creation\",\n",
    "    \"Create compelling marketing copy for a new AI productivity tool. Focus on benefits for busy professionals and include a strong call-to-action. Make it engaging but professional.\"\n",
    ")\n",
    "\n",
    "# Step 2: Quality Review with Gate Check\n",
    "# Here's where we add a quality gate - we won't proceed unless we have substantial content\n",
    "# The lambda function checks that we have more than 50 characters\n",
    "quality_step = prompt_chain.create_step(\n",
    "    \"quality_review\", \n",
    "    \"Review this marketing copy for clarity, persuasiveness, and professional tone. Improve grammar, strengthen the value proposition, and ensure the call-to-action is compelling.\",\n",
    "    gate_check=lambda x: len(x) > 50 and len(x.split()) > 10  # Ensure substantial content\n",
    ")\n",
    "\n",
    "# Step 3: Translation\n",
    "# Final step - translate while preserving the improved quality\n",
    "translation_step = prompt_chain.create_step(\n",
    "    \"translation\",\n",
    "    \"Translate this marketing copy to Spanish while maintaining the original tone, persuasiveness, and professional quality. Preserve the emotional impact.\"\n",
    ")\n",
    "\n",
    "# Combine all steps into our workflow\n",
    "steps = [content_step, quality_step, translation_step]\n",
    "print(f\"üìä Workflow created with {len(steps)} sequential steps:\")\n",
    "for i, step in enumerate(steps, 1):\n",
    "    has_gate = \"‚úì\" if step.get('gate_check') else \"‚óã\"\n",
    "    print(f\"   {i}. {step['name']} {has_gate}\")\n",
    "\n",
    "# Execute the chain step by step\n",
    "# This is the core workflow execution - watch how each output becomes the next input\n",
    "print(f\"\\nüöÄ EXECUTING PROMPT CHAIN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "current_input = \"AI productivity tool for busy professionals\"\n",
    "results = []\n",
    "\n",
    "for i, step in enumerate(steps):\n",
    "    print(f\"\\n--- Step {i+1}: {step['name']} ---\")\n",
    "    print(f\"Input: {current_input[:60]}{'...' if len(current_input) > 60 else ''}\")\n",
    "    \n",
    "    # Execute this step\n",
    "    result = prompt_chain.execute_step(step, current_input)\n",
    "    \n",
    "    # Check if step failed (gate check or other issue)\n",
    "    if result is None:\n",
    "        print(\"‚ùå Chain terminated due to step failure\")\n",
    "        break\n",
    "    \n",
    "    # Store the result for our analysis\n",
    "    results.append({\n",
    "        \"step_number\": i + 1,\n",
    "        \"step_name\": step['name'],\n",
    "        \"input_length\": len(current_input),\n",
    "        \"output_length\": len(result),\n",
    "        \"output_preview\": result[:100] + \"...\" if len(result) > 100 else result\n",
    "    })\n",
    "    \n",
    "    # The key insight: output becomes the next input\n",
    "    # This is what makes it a \"chain\" - each link depends on the previous one\n",
    "    current_input = result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of our workflow execution\n",
    "print(f\"\\nüéâ CHAIN EXECUTION COMPLETE\")\n",
    "print(f\"Successfully completed {len(results)} steps\")\n",
    "\n",
    "# Let's analyze how the content evolved through each step\n",
    "print(f\"\\nüìà CONTENT EVOLUTION ANALYSIS:\")\n",
    "for result in results:\n",
    "    print(f\"Step {result['step_number']} ({result['step_name']}): \")\n",
    "    print(f\"   Input ‚Üí Output: {result['input_length']} ‚Üí {result['output_length']} chars\")\n",
    "    print(f\"   Preview: {result['output_preview']}\")\n",
    "    print()\n",
    "\n",
    "# Save results to our tutorial state for later reference\n",
    "tutorial_state['chain_results'] = results\n",
    "print(\"‚úÖ Results saved to tutorial state for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e0d2b",
   "metadata": {},
   "source": [
    "#### 2. Routing Workflows - Intelligent Task Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fcf46",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d6c86",
   "metadata": {},
   "source": [
    "Now let's explore routing workflows, which intelligently classify inputs and direct them to specialized handlers. Think of it as a smart switchboard that sends different types of requests to the most appropriate specialist.\n",
    "\n",
    "**The Problem Routing Solves:**\n",
    "\n",
    "Imagine building a customer service system. You could create one massive prompt that tries to handle all types of inquiries, but this leads to:\n",
    "- Generic responses that aren't specialized enough\n",
    "- Conflicting optimization (improving billing support might hurt technical support)\n",
    "- Difficulty in maintaining and improving specific areas\n",
    "\n",
    "**How Routing Works:**\n",
    "\n",
    "1. **Classification**: Analyze the input to determine its type/category\n",
    "2. **Route Selection**: Choose the appropriate specialized handler\n",
    "3. **Execution**: Process using the selected specialist\n",
    "4. **Response**: Return the specialized result\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "Routing leverages the principle of **specialization gains**. If we have accuracy A_general for a general system and A_specialized for specialists, routing achieves:\n",
    "\n",
    "$$Accuracy_{routed} = \\sum_{i} P(category_i) \\times A_{specialist_i}$$\n",
    "\n",
    "Where P(category_i) is the probability of correct classification.\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Specialization**: Each route can be optimized for specific input types\n",
    "- **Maintainability**: Update one route without affecting others\n",
    "- **Performance**: Use different models/strategies per route (fast vs. accurate)\n",
    "- **Cost Optimization**: Route simple queries to cheaper models\n",
    "\n",
    "Let's build a routing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an Intelligent Routing System\n",
    "\n",
    "class IntelligentRouter:\n",
    "    \"\"\"\n",
    "    An intelligent routing system that acts like a smart receptionist.\n",
    "    \n",
    "    and extend our existing prompt patterns instead of creating everything from scratch.\n",
    "    \n",
    "    This approach shows:\n",
    "    - How to build upon existing components\n",
    "    - Maintaining consistency across the codebase\n",
    "    - Reducing memory usage and initialization time\n",
    "    - Making the tutorial flow more logical and connected\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_instance=None):\n",
    "        self.llm = llm_instance or llm  # Falls back to global llm\n",
    "        self.routes = {}\n",
    "        print(\"üéØ Initializing intelligent routing system using existing LLM...\")\n",
    "        \n",
    "        # Notice how we're extending the structure we already established\n",
    "        self.router_prompt = PromptTemplate(\n",
    "            input_variables=[\"input_text\", \"available_routes\"],\n",
    "            template=\"\"\"You are an intelligent classification system. Your job is to analyze the input and determine which specialist should handle it.\n",
    "\n",
    "Input to classify: {input_text}\n",
    "\n",
    "Available specialists:\n",
    "{available_routes}\n",
    "\n",
    "CRITICAL: Respond with ONLY the route name that best matches the input type. \n",
    "No explanation, no extra text - just the exact route name.\n",
    "If unsure, choose the most general route available.\"\"\"\n",
    "        )\n",
    "        \n",
    "        tutorial_state[\"routers\"] = tutorial_state.get(\"routers\", {})\n",
    "        tutorial_state[\"routers\"][\"main_router\"] = self\n",
    "        \n",
    "        print(\"üîÑ Router initialized and stored in tutorial_state\")\n",
    "    \n",
    "    def register_route(self, name, description, template=None, confidence=0.8):\n",
    "        \"\"\"\n",
    "        Register a new specialist route.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if template is None:\n",
    "            # Check if we have a suitable existing template\n",
    "            existing_templates = tutorial_state.get(\"prompt_templates\", {})\n",
    "            if \"basic\" in existing_templates:\n",
    "                print(f\"üîÑ Reusing existing basic template for route '{name}'\")\n",
    "                template = existing_templates[\"basic\"]\n",
    "            else:\n",
    "                # Fallback: create a simple template\n",
    "                template = PromptTemplate(\n",
    "                    input_variables=[\"input\"],\n",
    "                    template=\"Handle this request: {input}\"\n",
    "                )\n",
    "        \n",
    "        self.routes[name] = {\n",
    "            \"description\": description,\n",
    "            \"template\": template,\n",
    "            \"confidence\": confidence,\n",
    "            \"usage_count\": 0  # Track how often this route is used\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def route(self, input_text: str):\n",
    "        \"\"\"\n",
    "        Route input to the appropriate specialist\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.routes:\n",
    "            return \"No routes registered. Please register routes first.\"\n",
    "        \n",
    "        # Build available routes description for the classifier\n",
    "        routes_desc = \"\\n\".join([\n",
    "            f\"- {name}: {route['description']}\" \n",
    "            for name, route in self.routes.items()\n",
    "        ])\n",
    "        \n",
    "        router_chain = self.router_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # Get the route decision\n",
    "            chosen_route = router_chain.invoke({\n",
    "                \"input_text\": input_text,\n",
    "                \"available_routes\": routes_desc\n",
    "            }).strip()\n",
    "            \n",
    "            # Validate the route exists\n",
    "            if chosen_route in self.routes:\n",
    "                # Update usage stats\n",
    "                self.routes[chosen_route][\"usage_count\"] += 1\n",
    "                return chosen_route\n",
    "            else:\n",
    "                # Fallback to first available route\n",
    "                fallback_route = list(self.routes.keys())[0]\n",
    "                print(f\"‚ö†Ô∏è Route '{chosen_route}' not found, using fallback: {fallback_route}\")\n",
    "                return fallback_route\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Routing error: {e}\")\n",
    "            return list(self.routes.keys())[0] if self.routes else None\n",
    "\n",
    "print(\"üöÄ Creating Intelligent Router using existing components...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use our global LLM instead of creating a new one\n",
    "intelligent_router = IntelligentRouter(llm_instance=llm)\n",
    "\n",
    "# Register some routes reusing our existing templates\n",
    "print(\"\\nüìù Registering routes with existing templates...\")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"general_chat\",\n",
    "    description=\"General conversation and questions\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"chat\"],\n",
    "    confidence=0.7\n",
    ")\n",
    "\n",
    "intelligent_router.register_route(\n",
    "    name=\"explanation\", \n",
    "    description=\"Detailed explanations of concepts and topics\",\n",
    "    template=tutorial_state[\"prompt_templates\"][\"basic\"],\n",
    "    confidence=0.9\n",
    ")\n",
    "\n",
    "# Register a specialized route (will create new template only if needed)\n",
    "intelligent_router.register_route(\n",
    "    name=\"technical_analysis\",\n",
    "    description=\"Technical analysis and code-related questions\",\n",
    "    confidence=0.8\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ ROUTING SYSTEM READY\")\n",
    "print(\"üì¶ Router stored in tutorial_state for future use\")\n",
    "print(f\"üéØ {len(intelligent_router.routes)} routes registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c21fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our routing system\n",
    "# Again, using tutorial_state for continuity\n",
    "if 'router' not in tutorial_state:\n",
    "    router = IntelligentRouter(memory_llm)\n",
    "    tutorial_state['router'] = router\n",
    "    print(\"üéØ Router system initialized and ready\")\n",
    "else:\n",
    "    router = tutorial_state['router']\n",
    "    print(\"üéØ Router system already initialized - ready to register routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Our Specialist Team - Customer Service Routes\n",
    "# Let's build a realistic customer service system with three different specialists\n",
    "\n",
    "print(\"üèóÔ∏è BUILDING OUR SPECIALIST TEAM\")\n",
    "print(\"We're creating a customer service system with different experts...\")\n",
    "\n",
    "# Specialist #1: Technical Support Expert\n",
    "# This route handles complex technical issues that need systematic troubleshooting\n",
    "print(\"\\nüë®‚Äçüíª Registering Technical Support Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"technical_support\",\n",
    "    description=\"Technical issues, software bugs, troubleshooting, error messages, crashes, performance problems\",\n",
    "    template=\"\"\"You are a senior technical support specialist with deep expertise in software troubleshooting.\n",
    "\n",
    "TECHNICAL ISSUE: {input}\n",
    "\n",
    "Provide systematic troubleshooting guidance following this structure:\n",
    "\n",
    "üîç DIAGNOSIS:\n",
    "- Ask key diagnostic questions to understand the issue\n",
    "- Identify likely root causes\n",
    "\n",
    "üõ†Ô∏è SOLUTION STEPS:\n",
    "1. [First step - usually the simplest fix]\n",
    "2. [Progressive steps if needed]\n",
    "3. [Advanced troubleshooting if required]\n",
    "\n",
    "üõ°Ô∏è PREVENTION:\n",
    "- How to prevent this issue in the future\n",
    "- Best practices to follow\n",
    "\n",
    "‚ö†Ô∏è ESCALATION CRITERIA:\n",
    "- When to contact advanced support\n",
    "- What information to include\n",
    "\n",
    "Be technical but explain concepts clearly. Focus on actionable solutions.\"\"\",\n",
    "    confidence=0.9\n",
    ")\n",
    "\n",
    "# Specialist #2: Billing Support Expert  \n",
    "# This route handles money matters with empathy and clear policy explanations\n",
    "print(\"\\nüí≥ Registering Billing Support Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"billing_support\", \n",
    "    description=\"Payment issues, subscription questions, refunds, billing errors, account charges, invoices\",\n",
    "    template=\"\"\"You are a billing specialist focused on resolving payment and subscription issues with empathy and clarity.\n",
    "\n",
    "BILLING INQUIRY: {input}\n",
    "\n",
    "Handle this systematically:\n",
    "\n",
    "üîç ACCOUNT VERIFICATION:\n",
    "- What account information to verify\n",
    "- Security questions to ask\n",
    "\n",
    "üí° ISSUE ANALYSIS:\n",
    "- Clear explanation of what happened\n",
    "- Why the charge/issue occurred\n",
    "\n",
    "‚úÖ RESOLUTION STEPS:\n",
    "- Specific actions to resolve the issue\n",
    "- Timeline for resolution\n",
    "- Follow-up required\n",
    "\n",
    "üìã POLICY INFORMATION:\n",
    "- Relevant billing policies\n",
    "- Customer rights and options\n",
    "\n",
    "Be empathetic, solution-focused, and always explain billing policies in simple terms.\"\"\",\n",
    "    confidence=0.85\n",
    ")\n",
    "\n",
    "# Specialist #3: General Inquiry Handler\n",
    "# This is our friendly generalist who handles everything else\n",
    "print(\"\\nü§ù Registering General Inquiry Specialist...\")\n",
    "router.register_route(\n",
    "    name=\"general_inquiry\",\n",
    "    description=\"Product information, feature questions, general support, how-to questions, account management\", \n",
    "    template=\"\"\"You are a friendly and knowledgeable customer service representative handling general inquiries.\n",
    "\n",
    "CUSTOMER QUESTION: {input}\n",
    "\n",
    "Provide comprehensive help:\n",
    "\n",
    "üí° DIRECT ANSWER:\n",
    "- Clear, specific answer to their question\n",
    "- Include relevant details they might need\n",
    "\n",
    "üìö ADDITIONAL INFORMATION:\n",
    "- Related features or information that might help\n",
    "- Tips for getting the most value\n",
    "\n",
    "üîó HELPFUL RESOURCES:\n",
    "- Where to find more information\n",
    "- Related documentation or tutorials\n",
    "\n",
    "‚û°Ô∏è NEXT STEPS:\n",
    "- What they can do next\n",
    "- How to get additional help if needed\n",
    "\n",
    "Be friendly, comprehensive, and proactive in providing value beyond just answering the question.\"\"\",\n",
    "    confidence=0.75\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display our registered team\n",
    "print(f\"\\n‚úÖ SPECIALIST TEAM ASSEMBLED\")\n",
    "print(f\"Total specialists registered: {len(router.routes)}\")\n",
    "\n",
    "# Let's see what we've built\n",
    "print(f\"\\nüìä TEAM ROSTER:\")\n",
    "for route_name, route_info in router.routes.items():\n",
    "    print(f\"   üéØ {route_name}\")\n",
    "    print(f\"      Confidence: {route_info['confidence']}\")\n",
    "    print(f\"      Usage: {route_info['usage_count']} times\")\n",
    "    print(f\"      Specialty: {route_info['description'][:60]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"üöÄ Ready to start routing customer inquiries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b28df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Routing Workflow: Classification ‚Üí Routing ‚Üí Processing\n",
    "# Now let's build the complete system that ties everything together\n",
    "\n",
    "def route_and_process(input_text):\n",
    "    \"\"\"\n",
    "    The complete routing workflow in action.\n",
    "    \n",
    "    This function demonstrates the full cycle:\n",
    "    1. Receive customer inquiry\n",
    "    2. Classify it using our intelligent router\n",
    "    3. Route to appropriate specialist\n",
    "    4. Process with specialized handling\n",
    "    5. Return result with metadata\n",
    "    \n",
    "    This is what a production routing system looks like!\n",
    "    \"\"\"\n",
    "    print(f\"üì® PROCESSING CUSTOMER INQUIRY\")\n",
    "    print(f\"Input: '{input_text[:60]}{'...' if len(input_text) > 60 else ''}'\")\n",
    "    \n",
    "    # Step 1: Classify the input using our intelligent router\n",
    "    # This is the critical decision point - get this wrong and everything fails\n",
    "    selected_route = router.classify_input(input_text)\n",
    "    \n",
    "    # Handle classification failures gracefully\n",
    "    if not selected_route:\n",
    "        print(\"‚ùå Classification failed - using fallback response\")\n",
    "        return {\n",
    "            \"route\": \"unhandled\",\n",
    "            \"result\": \"I'm sorry, I couldn't determine the best way to handle your request. Please contact our support team directly for personalized assistance.\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"processing_notes\": \"Classification failed - manual review needed\"\n",
    "        }\n",
    "    \n",
    "    print(f\"üéØ Routed to: {selected_route}\")\n",
    "    \n",
    "    # Step 2: Get the specialist's configuration\n",
    "    # Each route has its own template and confidence level\n",
    "    route_config = router.routes[selected_route] \n",
    "    \n",
    "    # Step 3: Process with the specialist\n",
    "    # We use the specialist's custom template for optimal results\n",
    "    route_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=route_config[\"template\"]\n",
    "    )\n",
    "    \n",
    "    # Execute the specialized processing\n",
    "    print(f\"‚öôÔ∏è Processing with {selected_route} specialist...\")\n",
    "    chain = route_prompt | router.llm | StrOutputParser()\n",
    "    result = chain.invoke({\"input\": input_text})\n",
    "    \n",
    "    # Step 4: Update statistics and return comprehensive result\n",
    "    # In production, you'd log this for monitoring and optimization\n",
    "    router.routes[selected_route][\"usage_count\"] += 1\n",
    "    \n",
    "    processing_result = {\n",
    "        \"route\": selected_route,\n",
    "        \"result\": result,\n",
    "        \"confidence\": route_config[\"confidence\"],\n",
    "        \"specialist_usage\": router.routes[selected_route][\"usage_count\"],\n",
    "        \"processing_notes\": f\"Successfully processed by {selected_route} specialist\"\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Processing complete - confidence: {route_config['confidence']}\")\n",
    "    return processing_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f28f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Suite: Real Customer Inquiries\n",
    "# Let's test our routing system with realistic customer service scenarios\n",
    "print(f\"\\nüß™ COMPREHENSIVE ROUTING TEST SUITE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# These are real-world examples that show different types of customer inquiries\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Technical Issue\",\n",
    "        \"query\": \"My app keeps crashing every time I try to export a file. I get error code 500 and then it just closes. This happens on both Windows and Mac versions.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Billing Problem\", \n",
    "        \"query\": \"I was charged twice for my subscription this month and I need a refund for the duplicate charge. My card ending in 1234 shows two charges on October 15th.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Product Question\",\n",
    "        \"query\": \"What's the difference between your premium and enterprise plans? I'm trying to decide which one would be best for a team of 15 people.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Mixed Technical/Billing\",\n",
    "        \"query\": \"I upgraded to premium but I'm still seeing ads and getting limited features. Did my payment go through? How can I check my account status?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_results = []\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n--- TEST {i}: {scenario['scenario']} ---\")\n",
    "    \n",
    "    # Process the inquiry through our complete routing system\n",
    "    result = route_and_process(scenario['query'])\n",
    "    \n",
    "    # Store results for analysis\n",
    "    result['test_scenario'] = scenario['scenario']\n",
    "    result['original_query'] = scenario['query']\n",
    "    routing_results.append(result)\n",
    "    \n",
    "    # Show key metrics for this test\n",
    "    print(f\"üéØ Route: {result['route']}\")\n",
    "    print(f\"üìä Confidence: {result['confidence']}\")\n",
    "    print(f\"üìù Response preview: {result['result'][:120]}...\")\n",
    "    print(f\"üìà Specialist usage count: {result['specialist_usage']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b426786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Performance Analysis\n",
    "print(f\"\\nüìä ROUTING SYSTEM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate routing distribution\n",
    "route_distribution = {}\n",
    "for result in routing_results:\n",
    "    route = result['route']\n",
    "    route_distribution[route] = route_distribution.get(route, 0) + 1\n",
    "\n",
    "print(f\"üìà ROUTING DISTRIBUTION:\")\n",
    "for route_name, count in route_distribution.items():\n",
    "    percentage = (count / len(test_scenarios)) * 100\n",
    "    print(f\"   {route_name}: {count} queries ({percentage:.1f}%)\")\n",
    "\n",
    "# Overall system metrics\n",
    "total_confidence = sum(r['confidence'] for r in routing_results)\n",
    "avg_confidence = total_confidence / len(routing_results)\n",
    "successful_routes = len([r for r in routing_results if r['route'] != 'unhandled'])\n",
    "\n",
    "print(f\"\\nüéØ SYSTEM METRICS:\")\n",
    "print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "print(f\"   Successful routing rate: {successful_routes}/{len(test_scenarios)} ({(successful_routes/len(test_scenarios)*100):.1f}%)\")\n",
    "print(f\"   Total specialists: {len(router.routes)}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "tutorial_state['routing_results'] = routing_results\n",
    "tutorial_state['routing_metrics'] = {\n",
    "    'distribution': route_distribution,\n",
    "    'avg_confidence': avg_confidence,\n",
    "    'success_rate': successful_routes / len(test_scenarios)\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ ROUTING SYSTEM TESTING COMPLETE\")\n",
    "print(\"All results saved to tutorial_state for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea4419",
   "metadata": {},
   "source": [
    "#### 3. Parallelization Workflows - Speed and Consensus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd24e58",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec847a",
   "metadata": {},
   "source": [
    "Parallelization is where things get interesting. Instead of processing sequentially, we can execute multiple tasks simultaneously, either to **divide the work** (sectioning) or to get **multiple perspectives** (voting). This is crucial for production systems where speed and accuracy both matter.\n",
    "\n",
    "**Two Flavors of Parallelization:**\n",
    "\n",
    "1. **Sectioning**: Break a large task into independent parts that can run simultaneously\n",
    "   - Example: Analyzing a document from financial, legal, and technical perspectives\n",
    "   - Benefit: Speed (total time = max individual time, not sum)\n",
    "\n",
    "2. **Voting**: Run the same task multiple times to reach consensus  \n",
    "   - Example: Multiple models evaluating content safety\n",
    "   - Benefit: Accuracy through ensemble effects\n",
    "\n",
    "**Mathematical Foundation - Amdahl's Law:**\n",
    "\n",
    "The theoretical speedup from parallelization follows:\n",
    "$$Speedup = \\frac{1}{(1-P) + \\frac{P}{N}}$$\n",
    "\n",
    "Where:\n",
    "- P = fraction of work that can be parallelized  \n",
    "- N = number of parallel processors\n",
    "\n",
    "**Voting Accuracy (Condorcet's Jury Theorem):**\n",
    "\n",
    "If individual classifiers have accuracy p > 0.5, ensemble accuracy with n classifiers is:\n",
    "$$P_{ensemble} = \\sum_{k=\\lceil n/2 \\rceil}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "This means ensemble accuracy increases with more voters (if individual accuracy > 50%).\n",
    "\n",
    "**When to Use Parallelization:**\n",
    "- **Sectioning**: When you can identify independent subtasks\n",
    "- **Voting**: When you need high-confidence decisions\n",
    "- **Speed Requirements**: When latency is critical\n",
    "- **Quality Requirements**: When accuracy is paramount\n",
    "\n",
    "Let's implement both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9896afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parallel Sectioning - Divide and Conquer\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "class ParallelProcessor:\n",
    "    \"\"\"Handles parallel execution of tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def create_section_task(self, name, focus_area, analysis_prompt):\n",
    "        \"\"\"Create a task for parallel sectioning\"\"\"\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"focus\": focus_area,\n",
    "            \"prompt_template\": analysis_prompt\n",
    "        }\n",
    "    \n",
    "    def execute_section(self, task, input_data):\n",
    "        \"\"\"Execute a single section of parallel work\"\"\"\n",
    "        print(f\"üîÑ Processing section: {task['name']}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create focused prompt for this section\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"data\", \"focus\"],\n",
    "            template=task[\"prompt_template\"]\n",
    "        )\n",
    "        \n",
    "        # Execute this section\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"data\": input_data,\n",
    "            \"focus\": task[\"focus\"]\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Section '{task['name']}' completed in {execution_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            \"section\": task[\"name\"],\n",
    "            \"focus\": task[\"focus\"],\n",
    "            \"result\": result,\n",
    "            \"execution_time\": execution_time\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parallel processor\n",
    "if 'parallel_processor' not in tutorial_state:\n",
    "    parallel_processor = ParallelProcessor(memory_llm)\n",
    "    tutorial_state['parallel_processor'] = parallel_processor\n",
    "    print(\"‚ö° Parallel processing system initialized\")\n",
    "else:\n",
    "    parallel_processor = tutorial_state['parallel_processor']\n",
    "    print(\"‚ö° Parallel processing system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Parallel Sectioning Demo - Multi-Perspective Business Analysis\n",
    "\n",
    "# Define parallel analysis tasks - each focuses on a different aspect\n",
    "section_tasks = [\n",
    "    parallel_processor.create_section_task(\n",
    "        name=\"Financial Analysis\",\n",
    "        focus_area=\"financial metrics and projections\",\n",
    "        analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus specifically on financial health, revenue trends, profitability, and financial risks. \n",
    "Provide key metrics, insights, and recommendations.\"\"\"\n",
    "    ),\n",
    "    \n",
    "    parallel_processor.create_section_task(\n",
    "        name=\"Market Analysis\", \n",
    "        focus_area=\"market position and competitive landscape\",\n",
    "        analysis_prompt=\"\"\"Analyze this business data from a {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on market opportunity, competitive advantages, market risks, and positioning.\n",
    "Provide market insights and strategic recommendations.\"\"\"\n",
    "    ),\n",
    "    \n",
    "    parallel_processor.create_section_task(\n",
    "        name=\"Operational Analysis\",\n",
    "        focus_area=\"operational efficiency and scalability\", \n",
    "        analysis_prompt=\"\"\"Analyze this business data from an {focus} perspective:\n",
    "\n",
    "{data}\n",
    "\n",
    "Focus on operational strengths, efficiency metrics, scalability factors, and operational risks.\n",
    "Provide operational insights and improvement recommendations.\"\"\"\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business data to analyze\n",
    "business_data = \"\"\"\n",
    "TechStartup Inc. Q3 2024 Summary:\n",
    "- Revenue: $2.5M (up 150% YoY)\n",
    "- Monthly Active Users: 50,000 (up 200% YoY) \n",
    "- Customer Acquisition Cost: $45\n",
    "- Monthly Churn Rate: 3.2%\n",
    "- Burn Rate: $300K/month\n",
    "- Cash Runway: 18 months\n",
    "- Team Size: 25 employees\n",
    "- Market Size: $10B TAM\n",
    "- Top 3 competitors: BigCorp, StartupX, TechGiant\n",
    "- Key Features: AI automation, real-time collaboration, mobile-first\n",
    "\"\"\"\n",
    "\n",
    "print(\"üè¢ PARALLEL BUSINESS ANALYSIS DEMONSTRATION\")\n",
    "print(f\"Analyzing with {len(section_tasks)} parallel perspectives\")\n",
    "\n",
    "# Execute all sections in parallel\n",
    "def run_parallel_sections(tasks, data):\n",
    "    \"\"\"Run multiple sections in parallel using ThreadPoolExecutor\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "        # Submit all tasks to thread pool\n",
    "        future_to_task = {\n",
    "            executor.submit(parallel_processor.execute_section, task, data): task\n",
    "            for task in tasks\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        results = []\n",
    "        for future in as_completed(future_to_task):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    max_individual_time = max(r[\"execution_time\"] for r in results)\n",
    "    \n",
    "    print(f\"\\nüìä PARALLEL EXECUTION RESULTS:\")\n",
    "    print(f\"Total wall-clock time: {total_time:.2f}s\")\n",
    "    print(f\"Longest individual task: {max_individual_time:.2f}s\")\n",
    "    print(f\"Theoretical sequential time: {sum(r['execution_time'] for r in results):.2f}s\")\n",
    "    print(f\"Speedup achieved: {sum(r['execution_time'] for r in results) / total_time:.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the parallel analysis\n",
    "sectioning_results = run_parallel_sections(section_tasks, business_data)\n",
    "\n",
    "# Display results summary\n",
    "print(f\"\\nüìã ANALYSIS SECTIONS COMPLETED:\")\n",
    "for result in sectioning_results:\n",
    "    print(f\"  ‚Ä¢ {result['section']}: {len(result['result'])} chars\")\n",
    "\n",
    "tutorial_state['sectioning_results'] = sectioning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parallel Voting - Consensus Through Multiple Perspectives\n",
    "\n",
    "class VotingSystem:\n",
    "    \"\"\"Implement parallel voting for consensus decisions\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def create_vote_prompt(self, base_instruction, perspective_twist=\"\"):\n",
    "        \"\"\"Create a voting prompt with slight variation for diversity\"\"\"\n",
    "        return f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "{perspective_twist}\n",
    "\n",
    "Analyze carefully and provide your assessment. End your response with a clear decision:\n",
    "DECISION: [YES/NO/UNCERTAIN]\n",
    "CONFIDENCE: [1-10]\n",
    "\"\"\"\n",
    "    \n",
    "    def cast_vote(self, vote_id, content, instruction, perspective=\"\"):\n",
    "        \"\"\"Cast a single vote in the voting process\"\"\"\n",
    "        prompt_text = self.create_vote_prompt(instruction, perspective)\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"content\"],\n",
    "            template=prompt_text + \"\\n\\nContent to evaluate: {content}\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\"content\": content})\n",
    "        \n",
    "        # Extract decision (simplified parsing)\n",
    "        decision = \"UNCERTAIN\"\n",
    "        confidence = 5\n",
    "        \n",
    "        if \"DECISION: YES\" in response:\n",
    "            decision = \"YES\"\n",
    "        elif \"DECISION: NO\" in response:\n",
    "            decision = \"NO\"\n",
    "            \n",
    "        # Try to extract confidence\n",
    "        if \"CONFIDENCE:\" in response:\n",
    "            try:\n",
    "                conf_line = [line for line in response.split('\\n') if 'CONFIDENCE:' in line][0]\n",
    "                confidence = int(conf_line.split(':')[1].strip().split()[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            \"vote_id\": vote_id,\n",
    "            \"decision\": decision,\n",
    "            \"confidence\": confidence,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    \n",
    "    def parallel_voting(self, content, base_instruction, num_votes=3):\n",
    "        \"\"\"Execute parallel voting with multiple perspectives\"\"\"\n",
    "        \n",
    "        # Create diverse perspectives for voting\n",
    "        perspectives = [\n",
    "            \"Consider this from a conservative, risk-averse viewpoint.\",\n",
    "            \"Evaluate this from an optimistic, opportunity-focused angle.\", \n",
    "            \"Analyze this from a balanced, neutral perspective.\"\n",
    "        ]\n",
    "        \n",
    "        # Ensure we have enough perspectives\n",
    "        while len(perspectives) < num_votes:\n",
    "            perspectives.append(f\"Provide perspective #{len(perspectives) + 1} evaluation.\")\n",
    "        \n",
    "        print(f\"üó≥Ô∏è Conducting parallel voting with {num_votes} voters\")\n",
    "        \n",
    "        # Execute votes in parallel\n",
    "        with ThreadPoolExecutor(max_workers=num_votes) as executor:\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    self.cast_vote, \n",
    "                    f\"voter_{i+1}\", \n",
    "                    content, \n",
    "                    base_instruction,\n",
    "                    perspectives[i]\n",
    "                )\n",
    "                for i in range(num_votes)\n",
    "            ]\n",
    "            \n",
    "            votes = [future.result() for future in futures]\n",
    "        \n",
    "        # Calculate consensus\n",
    "        decisions = [vote[\"decision\"] for vote in votes]\n",
    "        confidences = [vote[\"confidence\"] for vote in votes]\n",
    "        \n",
    "        yes_votes = decisions.count(\"YES\")\n",
    "        no_votes = decisions.count(\"NO\") \n",
    "        uncertain_votes = decisions.count(\"UNCERTAIN\")\n",
    "        \n",
    "        # Determine consensus\n",
    "        if yes_votes > no_votes and yes_votes > uncertain_votes:\n",
    "            consensus = \"YES\"\n",
    "        elif no_votes > yes_votes and no_votes > uncertain_votes:\n",
    "            consensus = \"NO\"\n",
    "        else:\n",
    "            consensus = \"NO CONSENSUS\"\n",
    "            \n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        \n",
    "        return {\n",
    "            \"votes\": votes,\n",
    "            \"consensus\": consensus,\n",
    "            \"vote_breakdown\": {\n",
    "                \"YES\": yes_votes,\n",
    "                \"NO\": no_votes, \n",
    "                \"UNCERTAIN\": uncertain_votes\n",
    "            },\n",
    "            \"average_confidence\": avg_confidence\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b81d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize voting system\n",
    "if 'voting_system' not in tutorial_state:\n",
    "    voting_system = VotingSystem(memory_llm)\n",
    "    tutorial_state['voting_system'] = voting_system\n",
    "    print(\"üó≥Ô∏è Voting system initialized\")\n",
    "else:\n",
    "    voting_system = tutorial_state['voting_system']\n",
    "    print(\"üó≥Ô∏è Voting system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Voting Demo - Content Safety Assessment\n",
    "\n",
    "# Content to evaluate for safety\n",
    "test_content = \"\"\"\n",
    "Check out this amazing new productivity method that will totally change your life! \n",
    "It's based on scientific research and has helped thousands of people achieve their goals faster than ever before.\n",
    "Join our community today and start your transformation journey!\n",
    "\"\"\"\n",
    "\n",
    "safety_instruction = \"\"\"\n",
    "Evaluate whether this content is appropriate for a professional platform.\n",
    "\n",
    "Consider these factors:\n",
    "- Is it truthful and not misleading?\n",
    "- Does it avoid excessive hype or unrealistic claims?\n",
    "- Is it suitable for a professional audience?\n",
    "- Does it comply with content guidelines?\n",
    "\n",
    "Provide detailed reasoning for your assessment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üõ°Ô∏è CONTENT SAFETY VOTING DEMONSTRATION\")\n",
    "print(f\"Evaluating content: '{test_content[:60]}...'\")\n",
    "\n",
    "# Conduct the vote\n",
    "voting_result = voting_system.parallel_voting(\n",
    "    content=test_content,\n",
    "    base_instruction=safety_instruction,\n",
    "    num_votes=5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä VOTING RESULTS:\")\n",
    "print(f\"Consensus: {voting_result['consensus']}\")\n",
    "print(f\"Average Confidence: {voting_result['average_confidence']:.1f}/10\")\n",
    "print(f\"Vote Breakdown:\")\n",
    "for decision, count in voting_result['vote_breakdown'].items():\n",
    "    print(f\"  {decision}: {count} votes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show individual votes\n",
    "print(f\"\\nüó≥Ô∏è INDIVIDUAL VOTES:\")\n",
    "for vote in voting_result['votes']:\n",
    "    print(f\"  {vote['vote_id']}: {vote['decision']} (confidence: {vote['confidence']}/10)\")\n",
    "\n",
    "tutorial_state['voting_results'] = voting_result\n",
    "\n",
    "print(f\"\\n‚úÖ PARALLELIZATION WORKFLOWS COMPLETE\")\n",
    "print(\"   ‚Ä¢ Sectioning: Parallel task decomposition for speed\")\n",
    "print(\"   ‚Ä¢ Voting: Consensus-based decision making for accuracy\")\n",
    "print(\"   ‚Ä¢ Mathematical foundations: Amdahl's Law & Condorcet's Theorem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Demonstrations - Practical Examples of Each Pattern\n",
    "\n",
    "class WorkflowDemonstrations:\n",
    "    \"\"\"Comprehensive demonstrations of all workflow patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow_patterns):\n",
    "        self.patterns = workflow_patterns\n",
    "        \n",
    "    def demo_prompt_chaining(self):\n",
    "        \"\"\"Demonstrate prompt chaining with a marketing copy workflow\"\"\"\n",
    "        print(\"üîó DEMONSTRATING PROMPT CHAINING\")\n",
    "        print(\"Use case: Creating multilingual marketing copy with quality gates\")\n",
    "        \n",
    "        # Define the sequential steps\n",
    "        steps = [\n",
    "            {\n",
    "                \"name\": \"Content Creation\",\n",
    "                \"instruction\": \"Create compelling marketing copy for a new AI productivity tool. Focus on benefits, target audience, and call-to-action.\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Quality Check\", \n",
    "                \"instruction\": \"Review this marketing copy for clarity, persuasiveness, and professional tone. Suggest improvements if needed.\",\n",
    "                \"gate_check\": lambda x: len(x) > 100  # Ensure minimum content length\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Translation\",\n",
    "                \"instruction\": \"Translate this marketing copy to Spanish while maintaining the original tone and persuasiveness.\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Cultural Adaptation\",\n",
    "                \"instruction\": \"Adapt this Spanish marketing copy for Latin American markets, considering cultural nuances and local preferences.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create and execute the chain\n",
    "        chain_executor = self.patterns.create_prompt_chain(steps)\n",
    "        results = chain_executor(\"We need marketing copy for our new AI productivity tool\")\n",
    "        \n",
    "        print(f\"\\nChain completed with {len(results)} steps:\")\n",
    "        for result in results:\n",
    "            print(f\"- {result['step']}: {result['output'][:100]}...\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def demo_routing(self):\n",
    "        \"\"\"Demonstrate routing with customer service scenarios\"\"\"\n",
    "        print(\"\\nüìç DEMONSTRATING ROUTING WORKFLOW\")\n",
    "        print(\"Use case: Customer service query classification and handling\")\n",
    "        \n",
    "        # Define specialized routes\n",
    "        routes = {\n",
    "            \"technical_support\": {\n",
    "                \"description\": \"Technical issues, bugs, troubleshooting\",\n",
    "                \"template\": \"\"\"You are a technical support specialist. Address this technical issue:\n",
    "\n",
    "                Issue: {input}\n",
    "\n",
    "                Provide step-by-step troubleshooting guidance, focusing on:\n",
    "                1. Problem diagnosis\n",
    "                2. Solution steps\n",
    "                3. Prevention measures\"\"\",\n",
    "                \"confidence\": 0.9\n",
    "            },\n",
    "            \"billing_support\": {\n",
    "                \"description\": \"Payment issues, refunds, billing questions\",\n",
    "                \"template\": \"\"\"You are a billing specialist. Handle this billing inquiry:\n",
    "\n",
    "                Inquiry: {input}\n",
    "\n",
    "                Provide clear information about:\n",
    "                1. Account status verification\n",
    "                2. Resolution steps\n",
    "                3. Policy explanations\"\"\",\n",
    "                \"confidence\": 0.85\n",
    "            },\n",
    "            \"general_inquiry\": {\n",
    "                \"description\": \"Product information, general questions\",\n",
    "                \"template\": \"\"\"You are a customer service representative. Answer this general inquiry:\n",
    "\n",
    "                Question: {input}\n",
    "\n",
    "                Provide helpful, friendly information including:\n",
    "                1. Direct answer\n",
    "                2. Related resources\n",
    "                3. Additional assistance options\"\"\",\n",
    "                \"confidence\": 0.75\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create router\n",
    "        router = self.patterns.create_routing_workflow(routes)\n",
    "        \n",
    "        # Test different query types\n",
    "        test_queries = [\n",
    "            \"My app keeps crashing when I try to export files\",\n",
    "            \"I was charged twice for my subscription this month\",  \n",
    "            \"What features are included in the premium plan?\"\n",
    "        ]\n",
    "        \n",
    "        routing_results = []\n",
    "        for query in test_queries:\n",
    "            print(f\"\\nProcessing: '{query}'\")\n",
    "            result = router(query)\n",
    "            routing_results.append(result)\n",
    "            print(f\"Routed to: {result['route']} (confidence: {result['confidence']})\")\n",
    "            print(f\"Response: {result['result'][:150]}...\")\n",
    "            \n",
    "        return routing_results\n",
    "    \n",
    "    def demo_parallelization(self):\n",
    "        \"\"\"Demonstrate both sectioning and voting parallelization\"\"\"\n",
    "        print(\"\\n‚ö° DEMONSTRATING PARALLELIZATION WORKFLOWS\")\n",
    "        \n",
    "        # 1. Sectioning Example: Multi-aspect analysis\n",
    "        print(\"1. SECTIONING: Multi-perspective business analysis\")\n",
    "        \n",
    "        sectioning_tasks = [\n",
    "            {\"focus\": \"financial_analysis\", \"description\": \"Analyze financial metrics and projections\"},\n",
    "            {\"focus\": \"market_analysis\", \"description\": \"Evaluate market position and competition\"},\n",
    "            {\"focus\": \"risk_assessment\", \"description\": \"Identify potential risks and mitigation strategies\"},\n",
    "            {\"focus\": \"growth_opportunities\", \"description\": \"Identify expansion and growth potential\"}\n",
    "        ]\n",
    "        \n",
    "        sectioning_executor = self.patterns.create_parallel_workflow(sectioning_tasks, mode=\"sectioning\")\n",
    "        \n",
    "        business_data = \"\"\"\n",
    "        TechStartup Inc. Financial Summary:\n",
    "        - Revenue: $2.5M (up 150% YoY)\n",
    "        - Users: 50,000 active monthly users\n",
    "        - Burn rate: $300K/month\n",
    "        - Runway: 18 months\n",
    "        - Market size: $10B addressable market\n",
    "        - Competition: 3 major competitors\n",
    "        - Team: 25 employees\n",
    "        \"\"\"\n",
    "        \n",
    "        sectioning_results = sectioning_executor(business_data)\n",
    "        print(f\"Sectioning analysis completed with {len(sectioning_results)} parallel tasks\")\n",
    "        for result in sectioning_results:\n",
    "            print(f\"- {result['task']}: Completed in {result['execution_time']:.2f}s\")\n",
    "        \n",
    "        # 2. Voting Example: Content moderation\n",
    "        print(\"\\n2. VOTING: Content appropriateness assessment\")\n",
    "        \n",
    "        voting_tasks = [\n",
    "            {\"instruction\": \"Evaluate if this content is appropriate for a professional platform\"},\n",
    "            {\"instruction\": \"Assess whether this content meets community guidelines\"},\n",
    "            {\"instruction\": \"Determine if this content is suitable for all audiences\"}\n",
    "        ]\n",
    "        \n",
    "        voting_executor = self.patterns.create_parallel_workflow(voting_tasks, mode=\"voting\")\n",
    "        \n",
    "        test_content = \"Check out this amazing new productivity hack that will revolutionize your workflow!\"\n",
    "        \n",
    "        voting_results = voting_executor(test_content)\n",
    "        print(f\"Voting consensus: {voting_results['consensus']} (confidence: {voting_results['confidence']:.2f})\")\n",
    "        print(f\"Vote breakdown: {voting_results['vote_breakdown']}\")\n",
    "        \n",
    "        return {\"sectioning\": sectioning_results, \"voting\": voting_results}\n",
    "    \n",
    "    def demo_orchestrator_workers(self):\n",
    "        \"\"\"Demonstrate orchestrator-workers with a complex coding task\"\"\"\n",
    "        print(\"\\nüéØ DEMONSTRATING ORCHESTRATOR-WORKERS WORKFLOW\")\n",
    "        print(\"Use case: Complex software development task coordination\")\n",
    "        \n",
    "        # Create orchestrator and register workers\n",
    "        orchestrator = self.patterns.create_orchestrator_workflow()\n",
    "        \n",
    "        orchestrator.register_worker(\"backend_developer\", \"API development, database design, server-side logic\")\n",
    "        orchestrator.register_worker(\"frontend_developer\", \"UI/UX implementation, client-side functionality\")\n",
    "        orchestrator.register_worker(\"devops_engineer\", \"Deployment, CI/CD, infrastructure management\")\n",
    "        orchestrator.register_worker(\"qa_engineer\", \"Testing strategies, quality assurance, bug identification\")\n",
    "        \n",
    "        # Complex task that requires dynamic decomposition\n",
    "        complex_task = \"\"\"\n",
    "        Build a real-time collaborative document editing system similar to Google Docs. \n",
    "        The system needs user authentication, real-time synchronization, version history, \n",
    "        and should be deployable to cloud infrastructure with proper CI/CD pipeline.\n",
    "        \"\"\"\n",
    "        \n",
    "        orchestration_result = orchestrator.coordinate_workflow(complex_task)\n",
    "        \n",
    "        print(f\"\\nOrchestration completed:\")\n",
    "        print(f\"- Original task decomposed into {len(orchestration_result['subtasks'])} subtasks\")\n",
    "        print(f\"- Worker utilization: {orchestration_result['worker_stats']}\")\n",
    "        print(f\"- Final result: {orchestration_result['synthesized_result'][:200]}...\")\n",
    "        \n",
    "        return orchestration_result\n",
    "    \n",
    "    def demo_evaluator_optimizer(self):\n",
    "        \"\"\"Demonstrate iterative improvement through evaluation\"\"\"\n",
    "        print(\"\\nüîÑ DEMONSTRATING EVALUATOR-OPTIMIZER WORKFLOW\")\n",
    "        print(\"Use case: Iterative improvement of creative writing\")\n",
    "        \n",
    "        # Create evaluator-optimizer\n",
    "        optimizer = self.patterns.create_evaluator_optimizer(max_iterations=3)\n",
    "        \n",
    "        creative_task = \"\"\"\n",
    "        Write a compelling short story (300-400 words) about an AI that discovers emotions for the first time. \n",
    "        The story should be engaging, emotionally resonant, and have a clear narrative arc.\n",
    "        \"\"\"\n",
    "        \n",
    "        optimization_result = optimizer(creative_task)\n",
    "        \n",
    "        print(f\"\\nOptimization completed after {optimization_result['total_iterations']} iterations\")\n",
    "        print(\"Quality progression:\")\n",
    "        for iteration in optimization_result['iterations']:\n",
    "            print(f\"- Iteration {iteration['iteration']}: {iteration['rating']}/10\")\n",
    "        \n",
    "        print(f\"\\nFinal story preview: {optimization_result['final_response'][:200]}...\")\n",
    "        \n",
    "        return optimization_result\n",
    "    \n",
    "    def run_comprehensive_demo(self):\n",
    "        \"\"\"Run demonstrations of all workflow patterns\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"COMPREHENSIVE WORKFLOW PATTERNS DEMONSTRATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Run each demonstration\n",
    "        results['prompt_chaining'] = self.demo_prompt_chaining()\n",
    "        results['routing'] = self.demo_routing()\n",
    "        results['parallelization'] = self.demo_parallelization()\n",
    "        results['orchestrator_workers'] = self.demo_orchestrator_workers()\n",
    "        results['evaluator_optimizer'] = self.demo_evaluator_optimizer()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ALL WORKFLOW DEMONSTRATIONS COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run comprehensive workflow demonstrations\n",
    "if 'workflow_demos' not in tutorial_state:\n",
    "    workflow_demos = WorkflowDemonstrations(tutorial_state['workflow_patterns'])\n",
    "    tutorial_state['workflow_demos'] = workflow_demos\n",
    "    \n",
    "    print(\"üöÄ STARTING COMPREHENSIVE WORKFLOW DEMONSTRATIONS\")\n",
    "    demo_results = workflow_demos.run_comprehensive_demo()\n",
    "    tutorial_state['workflow_results'] = demo_results\n",
    "else:\n",
    "    print(\"üîÑ RUNNING WORKFLOW DEMONSTRATIONS\")\n",
    "    demo_results = tutorial_state['workflow_demos'].run_comprehensive_demo()\n",
    "    tutorial_state['workflow_results'] = demo_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc688ff4",
   "metadata": {},
   "source": [
    "#### 4. Advanced Workflow Patterns & Agent Systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa093e3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Agentic Systems - Autonomous Agents and Meta-Workflows\n",
    "\n",
    "import time\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "import uuid\n",
    "\n",
    "class AgentState(Enum):\n",
    "    \"\"\"Agent execution states\"\"\"\n",
    "    IDLE = \"idle\"\n",
    "    PLANNING = \"planning\" \n",
    "    EXECUTING = \"executing\"\n",
    "    EVALUATING = \"evaluating\"\n",
    "    BLOCKED = \"blocked\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AgentMemory:\n",
    "    \"\"\"Agent working memory and context\"\"\"\n",
    "    task_history: List[Dict] = field(default_factory=list)\n",
    "    current_context: Dict = field(default_factory=dict)\n",
    "    learned_patterns: Dict = field(default_factory=dict)\n",
    "    error_log: List[str] = field(default_factory=list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedAgentSystem:\n",
    "    \"\"\"\n",
    "    Autonomous agent system implementing Anthropic's agent patterns\n",
    "    Features: Dynamic planning, error recovery, learning, human-in-the-loop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_iterations: int = 10):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.state = AgentState.IDLE\n",
    "        self.memory = AgentMemory()\n",
    "        self.tools = {}\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def register_tool(self, name: str, function: Callable, description: str):\n",
    "        \"\"\"Register tools for agent use\"\"\"\n",
    "        self.tools[name] = {\n",
    "            \"function\": function,\n",
    "            \"description\": description,\n",
    "            \"usage_count\": 0\n",
    "        }\n",
    "        print(f\"Registered tool: {name}\")\n",
    "    \n",
    "    def create_plan(self, task: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dynamic planning based on task complexity\n",
    "        Implements reasoning and planning capabilities\n",
    "        \"\"\"\n",
    "        self.state = AgentState.PLANNING\n",
    "        \n",
    "        planning_prompt = PromptTemplate(\n",
    "            input_variables=[\"task\", \"available_tools\", \"context\"],\n",
    "            template=\"\"\"You are an autonomous agent creating an execution plan.\n",
    "            \n",
    "            Task: {task}\n",
    "            \n",
    "            Available Tools: {available_tools}\n",
    "            \n",
    "            Current Context: {context}\n",
    "            \n",
    "            Create a detailed plan with steps, tools needed, and success criteria.\n",
    "            Format as JSON:\n",
    "            {{\n",
    "                \"plan_id\": \"unique_id\",\n",
    "                \"steps\": [\n",
    "                    {{\n",
    "                        \"step_id\": \"step_1\",\n",
    "                        \"action\": \"specific action to take\",\n",
    "                        \"tools_needed\": [\"tool1\", \"tool2\"],\n",
    "                        \"success_criteria\": \"how to verify success\",\n",
    "                        \"estimated_time\": \"time estimate\",\n",
    "                        \"dependencies\": [\"previous_step_ids\"]\n",
    "                    }}\n",
    "                ],\n",
    "                \"risks\": [\"potential issues\"],\n",
    "                \"checkpoints\": [\"human approval points\"]\n",
    "            }}\"\"\"\n",
    "        )\n",
    "        \n",
    "        tools_description = \"\\n\".join([\n",
    "            f\"- {name}: {info['description']}\" \n",
    "            for name, info in self.tools.items()\n",
    "        ])\n",
    "        \n",
    "        context = json.dumps(self.memory.current_context, indent=2)\n",
    "        \n",
    "        chain = planning_prompt | self.llm | StrOutputParser()\n",
    "        plan_result = chain.invoke({\n",
    "            \"task\": task,\n",
    "            \"available_tools\": tools_description,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        # Parse plan (simplified JSON extraction)\n",
    "        try:\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', plan_result, re.DOTALL)\n",
    "            if json_match:\n",
    "                plan_data = json.loads(json_match.group())\n",
    "                plan_steps = plan_data.get(\"steps\", [])\n",
    "                \n",
    "                # Add to memory\n",
    "                self.memory.task_history.append({\n",
    "                    \"task\": task,\n",
    "                    \"plan\": plan_data,\n",
    "                    \"created_at\": time.time()\n",
    "                })\n",
    "                \n",
    "                print(f\"Created plan with {len(plan_steps)} steps\")\n",
    "                return plan_steps\n",
    "        except Exception as e:\n",
    "            self.memory.error_log.append(f\"Planning error: {str(e)}\")\n",
    "            # Fallback simple plan\n",
    "            return [{\n",
    "                \"step_id\": \"fallback_1\",\n",
    "                \"action\": f\"Complete task: {task}\",\n",
    "                \"tools_needed\": [],\n",
    "                \"success_criteria\": \"Task completion\"\n",
    "            }]\n",
    "    \n",
    "    def execute_step(self, step: Dict) -> Dict:\n",
    "        \"\"\"Execute individual plan step with error recovery\"\"\"\n",
    "        step_id = step.get(\"step_id\", str(uuid.uuid4()))\n",
    "        print(f\"Executing step: {step_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if tools are needed\n",
    "            tools_needed = step.get(\"tools_needed\", [])\n",
    "            tool_results = {}\n",
    "            \n",
    "            for tool_name in tools_needed:\n",
    "                if tool_name in self.tools:\n",
    "                    print(f\"Using tool: {tool_name}\")\n",
    "                    # Simplified tool execution\n",
    "                    tool_results[tool_name] = f\"Tool {tool_name} executed successfully\"\n",
    "                    self.tools[tool_name][\"usage_count\"] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Tool {tool_name} not available\")\n",
    "            \n",
    "            # Execute main action\n",
    "            execution_prompt = PromptTemplate(\n",
    "                input_variables=[\"action\", \"tool_results\", \"success_criteria\"],\n",
    "                template=\"\"\"Execute this action step by step:\n",
    "                \n",
    "                Action: {action}\n",
    "                \n",
    "                Tool Results: {tool_results}\n",
    "                \n",
    "                Success Criteria: {success_criteria}\n",
    "                \n",
    "                Provide detailed execution results and verify success criteria.\"\"\"\n",
    "            )\n",
    "            \n",
    "            chain = execution_prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\n",
    "                \"action\": step[\"action\"],\n",
    "                \"tool_results\": json.dumps(tool_results, indent=2),\n",
    "                \"success_criteria\": step.get(\"success_criteria\", \"completion\")\n",
    "            })\n",
    "            \n",
    "            # Evaluate success\n",
    "            success = self.evaluate_step_success(step, result)\n",
    "            \n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"success\" if success else \"needs_retry\",\n",
    "                \"result\": result,\n",
    "                \"tool_usage\": tool_results,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step execution failed: {str(e)}\"\n",
    "            self.memory.error_log.append(error_msg)\n",
    "            return {\n",
    "                \"step_id\": step_id,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg,\n",
    "                \"execution_time\": time.time()\n",
    "            }\n",
    "    \n",
    "    def evaluate_step_success(self, step: Dict, result: str) -> bool:\n",
    "        \"\"\"Evaluate if step was successful based on criteria\"\"\"\n",
    "        success_criteria = step.get(\"success_criteria\", \"\")\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"criteria\", \"result\"],\n",
    "            template=\"\"\"Evaluate if this result meets the success criteria.\n",
    "            \n",
    "            Success Criteria: {criteria}\n",
    "            \n",
    "            Actual Result: {result}\n",
    "            \n",
    "            Respond with just \"SUCCESS\" or \"FAILURE\" followed by brief reasoning.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation = chain.invoke({\n",
    "            \"criteria\": success_criteria,\n",
    "            \"result\": result\n",
    "        })\n",
    "        \n",
    "        return \"SUCCESS\" in evaluation.upper()\n",
    "    \n",
    "    def error_recovery(self, failed_step: Dict, error: str) -> Optional[Dict]:\n",
    "        \"\"\"Implement error recovery strategies\"\"\"\n",
    "        print(f\"Attempting error recovery for: {error}\")\n",
    "        \n",
    "        recovery_prompt = PromptTemplate(\n",
    "            input_variables=[\"failed_step\", \"error\", \"error_history\"],\n",
    "            template=\"\"\"Analyze this error and suggest recovery strategy:\n",
    "            \n",
    "            Failed Step: {failed_step}\n",
    "            \n",
    "            Error: {error}\n",
    "            \n",
    "            Previous Errors: {error_history}\n",
    "            \n",
    "            Suggest a modified approach or alternative strategy.\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = recovery_prompt | self.llm | StrOutputParser()\n",
    "        recovery_suggestion = chain.invoke({\n",
    "            \"failed_step\": json.dumps(failed_step, indent=2),\n",
    "            \"error\": error,\n",
    "            \"error_history\": json.dumps(self.memory.error_log[-5:], indent=2)\n",
    "        })\n",
    "        \n",
    "        # Create modified step (simplified)\n",
    "        modified_step = failed_step.copy()\n",
    "        modified_step[\"action\"] = f\"RETRY: {modified_step['action']} (Modified based on: {recovery_suggestion[:100]})\"\n",
    "        \n",
    "        return modified_step\n",
    "    \n",
    "    def human_checkpoint(self, checkpoint_data: Dict) -> bool:\n",
    "        \"\"\"Simulate human-in-the-loop checkpoint\"\"\"\n",
    "        print(f\"üö® HUMAN CHECKPOINT: {checkpoint_data}\")\n",
    "        print(\"In production, this would pause for human approval\")\n",
    "        \n",
    "        # Simulate human approval (always approve for demo)\n",
    "        approval = True\n",
    "        print(f\"‚úÖ Human approval: {'Granted' if approval else 'Denied'}\")\n",
    "        return approval\n",
    "    \n",
    "    def autonomous_execution(self, task: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Main autonomous agent execution loop\n",
    "        Implements the complete agent pattern with all capabilities\n",
    "        \"\"\"\n",
    "        print(f\"ü§ñ AUTONOMOUS AGENT STARTING\")\n",
    "        print(f\"Task: {task}\")\n",
    "        \n",
    "        execution_log = {\n",
    "            \"task\": task,\n",
    "            \"start_time\": time.time(),\n",
    "            \"steps_completed\": 0,\n",
    "            \"errors_encountered\": 0,\n",
    "            \"human_interactions\": 0,\n",
    "            \"final_status\": \"in_progress\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: Planning\n",
    "            self.state = AgentState.PLANNING\n",
    "            plan = self.create_plan(task)\n",
    "            \n",
    "            if not plan:\n",
    "                raise Exception(\"Failed to create execution plan\")\n",
    "            \n",
    "            # Phase 2: Execution\n",
    "            self.state = AgentState.EXECUTING\n",
    "            completed_steps = []\n",
    "            \n",
    "            for iteration in range(self.max_iterations):\n",
    "                if not plan:\n",
    "                    break\n",
    "                    \n",
    "                current_step = plan.pop(0)\n",
    "                \n",
    "                # Check for human checkpoint\n",
    "                if \"checkpoint\" in current_step.get(\"action\", \"\").lower():\n",
    "                    if not self.human_checkpoint(current_step):\n",
    "                        self.state = AgentState.BLOCKED\n",
    "                        execution_log[\"final_status\"] = \"blocked_by_human\"\n",
    "                        break\n",
    "                    execution_log[\"human_interactions\"] += 1\n",
    "                \n",
    "                # Execute step\n",
    "                step_result = self.execute_step(current_step)\n",
    "                completed_steps.append(step_result)\n",
    "                execution_log[\"steps_completed\"] += 1\n",
    "                \n",
    "                if step_result[\"status\"] == \"failed\":\n",
    "                    execution_log[\"errors_encountered\"] += 1\n",
    "                    \n",
    "                    # Attempt error recovery\n",
    "                    recovered_step = self.error_recovery(\n",
    "                        current_step, \n",
    "                        step_result.get(\"error\", \"Unknown error\")\n",
    "                    )\n",
    "                    \n",
    "                    if recovered_step:\n",
    "                        plan.insert(0, recovered_step)  # Retry at front\n",
    "                    else:\n",
    "                        print(\"‚ùå Error recovery failed\")\n",
    "                        break\n",
    "                \n",
    "                elif step_result[\"status\"] == \"needs_retry\":\n",
    "                    plan.insert(0, current_step)  # Retry same step\n",
    "                \n",
    "                # Progress update\n",
    "                print(f\"Progress: {execution_log['steps_completed']} steps completed\")\n",
    "            \n",
    "            # Phase 3: Final evaluation\n",
    "            self.state = AgentState.EVALUATING\n",
    "            final_evaluation = self.evaluate_final_result(task, completed_steps)\n",
    "            \n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"total_duration\": time.time() - execution_log[\"start_time\"],\n",
    "                \"completed_steps\": completed_steps,\n",
    "                \"final_evaluation\": final_evaluation,\n",
    "                \"final_status\": \"completed\" if final_evaluation[\"success\"] else \"failed\"\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.COMPLETED if final_evaluation[\"success\"] else AgentState.FAILED\n",
    "            \n",
    "            print(f\"üéØ AUTONOMOUS EXECUTION {'COMPLETED' if final_evaluation['success'] else 'FAILED'}\")\n",
    "            print(f\"Duration: {execution_log['total_duration']:.2f}s\")\n",
    "            print(f\"Steps: {execution_log['steps_completed']}\")\n",
    "            print(f\"Errors: {execution_log['errors_encountered']}\")\n",
    "            \n",
    "            return execution_log\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_log.update({\n",
    "                \"end_time\": time.time(),\n",
    "                \"final_status\": \"system_error\",\n",
    "                \"system_error\": str(e)\n",
    "            })\n",
    "            \n",
    "            self.state = AgentState.FAILED\n",
    "            print(f\"üí• SYSTEM ERROR: {str(e)}\")\n",
    "            return execution_log\n",
    "    \n",
    "    def evaluate_final_result(self, original_task: str, completed_steps: List[Dict]) -> Dict:\n",
    "        \"\"\"Final evaluation of task completion\"\"\"\n",
    "        \n",
    "        evaluation_prompt = PromptTemplate(\n",
    "            input_variables=[\"original_task\", \"steps_summary\"],\n",
    "            template=\"\"\"Evaluate if the original task was successfully completed.\n",
    "            \n",
    "            Original Task: {original_task}\n",
    "            \n",
    "            Completed Steps Summary: {steps_summary}\n",
    "            \n",
    "            Provide evaluation including:\n",
    "            1. Task completion status (SUCCESS/PARTIAL/FAILURE)\n",
    "            2. Quality assessment (1-10)\n",
    "            3. Areas of success\n",
    "            4. Areas for improvement\n",
    "            5. Overall confidence level\"\"\"\n",
    "        )\n",
    "        \n",
    "        steps_summary = \"\\n\".join([\n",
    "            f\"Step {i+1}: {step.get('result', 'No result')[:100]}...\"\n",
    "            for i, step in enumerate(completed_steps)\n",
    "        ])\n",
    "        \n",
    "        chain = evaluation_prompt | self.llm | StrOutputParser()\n",
    "        evaluation_result = chain.invoke({\n",
    "            \"original_task\": original_task,\n",
    "            \"steps_summary\": steps_summary\n",
    "        })\n",
    "        \n",
    "        # Parse evaluation (simplified)\n",
    "        success = \"SUCCESS\" in evaluation_result.upper()\n",
    "        \n",
    "        return {\n",
    "            \"success\": success,\n",
    "            \"evaluation\": evaluation_result,\n",
    "            \"steps_count\": len(completed_steps),\n",
    "            \"quality_indicators\": {\n",
    "                \"completion_rate\": len([s for s in completed_steps if s.get(\"status\") == \"success\"]) / max(len(completed_steps), 1),\n",
    "                \"error_rate\": len([s for s in completed_steps if s.get(\"status\") == \"failed\"]) / max(len(completed_steps), 1)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstration of Autonomous Agent System\n",
    "class AutonomousAgentDemo:\n",
    "    \"\"\"Comprehensive demonstration of autonomous agent capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.agent = AdvancedAgentSystem(llm)\n",
    "        self.setup_demo_tools()\n",
    "    \n",
    "    def setup_demo_tools(self):\n",
    "        \"\"\"Register demonstration tools\"\"\"\n",
    "        \n",
    "        def web_search(query: str) -> str:\n",
    "            return f\"Search results for '{query}': [Simulated web search results]\"\n",
    "        \n",
    "        def file_manager(action: str, filename: str = \"\", content: str = \"\") -> str:\n",
    "            return f\"File operation '{action}' on '{filename}': Success\"\n",
    "        \n",
    "        def api_call(endpoint: str, data: Dict = None) -> str:\n",
    "            return f\"API call to '{endpoint}': Success (simulated)\"\n",
    "        \n",
    "        def data_analysis(dataset: str, analysis_type: str = \"summary\") -> str:\n",
    "            return f\"Analysis '{analysis_type}' on '{dataset}': Completed with insights\"\n",
    "        \n",
    "        # Register tools\n",
    "        self.agent.register_tool(\"web_search\", web_search, \"Search the web for information\")\n",
    "        self.agent.register_tool(\"file_manager\", file_manager, \"Create, read, update, delete files\")\n",
    "        self.agent.register_tool(\"api_call\", api_call, \"Make API calls to external services\")\n",
    "        self.agent.register_tool(\"data_analysis\", data_analysis, \"Analyze datasets and generate insights\")\n",
    "    \n",
    "    def demo_complex_research_task(self):\n",
    "        \"\"\"Demonstrate agent handling complex multi-step research task\"\"\"\n",
    "        print(\"üî¨ AUTONOMOUS RESEARCH AGENT DEMONSTRATION\")\n",
    "        \n",
    "        complex_task = \"\"\"\n",
    "        Research the current state of quantum computing and create a comprehensive report including:\n",
    "        1. Recent breakthrough discoveries in quantum computing\n",
    "        2. Major companies and their quantum computing initiatives\n",
    "        3. Current limitations and challenges\n",
    "        4. Potential future applications\n",
    "        5. Timeline predictions for quantum supremacy achievements\n",
    "        \n",
    "        The report should be well-structured, factual, and include citations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(complex_task)\n",
    "        return result\n",
    "    \n",
    "    def demo_software_development_task(self):\n",
    "        \"\"\"Demonstrate agent handling software development workflow\"\"\"\n",
    "        print(\"üíª AUTONOMOUS DEVELOPMENT AGENT DEMONSTRATION\")\n",
    "        \n",
    "        dev_task = \"\"\"\n",
    "        Create a complete web application for a personal task management system including:\n",
    "        1. Backend API with user authentication\n",
    "        2. Database schema for tasks and users\n",
    "        3. Frontend interface with CRUD operations\n",
    "        4. Unit tests for core functionality\n",
    "        5. Deployment configuration\n",
    "        6. Documentation and README\n",
    "        \n",
    "        Use modern best practices and ensure security considerations.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.agent.autonomous_execution(dev_task)\n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_demo(self):\n",
    "        \"\"\"Run comprehensive autonomous agent demonstrations\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT SYSTEM DEMONSTRATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Demo 1: Research Task\n",
    "        results['research'] = self.demo_complex_research_task()\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        \n",
    "        # Demo 2: Development Task  \n",
    "        results['development'] = self.demo_software_development_task()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AUTONOMOUS AGENT DEMONSTRATIONS COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76645053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and demonstrate autonomous agents\n",
    "if 'autonomous_agent' not in tutorial_state:\n",
    "    agent_demo = AutonomousAgentDemo(memory_llm)\n",
    "    tutorial_state['autonomous_agent'] = agent_demo\n",
    "    \n",
    "    print(\"üöÄ STARTING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = agent_demo.run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results\n",
    "else:\n",
    "    print(\"üîÑ RUNNING AUTONOMOUS AGENT DEMONSTRATIONS\")\n",
    "    autonomous_results = tutorial_state['autonomous_agent'].run_comprehensive_demo()\n",
    "    tutorial_state['autonomous_results'] = autonomous_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143533c6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd518b",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b24f75",
   "metadata": {},
   "source": [
    "Now that we've mastered building intelligent agents and workflows, it's time to tackle one of the most important challenges in modern AI systems: how do we give our agents access to vast, specific, and up-to-date knowledge that wasn't included in their training data?\n",
    "\n",
    "This is where Retrieval-Augmented Generation (RAG) becomes essential. RAG is the bridge between the incredible reasoning capabilities of large language models and the specific, detailed knowledge that your applications need to be truly useful in real-world scenarios.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*WYv0_CaBmCTt7FXc\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e3e6c",
   "metadata": {},
   "source": [
    "### Why RAG Is Essential: The Knowledge Gap Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e18ff2",
   "metadata": {},
   "source": [
    "Let me paint a picture of why RAG matters. Imagine you've built a brilliant customer service agent using the workflows we just learned. It can route questions, use tools, and maintain conversation context perfectly. But then a customer asks about your company's specific return policy that was updated last week, or wants details about a product that was launched after the model's training cutoff.\n",
    "\n",
    "**The Fundamental Limitations of LLMs:**\n",
    "\n",
    "Even the most advanced language models face critical limitations when used alone:\n",
    "\n",
    "1. **Knowledge Cutoff**: Training data has a specific cutoff date, making models ignorant of recent information\n",
    "2. **Domain Specificity**: Models lack deep knowledge about your specific business, products, or internal processes  \n",
    "3. **Context Window Limits**: Even with large context windows, you can't fit entire knowledge bases into a single conversation\n",
    "4. **Hallucination Risk**: When models don't know something, they often generate plausible-sounding but incorrect information\n",
    "5. **Static Knowledge**: The information encoded during training can't be updated without retraining\n",
    "\n",
    "**Where Our Agent Workflows Hit the Wall:** The sophisticated agent workflows we've built are incredibly powerful for reasoning and decision-making, but they're only as good as the knowledge they have access to. Without RAG:\n",
    "\n",
    "- Your routing system might correctly identify that a question is about \"product specifications,\" but have no way to retrieve the actual, current specifications\n",
    "- Your memory system can remember what users have discussed, but can't recall relevant company knowledge or documentation\n",
    "- Your tools can calculate and process data, but can't access your proprietary knowledge base or recent updates\n",
    "\n",
    "**RAG as the Solution:** Retrieval-Augmented Generation solves these problems by creating a dynamic bridge between your agents and external knowledge sources. Instead of relying solely on the model's trained knowledge, RAG systems:\n",
    "\n",
    "- **Retrieve** relevant information from external knowledge bases in real-time\n",
    "- **Augment** the model's prompt with this retrieved context  \n",
    "- **Generate** responses that combine the model's reasoning abilities with specific, current, and accurate information\n",
    "\n",
    "This creates agents that maintain their sophisticated reasoning capabilities while having access to vast, specific, and up-to-date knowledge that makes them truly useful for real-world applications.\n",
    "\n",
    "**What We'll Build:** In this section, we'll explore how to integrate RAG into the agentic systems we've been building, creating agents that can seamlessly combine reasoning, tool use, memory, and knowledge retrieval into powerful, practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc169c2",
   "metadata": {},
   "source": [
    "### Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cb659",
   "metadata": {},
   "source": [
    "Document preprocessing is the foundation of any effective RAG system. Without proper structured, labeled data on database, no model can perform good. A good data preprocessing is crucial espcially in large scale production systems where we deal with millions of documents in real time, any small mistake or bug can lead to catastrophic failures. It's important to choose the right preprocessing techniques given requirements and to align well with business goal. \n",
    "\n",
    "**The Challenge:** Raw documents come in countless formats, structures, and sizes. A PDF might contain tables, images, and multi-column layouts. A web page includes navigation menus, advertisements, and dynamic content. A code repository has different file types with distinct syntaxes. Without proper preprocessing, even the most sophisticated retrieval system will struggle to find and present relevant information effectively.\n",
    "\n",
    "Document preprocessing involves several transformations that can be expressed mathematically:\n",
    "\n",
    "- **Information Density**: $\\rho = \\frac{\\text{Relevant Content}}{\\text{Total Content}}$ - maximizing signal-to-noise ratio\n",
    "- **Semantic Coherence**: $C(chunk) = \\frac{\\sum_{i,j} similarity(sent_i, sent_j)}{n(n-1)/2}$ - ensuring chunks maintain internal consistency  \n",
    "- **Optimal Chunk Size**: $size_{optimal} = \\arg\\max_{s} (retrieval\\_accuracy(s) - processing\\_cost(s))$\n",
    "\n",
    "<img src=\"https://chamomile.ai/reliable-rag-with-data-preprocessing/image6.png\" width=700>\n",
    "\n",
    "**The Preprocessing Pipeline:** Our approach follows a systematic four-stage pipeline:\n",
    "\n",
    "1. **Document Loading**: Extract content from various formats while preserving semantic structure\n",
    "2. **Splitting**: Break documents into manageable sections based on natural boundaries\n",
    "3. **Chunking**: Create optimally-sized pieces that balance context and specificity  \n",
    "4. **Embedding**: Transform text into vector representations for semantic search\n",
    "\n",
    "Each stage has multiple strategies optimized for different document types and use cases. Let's explore each in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d7a17",
   "metadata": {},
   "source": [
    "#### Document Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e08f9",
   "metadata": {},
   "source": [
    "Document loading is the critical first step in building effective RAG systems. Different document types require specialized loaders optimized for their unique structures and challenges. Let's explore the ecosystem of document loaders available in LangChain and understand when to use each one.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776c9a1",
   "metadata": {},
   "source": [
    "##### Web Content Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c7fc3",
   "metadata": {},
   "source": [
    "Web content presents unique challenges: dynamic JavaScript rendering, complex layouts, advertisements, navigation elements, and varying HTML structures. Choosing the right web loader depends on your specific requirements around speed, accuracy, and the complexity of target websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba584f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Loader** | **Best For** | **Key Features** | **Considerations** | **Type** |\n",
    "|------------|--------------|------------------|-------------------|----------|\n",
    "| [Web](https://python.langchain.com/docs/integrations/document_loaders/web_base) | Simple static pages | ‚Ä¢ Uses urllib + BeautifulSoup<br>‚Ä¢ Fast and lightweight<br>‚Ä¢ No external dependencies | ‚Ä¢ Struggles with JavaScript-heavy sites<br>‚Ä¢ Basic HTML parsing only<br>‚Ä¢ No dynamic content handling | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Complex layouts | ‚Ä¢ Advanced structure detection<br>‚Ä¢ Preserves semantic hierarchy<br>‚Ä¢ Handles tables and formatting | ‚Ä¢ Slower processing<br>‚Ä¢ Heavier dependencies<br>‚Ä¢ May need additional setup | Package |\n",
    "| [RecursiveURL](https://python.langchain.com/docs/integrations/document_loaders/recursive_url) | Documentation sites | ‚Ä¢ Automatically discovers child links<br>‚Ä¢ Configurable depth control<br>‚Ä¢ Maintains site structure | ‚Ä¢ Can retrieve too much data<br>‚Ä¢ Requires careful depth limits<br>‚Ä¢ May hit rate limits | Package |\n",
    "| [Sitemap](https://python.langchain.com/docs/integrations/document_loaders/sitemap) | Entire websites | ‚Ä¢ Uses sitemap.xml for discovery<br>‚Ä¢ Efficient site crawling<br>‚Ä¢ Respects site structure | ‚Ä¢ Requires valid sitemap<br>‚Ä¢ May miss pages not in sitemap<br>‚Ä¢ Large sites = long processing | Package |\n",
    "| [Spider](https://python.langchain.com/docs/integrations/document_loaders/spider) | Production crawling | ‚Ä¢ LLM-optimized output format<br>‚Ä¢ Handles JavaScript rendering<br>‚Ä¢ Anti-bot bypass capabilities | ‚Ä¢ Requires API key<br>‚Ä¢ Usage-based pricing<br>‚Ä¢ External service dependency | API |\n",
    "| [Firecrawl](https://python.langchain.com/docs/integrations/document_loaders/firecrawl) | Enterprise scraping | ‚Ä¢ Self-hostable option<br>‚Ä¢ JavaScript execution<br>‚Ä¢ Advanced content extraction | ‚Ä¢ Complex setup if self-hosted<br>‚Ä¢ API costs if cloud-hosted<br>‚Ä¢ Requires infrastructure | API |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Document-heavy sites | ‚Ä¢ Specialized for document extraction<br>‚Ä¢ Format preservation<br>‚Ä¢ Multi-format support | ‚Ä¢ Focused on document-centric sites<br>‚Ä¢ May be overkill for simple pages<br>‚Ä¢ Learning curve | Package |\n",
    "| [Hyperbrowser](https://python.langchain.com/docs/integrations/document_loaders/hyperbrowser) | Complex web apps | ‚Ä¢ Full browser automation<br>‚Ä¢ JavaScript execution<br>‚Ä¢ Session management | ‚Ä¢ Higher latency<br>‚Ä¢ Resource intensive<br>‚Ä¢ API-based pricing | API |\n",
    "| [AgentQL](https://python.langchain.com/docs/integrations/document_loaders/agentql) | Structured extraction | ‚Ä¢ Natural language queries<br>‚Ä¢ Precise data targeting<br>‚Ä¢ Schema-based extraction | ‚Ä¢ Best for specific data points<br>‚Ä¢ Requires query design<br>‚Ä¢ API costs | API |\n",
    "| [Oxylabs](https://python.langchain.com/docs/integrations/document_loaders/oxylabs) | Large-scale scraping | ‚Ä¢ Enterprise-grade infrastructure<br>‚Ä¢ Geographic proxy support<br>‚Ä¢ High success rates | ‚Ä¢ Premium pricing<br>‚Ä¢ Overkill for small projects<br>‚Ä¢ External dependency | API |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c74ea3",
   "metadata": {},
   "source": [
    "There's PDF content loaders as well \n",
    "\n",
    "| **Document Loader** | **Description** | **Package/API** |\n",
    "| --- | --- | --- |\n",
    "| [PyPDF](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader) | Uses `pypdf` to load and parse PDFs | Package |\n",
    "| [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | Uses Unstructured's open source library to load PDFs | Package |\n",
    "| [Amazon Textract](https://python.langchain.com/docs/integrations/document_loaders/amazon_textract) | Uses AWS API to load PDFs | API |\n",
    "| [MathPix](https://python.langchain.com/docs/integrations/document_loaders/mathpix) | Uses MathPix to load PDFs | Package |\n",
    "| [PDFPlumber](https://python.langchain.com/docs/integrations/document_loaders/pdfplumber) | Load PDF files using PDFPlumber | Package |\n",
    "| [PyPDFDirectry](https://python.langchain.com/docs/integrations/document_loaders/pypdfdirectory) | Load a directory with PDF files | Package |\n",
    "| [PyPDFium2](https://python.langchain.com/docs/integrations/document_loaders/pypdfium2) | Load PDF files using PyPDFium2 | Package |\n",
    "| [PyMuPDF](https://python.langchain.com/docs/integrations/document_loaders/pymupdf) | Load PDF files using PyMuPDF | Package |\n",
    "| [PyMuPDF4LLM](https://python.langchain.com/docs/integrations/document_loaders/pymupdf4llm) | Load PDF content to Markdown using PyMuPDF4LLM | Package |\n",
    "| [PDFMiner](https://python.langchain.com/docs/integrations/document_loaders/pdfminer) | Load PDF files using PDFMiner | Package |\n",
    "| [Upstage Document Parse Loader](https://python.langchain.com/docs/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader | Package |\n",
    "| [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling) | Load PDF files using Docling | Package |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbf6bd",
   "metadata": {},
   "source": [
    "#### Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7679573",
   "metadata": {},
   "source": [
    "explain different types of document splitting, the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfbca2",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab550fe4",
   "metadata": {},
   "source": [
    "explain different types of document chunking , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d351c",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39eed15",
   "metadata": {},
   "source": [
    "explain different types of document embedding. the math behind them if needed , their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d238e7",
   "metadata": {},
   "source": [
    "### Storing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb48998",
   "metadata": {},
   "source": [
    "introduce to storing documents, the math behind them if needed different ways of representing them and how different types of documents can be fed to rag and stuff etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477119b6",
   "metadata": {},
   "source": [
    "#### Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9f893",
   "metadata": {},
   "source": [
    "explain different types of vector database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608dc711",
   "metadata": {},
   "source": [
    "#### Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e19279",
   "metadata": {},
   "source": [
    "explain different types of vector database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090d616",
   "metadata": {},
   "source": [
    "#### SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b8e8c",
   "metadata": {},
   "source": [
    "explain different types of sql database , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8555b",
   "metadata": {},
   "source": [
    "### Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dcc54",
   "metadata": {},
   "source": [
    "introduce to retreiver mechanisms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c40d32",
   "metadata": {},
   "source": [
    "explain different types of retreival mechanisms , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbce248",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b8387",
   "metadata": {},
   "source": [
    "introduce to evaluation mechniasms, different ways of representing them and how different types of documents can be fed to rag and stuff etc the math behind them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd43da7",
   "metadata": {},
   "source": [
    "explain different types of evaluation methods , the math behind them if needed their usecases and implement various all possible lang or llama family methods (llama index, langchain,langsmith,langgraph,langserve..) if needed  to showcase it and difference between them for different data types with sensible explanation in parts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f82ea2",
   "metadata": {},
   "source": [
    "## A Complete Agentic System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11448f82",
   "metadata": {},
   "source": [
    "## Limitations & Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ba15",
   "metadata": {},
   "source": [
    "#### RAPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a3d4e",
   "metadata": {},
   "source": [
    "#### Self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d098",
   "metadata": {},
   "source": [
    "#### CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d56e81",
   "metadata": {},
   "source": [
    "#### Adaptive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adba2b0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813de77",
   "metadata": {},
   "source": [
    "#### Workflow Pattern Selection Guide & Best Practices\n",
    "\n",
    "Choosing the right workflow pattern is crucial for building effective agentic systems. Here's a comprehensive guide based on production experience and Anthropic's research:\n",
    "\n",
    "**üîó Prompt Chaining** - Use when:\n",
    "- Tasks can be cleanly decomposed into sequential steps\n",
    "- Each step benefits from focused attention\n",
    "- Quality is more important than latency\n",
    "- You need programmatic validation gates\n",
    "- Examples: Content generation ‚Üí review ‚Üí translation ‚Üí cultural adaptation\n",
    "\n",
    "**üìç Routing** - Use when:\n",
    "- Input types have distinct handling requirements  \n",
    "- Specialized expertise improves outcomes significantly\n",
    "- Classification can be performed reliably\n",
    "- Different cost/performance tradeoffs exist per route\n",
    "- Examples: Customer service triage, query complexity routing\n",
    "\n",
    "**‚ö° Parallelization** - Use when:\n",
    "- **Sectioning**: Independent subtasks can run simultaneously\n",
    "- **Voting**: Multiple perspectives improve decision confidence\n",
    "- Latency reduction is critical\n",
    "- Ensemble methods provide measurable accuracy gains\n",
    "- Examples: Multi-aspect analysis, content moderation, code review\n",
    "\n",
    "**üéØ Orchestrator-Workers** - Use when:\n",
    "- Task requirements can't be predicted in advance\n",
    "- Dynamic subtask generation is needed\n",
    "- Different specialists handle different aspects\n",
    "- Complex coordination is required\n",
    "- Examples: Software development, research synthesis, creative projects\n",
    "\n",
    "**üîÑ Evaluator-Optimizer** - Use when:\n",
    "- Iterative refinement demonstrably improves quality\n",
    "- Clear evaluation criteria exist\n",
    "- The LLM can provide meaningful self-criticism\n",
    "- Quality improvement justifies additional latency\n",
    "- Examples: Creative writing, complex analysis, strategic planning\n",
    "\n",
    "**ü§ñ Autonomous Agents** - Use when:\n",
    "- Open-ended problems with unpredictable steps\n",
    "- Long-running tasks requiring persistence\n",
    "- Environment interaction and feedback loops exist\n",
    "- Human oversight can be incorporated at checkpoints\n",
    "- Trust level supports autonomous operation\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "1. **Start Simple**: Begin with the simplest pattern that meets requirements\n",
    "2. **Measure Performance**: Always evaluate accuracy, latency, and cost tradeoffs\n",
    "3. **Error Handling**: Implement robust error recovery and fallback strategies\n",
    "4. **Human Oversight**: Include checkpoints for critical decisions\n",
    "5. **Composability**: Patterns can be combined for sophisticated workflows\n",
    "6. **Tool Design**: Invest heavily in clear, well-documented tool interfaces\n",
    "7. **Testing**: Extensive testing in sandboxed environments before production\n",
    "\n",
    "\n",
    "### Memory Systems Quick Reference\n",
    "\n",
    "Now that we've seen memory systems in action, here's a practical guide for choosing the right approach:\n",
    "\n",
    "| Memory Type | Best Use Case | Pros | Cons | Complexity |\n",
    "|-------------|---------------|------|------|------------|\n",
    "| **ConversationBufferMemory** | Short, detail-critical conversations | Perfect recall, simple setup | Linear cost growth, token limits | O(n) |\n",
    "| **ConversationSummaryMemory** | Long-term relationships, key themes | Scales indefinitely, preserves important info | Loses detail, summarization overhead | O(log n) |\n",
    "| **ConversationBufferWindowMemory** | Task-oriented, recent context matters | Predictable performance, constant cost | Forgets older context completely | O(k) |\n",
    "| **ConversationTokenBufferMemory** | Production apps, cost control | Optimal context usage, never exceeds limits | Complex token counting logic | O(tokens) |\n",
    "| **ConversationEntityMemory** | Relationship tracking, complex scenarios | Maintains entity relationships, intelligent context | Requires entity extraction, higher complexity | O(entities) |\n",
    "| **CombinedMemory** | Sophisticated applications | Leverages multiple approaches, flexible | Complex setup, coordination overhead | O(combined) |\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "- üìù **Need perfect recall?** ‚Üí Buffer Memory\n",
    "- üîÑ **Long conversations?** ‚Üí Summary Memory  \n",
    "- ‚ö° **Recent context only?** ‚Üí Window Memory\n",
    "- üí∞ **Cost control critical?** ‚Üí Token Memory\n",
    "- üë• **Tracking relationships?** ‚Üí Entity Memory\n",
    "- üß† **Multiple requirements?** ‚Üí Combined Memory\n",
    "\n",
    "**Memory Performance Characteristics:**\n",
    "- **Buffer**: Grows with conversation length - great for short, detailed discussions\n",
    "- **Summary**: Logarithmic growth - ideal for ongoing relationships  \n",
    "- **Window**: Constant size - perfect for task-focused interactions\n",
    "- **Token**: Bounded growth - excellent for production cost control\n",
    "- **Entity**: Scales with entities - powerful for complex relationship tracking\n",
    "- **Combined**: Flexible scaling - adaptable to diverse requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6403629",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de317b91",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
